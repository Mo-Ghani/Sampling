{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Uses Of MCMC and Nested Sampling Algorithms There are various off-the-shelf samplers that make use MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used to analyse real world, open source data sets. The Samplers The following samplers are used in one of the demonstrations on this site: PyMC3 dynesty emcee UltraNest Zeus Nestle The Data All the data used in these examples is open source, and publicly available on the following links: Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Average solar sunspot numbers since 1750 Gravitational wave signal from GW150914","title":"Introduction"},{"location":"#the-uses-of-mcmc-and-nested-sampling-algorithms","text":"There are various off-the-shelf samplers that make use MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used to analyse real world, open source data sets.","title":"The Uses Of MCMC and Nested Sampling Algorithms"},{"location":"#the-samplers","text":"The following samplers are used in one of the demonstrations on this site: PyMC3 dynesty emcee UltraNest Zeus Nestle","title":"The Samplers"},{"location":"#the-data","text":"All the data used in these examples is open source, and publicly available on the following links: Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Average solar sunspot numbers since 1750 Gravitational wave signal from GW150914","title":"The Data"},{"location":"LightCurve/LightCurve/","text":"Using emcee and UltraNest to model the light curves from Kepler-10 Kepler-10 is a star located roughly 608 lightyears from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes infront of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intesity, occuring every time the exoplanet is infront of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existense of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA. Useful imports # numpy import numpy as np # scipy from scipy.special import gammaln, ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt %matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print('emcee version: {}'.format(mc.__version__)) print('UltraNest version: {}'.format(ultranest.__version__)) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2 Viewing the data Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (simillar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits.open(\"kplr011904151-2010265121752_llc.fits\") tab = table[1] data = Table(tab.data) flux_orig = data['PDCSAP_FLUX'][:1325] time_orig = data['TIME'][:1325] flux = [flux_orig[i] for i in range(len(flux_orig)) if str(flux_orig[i]) != \"nan\"] time = [time_orig[i] for i in range(len(time_orig)) if str(flux_orig[i]) != \"nan\"] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # plot all useful data ax1.plot(time, flux, \"k:\") ax1.set_ylabel(\"Flux / e-/s\") # plot zoomed in view of transits ax2.plot(time, flux, \"k:\") ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541325, 541700) ax2.set_xlim(540, 545) plt.suptitle(\"Kepler-10 light curves showing evidence of exoplanet transits\") plt.show() The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy. A problem you may notice right away with this data set is that the flux drop during each transit is small; the flux only decreases by 0.025%. This fluctuation is even comparable to the level of noise in the data. To solve this, I augmented the data set by making two changes. First, I subtracted all the flux below the light curve, so that the lowest points in the data sit just above 1 unit of flux, and then took the natural log of the entire data set. The purpose of this was to stretch out the transit flux change, whilst keeping the noise level in check. This augmentation reults in the flux drop increasing to 20%. The second augmentation is shortening the data set. The full data is made up of over 1300 points, which takes around 8 minutes total to run this script on my machine. To save time, I'll just use the \"zoomed in\" data shown above. floor = 541300 flux_aug = [np.log(i-floor) if i - floor > 1 else 0 for i in flux][:155] time_aug = time[:155] # plot augmented lightcurve plt.figure(figsize=(15,3)) plt.plot(time_aug, flux_aug, 'k:') plt.ylabel('Augmented flux') plt.xlabel('Time / days') plt.title('Augmented light curve from Kepler-10') plt.show() The model Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is simillar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit(time, f, df, p, tt, tf=None, off=0, square=False): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immidiately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None: tf = tt if tt <= tf: # Default to square shaped transit square = True y = [] if not square: # define slope of sides of trapezium h = f*df*tt/(tt-tf) grad = 2*h/tt for i in time: j = (i + off) % p if j < tt: # transit # square shaped transit if square: y.append(f*(1 - df)) # trapezium shaped transit elif j/tt < 0.5: # first half of transit val = f - grad*j if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # last half of transit val = (grad*j) - 2*h + f if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # no transit y.append(f) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models. After the sampling has been completed, all of the augmentations I made earlier need to be undone. The following functions will do just that: def unaug_f(f_aug): \"\"\" returns array of original f, given array of augmented f \"\"\" f = np.exp(np.array(f_aug))+floor return f def unaug_df(df_aug, f_mean, f_is_aug=False): \"\"\" returns array of original df, given array of augmented df and mean original/augmented f \"\"\" if f_is_aug: f_mean = unaug_f(f_mean) df = 1 - ( (f_mean-floor)**(1-df_aug) + floor ) / f return df Modelling with emcee Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Due to the noise, I'll use a uniform prior on f, but a normal prior on other parameters. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # uniform prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [(f_min, f_max), (df_mu, df_sig), (p_mu, p_sig), (tt_mu, tt_sig), (tf_mu, tf_sig), (off_mu, off_sig)] # remove tf for square transit parameters priors_square = priors[:4] + priors[5:] Sampling the data The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very simillar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior(theta): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors[i][0] < theta[i] < priors[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors[i][0]) / priors[i][1])**2 return lprior def logprior_square(theta): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors_square)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors_square[i][0] < theta[i] < priors_square[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors_square[i][0]) / priors_square[i][1])**2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike(theta): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, tf_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, tf_like, off=off_like)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b def loglike_square(theta): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, off=off_like, square=True)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior(theta): lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta) def logposterior_square(theta): lprior = logprior_square(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_square(theta) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" sanples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range(len(priors)): if i == 0: inisamples.append(np.random.uniform(priors[i][0], priors[i][1],Nens)) else: inisamples.append(np.random.normal(priors[i][0], priors[i][1],Nens)) inisamples = np.array(inisamples).T inisamples_square = [] for i in range(len(priors_square)): if i == 0: inisamples_square.append(np.random.uniform(priors_square[i][0], priors_square[i][1],Nens)) else: inisamples_square.append(np.random.normal(priors_square[i][0], priors_square[i][1],Nens)) inisamples_square = np.array(inisamples_square).T ndims = inisamples.shape[1] ndims_square = inisamples_square.shape[1] # no. iterations Nburn = 500 Nsamples = 500 loglike.ncalls = 0 loglike_square.ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc.EnsembleSampler(Nens, ndims, logposterior) sampler_square = mc.EnsembleSampler(Nens, ndims_square, logposterior_square) # perform sampling t0 = timer() sampler.run_mcmc(inisamples, Nsamples+Nburn) t1 = timer() print(\"time taken to sample a trapezium transit model with emcee: {} seconds\".format(t1-t0)) sampler_square.run_mcmc(inisamples_square, Nsamples+Nburn) t2 = timer() print(\"time taken to sample a square transit model with emcee: {} seconds\".format(t2-t1)) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler.chain[:, Nburn:, :].reshape((-1, ndims)) samples_square = sampler_square.chain[:, Nburn:, :].reshape((-1, ndims_square)) Results Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['Aug Flux', 'Aug dFlux', 'Period', 'Transit Time', 'Transit Flat Time', 'Offset'] plotposts(samples_trapez, labels) For each model, we can find the mean and standard deviation of each parameter using the traces. To do this, we have to unaugment f and df, using the functions described above. For the trapezium transit model: f, ferr = np.mean(unaug_f(samples_trapez[:,0]) ), np.std(unaug_f(samples_trapez[:,0])) df, dferr = np.mean(unaug_df(samples_trapez[:,1],f) ), np.std(unaug_df(samples_trapez[:,1],f)) p, perr = np.mean(samples_trapez[:,2]), np.std(samples_trapez[:,2]) tt, tterr = np.mean(samples_trapez[:,3]), np.std(samples_trapez[:,3]) tf, tferr = np.mean(samples_trapez[:,4]), np.std(samples_trapez[:,4]) off, offerr = np.mean(samples_trapez[:,5]), np.std(samples_trapez[:,5]) print(\"Parameters describing a trapezium shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f,ferr) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df,dferr) + \" period = {} \\u00B1 {} days \\n\".format(p,perr) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt,tterr) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf,tferr) + \" offset = {} \\u00B1 {} days \\n\".format(off,offerr)) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square, ferr_square = np.mean(unaug_f(samples_square[:,0]) ), np.std(unaug_f(samples_square[:,0])) df_square, dferr_square = np.mean(unaug_df(samples_square[:,1],f_square) ), np.std(unaug_df(samples_square[:,1],f_square)) p_square, perr_square = np.mean(samples_square[:,2]), np.std(samples_square[:,2]) tt_square, tterr_square = np.mean(samples_square[:,3]), np.std(samples_square[:,3]) off_square, offerr_square = np.mean(samples_square[:,4]), np.std(samples_square[:,4]) print(\"Parameters describing a square shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_square,ferr_square) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_square,dferr_square) + \" period = {} \\u00B1 {} days \\n\".format(p_square,perr_square) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_square,tterr_square) + \" offset = {} \\u00B1 {} days \\n\".format(off_square,offerr_square)) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b. Plotting the posterior We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np.random.choice(unaug_f(samples_trapez[:,0]),n_fits) dfsamps_trap_emcee = np.random.choice(unaug_df(samples_trapez[:,1],f),n_fits) psamps_trap_emcee = np.random.choice(samples_trapez[:,2],n_fits) ttsamps_trap_emcee = np.random.choice(samples_trapez[:,3],n_fits) tfsamps_trap_emcee = np.random.choice(samples_trapez[:,4],n_fits) offsamps_trap_emcee = np.random.choice(samples_trapez[:,5],n_fits) fsamps_square_emcee = np.random.choice(unaug_f(samples_square[:,0]),n_fits) dfsamps_square_emcee = np.random.choice(unaug_df(samples_square[:,1],f_square),n_fits) psamps_square_emcee = np.random.choice(samples_square[:,2],n_fits) ttsamps_square_emcee = np.random.choice(samples_square[:,3],n_fits) offsamps_square_emcee = np.random.choice(samples_square[:,4],n_fits) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f,df,p,tt,tf,off) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\") plt.show() The same process can be repeated for the square transit model: fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_square,df_square,p_square,tt_square,off=off_square) ax1.plot(x, y, \"b-\", alpha = 0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_square_emcee[i], dfsamps_square_emcee[i], psamps_square_emcee[i], ttsamps_square_emcee[i], off=offsamps_square_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\") plt.show() From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system. Model comparisons Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs, RpRs_err = np.sqrt(df), dferr/(np.sqrt(df)) RpRs_square, RpRs_square_err = np.sqrt(df_square), dferr_square/(np.sqrt(df_square)) print(\"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {}\".format(RpRs, RpRs_err)) print(\"Planet to star radius ratio (square transit model): {} \\u00B1 {}\".format( RpRs_square, RpRs_square_err)) print(\"True planet to star radius ratio: {}\".format(0.0127)) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 True planet to star radius ratio: 0.0127 Both models are in close agreement with eachother, and are quite close to the true value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and true ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square, tferr_square = 0.99*tt_square, 0.99*tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor(df,tt,tf): # return semi-minor axis, given model parameters numerator = (tt**2)*(1-np.sqrt(df))**2 - (tf**2)*(1+np.sqrt(df))**2 denominator = tt**2 - tf**2 return np.sqrt(numerator / denominator) def normsemimajor(df,p,tt,tf): # return normalised semi-major axis, given model parameters numerator = 2*p*(df)**(1/4) denominator = np.pi*np.sqrt(tt**2 - tf**2) return numerator / denominator # semiminor for trapezium model b_trap = semiminor(df, tt, tf) b_trap_err = abs(semiminor(df + dferr, tt, tf) - b_trap) # semiminor for square model b_square = semiminor(df_square, tt_square, tf_square) b_square_err = abs(semiminor(df_square + dferr_square, tt_square, tf_square) - b_square) # semimajor for trapezium model aRs_trap = normsemimajor(df, p, tt, tf) aRs_trap_err = abs(normsemimajor(df+dferr,p+perr,tt+tterr,tf+tferr) - aRs_trap) # semimajor for square model aRs_square = normsemimajor(df_square, p_square, tt_square, tf_square) aRs_square_err = abs(normsemimajor(df_square+dferr_square,p_square+perr_square, tt_square+tterr_square,tf_square+tferr_square) - aRs_square) print(\"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {}\".format(aRs_trap, aRs_trap_err)) print(\"Normalised semi-major axis (square transit model): {} \\u00B1 {}\".format(aRs_square, aRs_square_err)) print(\"True normalised semi-major axis: {}\".format(3.40)) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 True normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np.arccos(b_trap/aRs_trap) * (180/np.pi) I_trap_err = abs(np.arccos((b_trap+b_trap_err)/(aRs_trap-aRs_trap_err)) * (180/np.pi) - I_trap) I_square = np.arccos(b_square/aRs_square) * (180/np.pi) I_square_err = abs(np.arccos((b_square+b_square_err)/(aRs_square-aRs_square_err) ) * (180/np.pi) - I_square) print(\"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\".format(I_trap,I_trap_err)) print(\"Inclination angle (square transit model): {} \\u00B1 {} degrees\".format(I_square,I_square_err)) print(\"True inclination angle: {} degrees\".format(84.4)) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees True inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density(df,p,tt,aRs): # return density of star given model parameters G = 6.67408e-11 wt = tt*np.pi/p # transform p from days to seconds p *= 86400 numerator = 3*np.pi*aRs**3 denominator = G*p**2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density(df,p,tt,aRs_trap) stard_trap_err = abs(star_density(df+dferr,p-perr,tt-tterr,aRs_trap+aRs_trap_err) - stard_trap) # star density predicted by square model stard_square = star_density(df_square,p_square,tt_square,aRs_square) stard_square_err = abs(star_density(df_square+dferr_square,p_square-perr_square,tt_square-tterr_square, aRs_square+aRs_square_err) - stard_square) print(\"Star density (trapezium transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_trap, stard_trap_err)) print(\"Star density (square transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_square, stard_square_err)) print(\"True star density: {} kg/m\\u00b3\".format(1070)) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only infront of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above. Modelling with UltraNest If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package. Sampling the data This works in a simillar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors[i][1]-priors[i][0])*theta[i] + priors[i][0] else: # normal transform for remaining parameters params[i] = priors[i][0] + priors[i][1]*ndtri(theta[i]) return np.array(params) def prior_transform_square(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors_square[i][1]-priors_square[i][0] )*theta[i] + priors_square[i][0] else: # normal transform for remaining parameters params[i] = priors_square[i][0] + priors_square[i][1]*ndtri(theta[i]) return np.array(params) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','tf','off'], loglike, prior_transform) sampler_square = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','off'], loglike_square, prior_transform_square) # use \"slice\" sampler, due to high dimensionality nsteps = 2*len(priors) sampler.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) sampler_square.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer() results_trap = sampler.run(min_num_live_points=nlive) t1 = timer() results_square = sampler_square.run(min_num_live_points=nlive) t2 = timer() print('\\n \\n'+ 'Time taken to sample trapezium shaped model with UltraNest: {} seconds'.format(t1-t0)) print('Time taken to sample square shaped model with UltraNest: {} seconds'.format(t2-t1)) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds Results To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np.array(results_trap[\"weighted_samples\"][\"points\"]) weights_trap = np.array(results_trap[\"weighted_samples\"][\"weights\"]) resample_trap = np.random.rand(len(weights_trap)) < weights_trap/max(weights_trap) samples_trap_ultra = samples_points_trap[resample_trap, :] The mean parameter values and their errors can be found easily, remembering to unaugment f and df: # parameter means and errors for trapezium transit f_trap_ultra, ferr_trap_ultra = np.mean(unaug_f(samples_trap_ultra[:,0]) ), np.std(unaug_f(samples_trap_ultra[:,0])) df_trap_ultra, dferr_trap_ultra = np.mean(unaug_df(samples_trap_ultra[:,1],f_trap_ultra) ), np.std(unaug_df(samples_trap_ultra[:,1],f_trap_ultra)) p_trap_ultra, perr_trap_ultra = np.mean(samples_trap_ultra[:,2] ), np.std(samples_trap_ultra[:,2]) tt_trap_ultra, tterr_trap_ultra = np.mean(samples_trap_ultra[:,3] ), np.std(samples_trap_ultra[:,3]) tf_trap_ultra, tferr_trap_ultra = np.mean(samples_trap_ultra[:,4] ), np.std(samples_trap_ultra[:,4]) off_trap_ultra, offerr_trap_ultra = np.mean(samples_trap_ultra[:,5] ), np.std(samples_trap_ultra[:,5]) print(\"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_trap_ultra,ferr_trap_ultra) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_trap_ultra,dferr_trap_ultra) + \" period = {} \\u00B1 {} days \\n\".format(p_trap_ultra,perr_trap_ultra) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_trap_ultra,tterr_trap_ultra) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf_trap_ultra,tferr_trap_ultra) + \" offset = {} \\u00B1 {} days \\n\".format(off_trap_ultra,offerr_trap_ultra)) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days Plotting the posterior We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np.random.choice(unaug_f(samples_trap_ultra[:,0]),nfits) dfsamps_trap_ultra = np.random.choice(unaug_df(samples_trap_ultra[:,1],f_trap_ultra),nfits) psamps_trap_ultra = np.random.choice(samples_trap_ultra[:,2],nfits) ttsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,3],nfits) tfsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,4],nfits) offsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,5],nfits) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15,10)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_trap_ultra,df_trap_ultra,p_trap_ultra, tt_trap_ultra,tf_trap_ultra,off_trap_ultra) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") ax1.set_title(\"UltraNest mean posterior plot\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_title(\"UltraNest posterior predictive plot\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) # emcee and UltraNest overlapping posterior predictive plot for i in range(n_fits): y_ultra = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) y_emcee = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax3.plot(x, y_ultra, 'b-', alpha=0.008, linewidth=5, label='UltraNest' if i == 0 else '') ax3.plot(x, y_emcee, 'r-', alpha=0.008, linewidth=5, label='emcee' if i == 0 else '') ax3.plot(time, flux, 'k:', linewidth=3) ax3.set_ylabel('Flux / e-/s') ax3.set_xlabel('Time / days') ax3.set_title('emcee (red) and UltraNest (blue) overlayed posterior predictive plots') ax3.set_ylim(541300, 541600) ax3.set_xlim(540, 545) leg = ax3.legend(loc='lower right') for lh in leg.legendHandles: lh.set_alpha(0.8) plt.suptitle('A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)') plt.show() It seems from this that the two models are very simillar, however since the low density regious are mostly blue, UltraNest may produce a greater uncertainty than emcee. Model comparisons Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap, logZerr_trap = results_trap['logz'], results_trap['logzerr'] logZ_square, logZerr_square = results_square['logz'], results_square['logzerr'] print(\"Marginalised likelihood for trapezium transit model: {} \u00b1 {}\".format(logZ_trap, logZerr_trap)) print(\"Marginalised likelihood for square transit model: {} \u00b1 {}\".format(logZ_square, logZerr_square)) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np.exp(logZ_trap - logZ_square) print(\"Bayes factor: {}\".format(K)) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"Exoplanet Light Curve Analysis with emcee and UltraNest"},{"location":"LightCurve/LightCurve/#using-emcee-and-ultranest-to-model-the-light-curves-from-kepler-10","text":"Kepler-10 is a star located roughly 608 lightyears from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes infront of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intesity, occuring every time the exoplanet is infront of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existense of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA.","title":"Using emcee and UltraNest to model the light curves from Kepler-10"},{"location":"LightCurve/LightCurve/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.special import gammaln, ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt %matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print('emcee version: {}'.format(mc.__version__)) print('UltraNest version: {}'.format(ultranest.__version__)) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2","title":"Useful imports"},{"location":"LightCurve/LightCurve/#viewing-the-data","text":"Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (simillar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits.open(\"kplr011904151-2010265121752_llc.fits\") tab = table[1] data = Table(tab.data) flux_orig = data['PDCSAP_FLUX'][:1325] time_orig = data['TIME'][:1325] flux = [flux_orig[i] for i in range(len(flux_orig)) if str(flux_orig[i]) != \"nan\"] time = [time_orig[i] for i in range(len(time_orig)) if str(flux_orig[i]) != \"nan\"] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # plot all useful data ax1.plot(time, flux, \"k:\") ax1.set_ylabel(\"Flux / e-/s\") # plot zoomed in view of transits ax2.plot(time, flux, \"k:\") ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541325, 541700) ax2.set_xlim(540, 545) plt.suptitle(\"Kepler-10 light curves showing evidence of exoplanet transits\") plt.show() The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy. A problem you may notice right away with this data set is that the flux drop during each transit is small; the flux only decreases by 0.025%. This fluctuation is even comparable to the level of noise in the data. To solve this, I augmented the data set by making two changes. First, I subtracted all the flux below the light curve, so that the lowest points in the data sit just above 1 unit of flux, and then took the natural log of the entire data set. The purpose of this was to stretch out the transit flux change, whilst keeping the noise level in check. This augmentation reults in the flux drop increasing to 20%. The second augmentation is shortening the data set. The full data is made up of over 1300 points, which takes around 8 minutes total to run this script on my machine. To save time, I'll just use the \"zoomed in\" data shown above. floor = 541300 flux_aug = [np.log(i-floor) if i - floor > 1 else 0 for i in flux][:155] time_aug = time[:155] # plot augmented lightcurve plt.figure(figsize=(15,3)) plt.plot(time_aug, flux_aug, 'k:') plt.ylabel('Augmented flux') plt.xlabel('Time / days') plt.title('Augmented light curve from Kepler-10') plt.show()","title":"Viewing the data"},{"location":"LightCurve/LightCurve/#the-model","text":"Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is simillar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit(time, f, df, p, tt, tf=None, off=0, square=False): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immidiately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None: tf = tt if tt <= tf: # Default to square shaped transit square = True y = [] if not square: # define slope of sides of trapezium h = f*df*tt/(tt-tf) grad = 2*h/tt for i in time: j = (i + off) % p if j < tt: # transit # square shaped transit if square: y.append(f*(1 - df)) # trapezium shaped transit elif j/tt < 0.5: # first half of transit val = f - grad*j if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # last half of transit val = (grad*j) - 2*h + f if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # no transit y.append(f) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models. After the sampling has been completed, all of the augmentations I made earlier need to be undone. The following functions will do just that: def unaug_f(f_aug): \"\"\" returns array of original f, given array of augmented f \"\"\" f = np.exp(np.array(f_aug))+floor return f def unaug_df(df_aug, f_mean, f_is_aug=False): \"\"\" returns array of original df, given array of augmented df and mean original/augmented f \"\"\" if f_is_aug: f_mean = unaug_f(f_mean) df = 1 - ( (f_mean-floor)**(1-df_aug) + floor ) / f return df","title":"The model"},{"location":"LightCurve/LightCurve/#modelling-with-emcee","text":"Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Due to the noise, I'll use a uniform prior on f, but a normal prior on other parameters. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # uniform prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [(f_min, f_max), (df_mu, df_sig), (p_mu, p_sig), (tt_mu, tt_sig), (tf_mu, tf_sig), (off_mu, off_sig)] # remove tf for square transit parameters priors_square = priors[:4] + priors[5:]","title":"Modelling with emcee"},{"location":"LightCurve/LightCurve/#sampling-the-data","text":"The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very simillar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior(theta): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors[i][0] < theta[i] < priors[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors[i][0]) / priors[i][1])**2 return lprior def logprior_square(theta): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors_square)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors_square[i][0] < theta[i] < priors_square[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors_square[i][0]) / priors_square[i][1])**2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike(theta): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, tf_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, tf_like, off=off_like)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b def loglike_square(theta): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, off=off_like, square=True)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior(theta): lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta) def logposterior_square(theta): lprior = logprior_square(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_square(theta) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" sanples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range(len(priors)): if i == 0: inisamples.append(np.random.uniform(priors[i][0], priors[i][1],Nens)) else: inisamples.append(np.random.normal(priors[i][0], priors[i][1],Nens)) inisamples = np.array(inisamples).T inisamples_square = [] for i in range(len(priors_square)): if i == 0: inisamples_square.append(np.random.uniform(priors_square[i][0], priors_square[i][1],Nens)) else: inisamples_square.append(np.random.normal(priors_square[i][0], priors_square[i][1],Nens)) inisamples_square = np.array(inisamples_square).T ndims = inisamples.shape[1] ndims_square = inisamples_square.shape[1] # no. iterations Nburn = 500 Nsamples = 500 loglike.ncalls = 0 loglike_square.ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc.EnsembleSampler(Nens, ndims, logposterior) sampler_square = mc.EnsembleSampler(Nens, ndims_square, logposterior_square) # perform sampling t0 = timer() sampler.run_mcmc(inisamples, Nsamples+Nburn) t1 = timer() print(\"time taken to sample a trapezium transit model with emcee: {} seconds\".format(t1-t0)) sampler_square.run_mcmc(inisamples_square, Nsamples+Nburn) t2 = timer() print(\"time taken to sample a square transit model with emcee: {} seconds\".format(t2-t1)) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler.chain[:, Nburn:, :].reshape((-1, ndims)) samples_square = sampler_square.chain[:, Nburn:, :].reshape((-1, ndims_square))","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results","text":"Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['Aug Flux', 'Aug dFlux', 'Period', 'Transit Time', 'Transit Flat Time', 'Offset'] plotposts(samples_trapez, labels) For each model, we can find the mean and standard deviation of each parameter using the traces. To do this, we have to unaugment f and df, using the functions described above. For the trapezium transit model: f, ferr = np.mean(unaug_f(samples_trapez[:,0]) ), np.std(unaug_f(samples_trapez[:,0])) df, dferr = np.mean(unaug_df(samples_trapez[:,1],f) ), np.std(unaug_df(samples_trapez[:,1],f)) p, perr = np.mean(samples_trapez[:,2]), np.std(samples_trapez[:,2]) tt, tterr = np.mean(samples_trapez[:,3]), np.std(samples_trapez[:,3]) tf, tferr = np.mean(samples_trapez[:,4]), np.std(samples_trapez[:,4]) off, offerr = np.mean(samples_trapez[:,5]), np.std(samples_trapez[:,5]) print(\"Parameters describing a trapezium shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f,ferr) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df,dferr) + \" period = {} \\u00B1 {} days \\n\".format(p,perr) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt,tterr) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf,tferr) + \" offset = {} \\u00B1 {} days \\n\".format(off,offerr)) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square, ferr_square = np.mean(unaug_f(samples_square[:,0]) ), np.std(unaug_f(samples_square[:,0])) df_square, dferr_square = np.mean(unaug_df(samples_square[:,1],f_square) ), np.std(unaug_df(samples_square[:,1],f_square)) p_square, perr_square = np.mean(samples_square[:,2]), np.std(samples_square[:,2]) tt_square, tterr_square = np.mean(samples_square[:,3]), np.std(samples_square[:,3]) off_square, offerr_square = np.mean(samples_square[:,4]), np.std(samples_square[:,4]) print(\"Parameters describing a square shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_square,ferr_square) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_square,dferr_square) + \" period = {} \\u00B1 {} days \\n\".format(p_square,perr_square) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_square,tterr_square) + \" offset = {} \\u00B1 {} days \\n\".format(off_square,offerr_square)) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b.","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior","text":"We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np.random.choice(unaug_f(samples_trapez[:,0]),n_fits) dfsamps_trap_emcee = np.random.choice(unaug_df(samples_trapez[:,1],f),n_fits) psamps_trap_emcee = np.random.choice(samples_trapez[:,2],n_fits) ttsamps_trap_emcee = np.random.choice(samples_trapez[:,3],n_fits) tfsamps_trap_emcee = np.random.choice(samples_trapez[:,4],n_fits) offsamps_trap_emcee = np.random.choice(samples_trapez[:,5],n_fits) fsamps_square_emcee = np.random.choice(unaug_f(samples_square[:,0]),n_fits) dfsamps_square_emcee = np.random.choice(unaug_df(samples_square[:,1],f_square),n_fits) psamps_square_emcee = np.random.choice(samples_square[:,2],n_fits) ttsamps_square_emcee = np.random.choice(samples_square[:,3],n_fits) offsamps_square_emcee = np.random.choice(samples_square[:,4],n_fits) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f,df,p,tt,tf,off) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\") plt.show() The same process can be repeated for the square transit model: fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_square,df_square,p_square,tt_square,off=off_square) ax1.plot(x, y, \"b-\", alpha = 0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_square_emcee[i], dfsamps_square_emcee[i], psamps_square_emcee[i], ttsamps_square_emcee[i], off=offsamps_square_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\") plt.show() From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system.","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons","text":"Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs, RpRs_err = np.sqrt(df), dferr/(np.sqrt(df)) RpRs_square, RpRs_square_err = np.sqrt(df_square), dferr_square/(np.sqrt(df_square)) print(\"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {}\".format(RpRs, RpRs_err)) print(\"Planet to star radius ratio (square transit model): {} \\u00B1 {}\".format( RpRs_square, RpRs_square_err)) print(\"True planet to star radius ratio: {}\".format(0.0127)) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 True planet to star radius ratio: 0.0127 Both models are in close agreement with eachother, and are quite close to the true value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and true ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square, tferr_square = 0.99*tt_square, 0.99*tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor(df,tt,tf): # return semi-minor axis, given model parameters numerator = (tt**2)*(1-np.sqrt(df))**2 - (tf**2)*(1+np.sqrt(df))**2 denominator = tt**2 - tf**2 return np.sqrt(numerator / denominator) def normsemimajor(df,p,tt,tf): # return normalised semi-major axis, given model parameters numerator = 2*p*(df)**(1/4) denominator = np.pi*np.sqrt(tt**2 - tf**2) return numerator / denominator # semiminor for trapezium model b_trap = semiminor(df, tt, tf) b_trap_err = abs(semiminor(df + dferr, tt, tf) - b_trap) # semiminor for square model b_square = semiminor(df_square, tt_square, tf_square) b_square_err = abs(semiminor(df_square + dferr_square, tt_square, tf_square) - b_square) # semimajor for trapezium model aRs_trap = normsemimajor(df, p, tt, tf) aRs_trap_err = abs(normsemimajor(df+dferr,p+perr,tt+tterr,tf+tferr) - aRs_trap) # semimajor for square model aRs_square = normsemimajor(df_square, p_square, tt_square, tf_square) aRs_square_err = abs(normsemimajor(df_square+dferr_square,p_square+perr_square, tt_square+tterr_square,tf_square+tferr_square) - aRs_square) print(\"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {}\".format(aRs_trap, aRs_trap_err)) print(\"Normalised semi-major axis (square transit model): {} \\u00B1 {}\".format(aRs_square, aRs_square_err)) print(\"True normalised semi-major axis: {}\".format(3.40)) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 True normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np.arccos(b_trap/aRs_trap) * (180/np.pi) I_trap_err = abs(np.arccos((b_trap+b_trap_err)/(aRs_trap-aRs_trap_err)) * (180/np.pi) - I_trap) I_square = np.arccos(b_square/aRs_square) * (180/np.pi) I_square_err = abs(np.arccos((b_square+b_square_err)/(aRs_square-aRs_square_err) ) * (180/np.pi) - I_square) print(\"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\".format(I_trap,I_trap_err)) print(\"Inclination angle (square transit model): {} \\u00B1 {} degrees\".format(I_square,I_square_err)) print(\"True inclination angle: {} degrees\".format(84.4)) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees True inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density(df,p,tt,aRs): # return density of star given model parameters G = 6.67408e-11 wt = tt*np.pi/p # transform p from days to seconds p *= 86400 numerator = 3*np.pi*aRs**3 denominator = G*p**2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density(df,p,tt,aRs_trap) stard_trap_err = abs(star_density(df+dferr,p-perr,tt-tterr,aRs_trap+aRs_trap_err) - stard_trap) # star density predicted by square model stard_square = star_density(df_square,p_square,tt_square,aRs_square) stard_square_err = abs(star_density(df_square+dferr_square,p_square-perr_square,tt_square-tterr_square, aRs_square+aRs_square_err) - stard_square) print(\"Star density (trapezium transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_trap, stard_trap_err)) print(\"Star density (square transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_square, stard_square_err)) print(\"True star density: {} kg/m\\u00b3\".format(1070)) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only infront of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above.","title":"Model comparisons"},{"location":"LightCurve/LightCurve/#modelling-with-ultranest","text":"If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package.","title":"Modelling with UltraNest"},{"location":"LightCurve/LightCurve/#sampling-the-data_1","text":"This works in a simillar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors[i][1]-priors[i][0])*theta[i] + priors[i][0] else: # normal transform for remaining parameters params[i] = priors[i][0] + priors[i][1]*ndtri(theta[i]) return np.array(params) def prior_transform_square(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors_square[i][1]-priors_square[i][0] )*theta[i] + priors_square[i][0] else: # normal transform for remaining parameters params[i] = priors_square[i][0] + priors_square[i][1]*ndtri(theta[i]) return np.array(params) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','tf','off'], loglike, prior_transform) sampler_square = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','off'], loglike_square, prior_transform_square) # use \"slice\" sampler, due to high dimensionality nsteps = 2*len(priors) sampler.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) sampler_square.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer() results_trap = sampler.run(min_num_live_points=nlive) t1 = timer() results_square = sampler_square.run(min_num_live_points=nlive) t2 = timer() print('\\n \\n'+ 'Time taken to sample trapezium shaped model with UltraNest: {} seconds'.format(t1-t0)) print('Time taken to sample square shaped model with UltraNest: {} seconds'.format(t2-t1)) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results_1","text":"To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np.array(results_trap[\"weighted_samples\"][\"points\"]) weights_trap = np.array(results_trap[\"weighted_samples\"][\"weights\"]) resample_trap = np.random.rand(len(weights_trap)) < weights_trap/max(weights_trap) samples_trap_ultra = samples_points_trap[resample_trap, :] The mean parameter values and their errors can be found easily, remembering to unaugment f and df: # parameter means and errors for trapezium transit f_trap_ultra, ferr_trap_ultra = np.mean(unaug_f(samples_trap_ultra[:,0]) ), np.std(unaug_f(samples_trap_ultra[:,0])) df_trap_ultra, dferr_trap_ultra = np.mean(unaug_df(samples_trap_ultra[:,1],f_trap_ultra) ), np.std(unaug_df(samples_trap_ultra[:,1],f_trap_ultra)) p_trap_ultra, perr_trap_ultra = np.mean(samples_trap_ultra[:,2] ), np.std(samples_trap_ultra[:,2]) tt_trap_ultra, tterr_trap_ultra = np.mean(samples_trap_ultra[:,3] ), np.std(samples_trap_ultra[:,3]) tf_trap_ultra, tferr_trap_ultra = np.mean(samples_trap_ultra[:,4] ), np.std(samples_trap_ultra[:,4]) off_trap_ultra, offerr_trap_ultra = np.mean(samples_trap_ultra[:,5] ), np.std(samples_trap_ultra[:,5]) print(\"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_trap_ultra,ferr_trap_ultra) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_trap_ultra,dferr_trap_ultra) + \" period = {} \\u00B1 {} days \\n\".format(p_trap_ultra,perr_trap_ultra) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_trap_ultra,tterr_trap_ultra) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf_trap_ultra,tferr_trap_ultra) + \" offset = {} \\u00B1 {} days \\n\".format(off_trap_ultra,offerr_trap_ultra)) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior_1","text":"We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np.random.choice(unaug_f(samples_trap_ultra[:,0]),nfits) dfsamps_trap_ultra = np.random.choice(unaug_df(samples_trap_ultra[:,1],f_trap_ultra),nfits) psamps_trap_ultra = np.random.choice(samples_trap_ultra[:,2],nfits) ttsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,3],nfits) tfsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,4],nfits) offsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,5],nfits) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15,10)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_trap_ultra,df_trap_ultra,p_trap_ultra, tt_trap_ultra,tf_trap_ultra,off_trap_ultra) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") ax1.set_title(\"UltraNest mean posterior plot\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_title(\"UltraNest posterior predictive plot\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) # emcee and UltraNest overlapping posterior predictive plot for i in range(n_fits): y_ultra = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) y_emcee = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax3.plot(x, y_ultra, 'b-', alpha=0.008, linewidth=5, label='UltraNest' if i == 0 else '') ax3.plot(x, y_emcee, 'r-', alpha=0.008, linewidth=5, label='emcee' if i == 0 else '') ax3.plot(time, flux, 'k:', linewidth=3) ax3.set_ylabel('Flux / e-/s') ax3.set_xlabel('Time / days') ax3.set_title('emcee (red) and UltraNest (blue) overlayed posterior predictive plots') ax3.set_ylim(541300, 541600) ax3.set_xlim(540, 545) leg = ax3.legend(loc='lower right') for lh in leg.legendHandles: lh.set_alpha(0.8) plt.suptitle('A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)') plt.show() It seems from this that the two models are very simillar, however since the low density regious are mostly blue, UltraNest may produce a greater uncertainty than emcee.","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons_1","text":"Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap, logZerr_trap = results_trap['logz'], results_trap['logzerr'] logZ_square, logZerr_square = results_square['logz'], results_square['logzerr'] print(\"Marginalised likelihood for trapezium transit model: {} \u00b1 {}\".format(logZ_trap, logZerr_trap)) print(\"Marginalised likelihood for square transit model: {} \u00b1 {}\".format(logZ_square, logZerr_square)) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np.exp(logZ_trap - logZ_square) print(\"Bayes factor: {}\".format(K)) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/","text":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133. Useful imports # numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import pymc3 as pm print('PyMC3 version: {}'.format(pm.__version__)) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print('dynesty version: {}'.format(dynesty.__version__)) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1 Viewing the data The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\". This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open(cali_dir, 'r') as file: calibration = file.read().splitlines() calibration = list(map(float, calibration)) c_0 = calibration[0] c_2 = calibration[2] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open(spectra_dir, 'r') as file: counts = [int(j) for j in file] counts = counts[2:] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np.array(range(len(counts))) # Bins for gamma-ray spectrum # Plot the spectrum plt.figure(figsize=(15,5)) plt.plot(xrange, counts, 'b') plt.fill(xrange, counts, 'b', alpha= 0.4) plt.xlabel('Energy / MCA Channels') plt.ylabel('Counts') plt.title('Gamma-Ray Spectrum of a sample of Ba-133') plt.yscale('log') plt.xlim(540, 3500) plt.show() The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks(counts, height=1300, distance=100)[0][3:] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Lets plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [c_0*np.array(range(peak-ranger, peak+ranger)) + c_2 for peak in peaks] peaks_y = [counts[peak-ranger:peak+ranger] for peak in peaks] # Plot selected peaks from gamma-ray spectrum fig, axs = plt.subplots(2,3, figsize=(12,7)) for i in range(2): for j in range(3): ind = 3*i + j axs[i,j].plot(peaks_x[ind], peaks_y[ind], 'b') axs[i, j].fill(peaks_x[ind], peaks_y[ind], 'b', alpha=0.2) if i == 1: axs[i,j].set_xlabel('Energy / KeV') if j == 0: axs[i,j].set_ylabel('Counts') fig.suptitle('Photopeaks produced by a sample of Ba-133', y=0.95) plt.show() The model The decays that cause the photopeaks in a GRS have a descrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature, rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian fuction to be used: def gauss(x, a, xc, w, y0): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a*np.exp(-(x-xc)**2/(2*w**2))+y0 Modelling with PyMC3 Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [pm.Model() for i in range(len(peaks))] a_guesses = [23000., 900., 6100., 13800., 39800., 5300.] xc_guesses = [81., 161., 276.5, 303., 356., 384.] y0_guesses = [1700., 1350., 300., 300., 250., 50.] Sampling the data Next, I'll use the above guesses to initialise each model. PyMC3 requires the used to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probabillity distribution functions. This is done within the scope of each model defined above, using the \"with\" statement: for i in range(len(peaks)): with gauss_models[i]: # set prior parameters # amplitude a_mu = a_guesses[i] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[i] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses[i] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Uniform('Standard Deviation', lower=w_min, upper=w_max) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # Expected value of outcome mu = gauss(peaks_x[i], a_model, xc_model, w_model, y0_model) # Poisson likelihood of observations Y_obs = pm.Poisson('Y_obs', mu=mu, observed=peaks_y[i]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging.getLogger('pymc3') logger.setLevel(logging.ERROR) # perform sampling traces = [] t0 = time() for i in range(len(peaks)): with gauss_models[i]: traces.append(pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True)) t1 = time() timepymc3 = t1-t0 # time taken to sample all of the photopeaks print('{} seconds ({} seconds per peak) taken to run PyMC3 sampling.'.format(timepymc3, timepymc3/6)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling. Sampling the data - Alternate method The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probabillity distribution in log-space, meaning the standard deviation has an equal probabillity of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using simillar code as before, but with the new likelihood. gauss_model_alt = pm.Model() with gauss_model_alt: # set prior parameters # amplitude a_mu = a_guesses[1] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[1] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses[1] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = -1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Normal('Standard Deviation', mu=w_mu, sd=w_sig) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # set uniform prior sigma_model = pm.Uniform('Noise', lower=sigma_min, upper=sigma_max) # Expected value of outcome mu = gauss(peaks_x[1], a_model, xc_model, w_model, y0_model) # Normal likelihood of observations with noise Y_obs = pm.Normal('Y_obs', mu=mu, sd=10 ** sigma_model, observed=peaks_y[1]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time() with gauss_model_alt: trace_alt = pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True) t1_alt = time() timepymc3_alt = t1_alt-t0_alt print('{} seconds taken to run PyMC3 alternate sampling.'.format(timepymc3_alt)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np.vstack((trace_alt['Amplitude'], trace_alt['Peak Energy'], trace_alt['Standard Deviation'], trace_alt['Background Counts'], trace_alt['Noise'])).T # mean and standard deviation error of each parameter a_alt, a_err_alt = np.mean(samples_alt[:,0]), np.std(samples_alt[:,0]) xc_alt, xc_err_alt = np.mean(samples_alt[:,1]), np.std(samples_alt[:,1]) w_alt, w_err_alt = np.mean(samples_alt[:,2]), np.std(samples_alt[:,2]) y0_alt, y0_err_alt = np.mean(samples_alt[:,3]), np.std(samples_alt[:,3]) sigma_alt, sigma_err_alt = np.mean(samples_alt[:,4]), np.std(samples_alt[:,4]) # print values print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_alt, a_err_alt) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_alt, xc_err_alt) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_alt, w_err_alt) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_alt, y0_err_alt) + ' Noise = {} \\u00B1 {} counts'.format(sigma_alt, sigma_err_alt)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts Results Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [np.vstack((trace['Amplitude'], trace['Peak Energy'], trace['Standard Deviation'], trace['Background Counts'])).T for trace in traces] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range(len(peaks))] for ind in range(len(peaks)): resdict[ind]['a_mu'] = np.mean(all_pymc3_samples[ind][:, 0]) resdict[ind]['a_sig'] = np.std(all_pymc3_samples[ind][:, 0]) resdict[ind]['xc_mu'] = np.mean(all_pymc3_samples[ind][:, 1]) resdict[ind]['xc_sig'] = np.std(all_pymc3_samples[ind][:, 1]) resdict[ind]['w_mu'] = np.mean(all_pymc3_samples[ind][:, 2]) resdict[ind]['w_sig'] = np.std(all_pymc3_samples[ind][:, 2]) resdict[ind]['y0_mu'] = np.mean(all_pymc3_samples[ind][:, 3]) resdict[ind]['y0_sig'] = np.std(all_pymc3_samples[ind][:, 3]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts(samples, labels, **kwargs): \"\"\" Function to plot posteriors using corner.py and scipy's gaussian KDE function. \"\"\" fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) plt.subplots_adjust(wspace=0.2, hspace=0.2) # plot KDE smoothed version of distributions for axidx, samps in zip([0, 5, 10, 15], samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 100) fig.axes[axidx].plot(xvals, kde(xvals), color=\"firebrick\") # create corner plot for peak with noisiest data labels = [r'Amplitude', r'Peak Energy', r'Standard Deviation', r'Background Counts'] corner_plot_samples = all_pymc3_samples[1] plotposts(corner_plot_samples, labels) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a, a_err = resdict[1]['a_mu'], resdict[1]['a_sig'] xc, xc_err = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w, w_err = resdict[1]['w_mu'], resdict[1]['w_sig'] y0, y0_err = resdict[1]['y0_mu'], resdict[1]['y0_sig'] print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a, a_err) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc, xc_err) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w, w_err) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0, y0_err)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts Plotting the posterior Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig, axs = plt.subplots(2, 3, figsize=(12, 7)) for i in range(2): for j in range(3): ind = 3 * i + j a = resdict[ind]['a_mu'] xc = resdict[ind]['xc_mu'] w = resdict[ind]['w_mu'] y0 = resdict[ind]['y0_mu'] x = peaks_x[ind] y = peaks_y[ind] # plot original data axs[i, j].plot(x, y, 'b.', alpha=1, label=('Original Data' if all(num == 0 for num in [i,j]) else '')) # plot fitted curve over the data xsmooth = np.linspace(x[0], x[-1], len(x) * 100) axs[i, j].plot(xsmooth, gauss(xsmooth, a, xc, w, y0), 'k:', alpha=1, label=('Fitted Model' if all(num == 0 for num in [i,j]) else '')) if i == 1: axs[i, j].set_xlabel('Energy / keV') if j == 0: axs[i, j].set_ylabel('Counts') leg = fig.legend(loc='lower right', numpoints=1) for lh in leg.legendHandles: lh.set_alpha(1) fig.suptitle('Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling') plt.show() Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3, xc_samps_pymc3, w_samps_pymc3, y0_samps_pymc3 = ([] for i in range(4)) for ind in range(len(peaks)): a_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 0], size=n_fits)) xc_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 1], size=n_fits)) w_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 2], size=n_fits)) y0_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 3], size=n_fits)) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x[ind] xsmooth = np.linspace(x[0], x[-1], len(x) * 100) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4)) for i in range(n_fits): ax1.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.01, linewidth=2) ax2.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.02, linewidth=2) ax1.set_ylabel('Counts') ax1.set_xlabel('Energy / keV') ax2.set_xlim(159.4,161.8) ax2.set_ylim(1800, 2250) ax2.set_xlabel('Energy / keV') fig.suptitle('Posterior predictive plot for a photopeak from a sample of Ba-133') plt.show() Model comparisons Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak: # parameter means and standard deviations of peak a_pymc3, aerr_pymc3 = resdict[1]['a_mu'], resdict[1]['a_sig'] xc_pymc3, xcerr_pymc3 = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w_pymc3, werr_pymc3 = resdict[1]['w_mu'], resdict[1]['w_sig'] y0_pymc3, y0err_pymc3 = resdict[1]['y0_mu'], resdict[1]['y0_sig'] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate.quad(lambda t: gauss(t, a_pymc3, xc_pymc3, w_pymc3, y0_pymc3), 159.1, 162.2)[0] / c_0 peak_integral_err = np.sqrt(2 * np.pi * ((w_pymc3 * aerr_pymc3) ** 2 + (a_pymc3 * werr_pymc3) ** 2 )) / c_0 percent_err = 100*peak_integral_err/peak_integral print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral, peak_integral_err) + 'Percentage error = {}%'.format(percent_err)) Total counts = 22933.86204613347 \u00b1 215.58699451020811 counts Percentage error = 0.9400378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate.quad(lambda t: gauss(t, a_alt, xc_alt, w_alt, y0_alt), 159.1, 162.2)[0] / c_0 peak_integral_err_alt = np.sqrt(2 * np.pi * ((w_alt * a_err_alt) ** 2 + (a_alt * w_err_alt) ** 2 )) / c_0 percent_err_alt = 100*peak_integral_err_alt/peak_integral_alt print('Alternate total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_alt, peak_integral_err_alt) + 'Alternate percentage error = {}%'.format(percent_err_alt)) Alternate total counts = 22943.751210742565 \u00b1 216.76313608385547 counts Alternate percentage error = 0.9447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful. Modelling with dynesty Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they can do a better job. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here. Sampling the data Using dynesty is slightly more complicated than PyMC3. Nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform(theta): # unpack the transformed parameters a_t, xc_t, w_t, y0_t = theta # define our prior guesses for each parameter a_mu, a_sig = a_guesses[1], 50. xc_mu, xc_sig = xc_guesses[1], 1. w_mu, w_sig = 0.7, 0.3 y0_mu, y0_sig = y0_guesses[1], 30. # convert back to a = a_mu + a_sig*ndtri(a_t) xc = xc_mu + xc_sig*ndtri(xc_t) w = w_mu + w_sig*ndtri(w_t) y0 = y0_mu + y0_sig*ndtri(y0_t) return a,xc,w,y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike(theta): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like, xc_like, w_like, y0_like = theta # expected value lmbda = np.array(gauss(peaks_x[1], a_like, xc_like, w_like, y0_like)) n = len(peaks_y[1]) a = np.sum(gammaln(np.array(peaks_y[1])+1)) b = np.sum(np.array(peaks_y[1]) * np.log(lmbda)) return -np.sum(lmbda) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points, sampling algorithm, sampling method, and a stopping criterion. Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler(loglike, priortransform, nparam, bound='multi', sample='unif', nlive=1000) t0_dynesty = time() sampler.run_nested(dlogz=stop, print_progress=False) t1_dynesty = time() print('{} seconds taken to run dynesty sampling'.format(t1_dynesty-t0_dynesty)) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler.results print(results.summary()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None Results To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np.exp(results['logwt'] - results['logz'][-1]) samples = results.samples dynesty_samples = resample_equal(samples, weights) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a, xc, w, y0 = [dynesty_samples[:,i] for i in range(4)] a_dynesty, aerr_dynesty = np.mean(a), np.std(a) xc_dynesty, xcerr_dynesty = np.mean(xc), np.std(xc) w_dynesty, werr_dynesty = np.mean(w), np.std(w) y0_dynesty, y0err_dynesty = np.mean(y0), np.std(y0) nfits = 300 a_samps_dynesty = np.random.normal(a_dynesty, aerr_dynesty, nfits) xc_samps_dynesty = np.random.normal(xc_dynesty, xcerr_dynesty, nfits) w_samps_dynesty = np.random.normal(w_dynesty, werr_dynesty, nfits) y0_samps_dynesty = np.random.normal(y0_dynesty, y0err_dynesty, nfits) print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_dynesty, aerr_dynesty) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_dynesty, xcerr_dynesty) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_dynesty, werr_dynesty) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_dynesty, y0err_dynesty)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts Plotting the posterior So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'b.') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'k:') ax1.set_ylim(1200) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01) ax2.set_ylim(1200) plt.suptitle('Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of\\n' + ' Ba-133, sampled by dynesty') plt.show() Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'k.',label='Data') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'b:',label='dynesty') ax1.plot(xsmooth, gauss(xsmooth,a_pymc3,xc_pymc3,w_pymc3,y0_pymc3),'r:',label='PyMC3') ax1.set_ylim(1200) leg1 = ax1.legend(loc='upper right') for lh in leg1.legendHandles: lh.set_alpha(1) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01, label=('dynesty' if i == 0 else '')) ax2.plot(xsmooth, gauss(xsmooth,a_samps_pymc3[1][i],xc_samps_pymc3[1][i], w_samps_pymc3[1][i],y0_samps_pymc3[1][i]),'r-',alpha=0.01, label=('PyMC3' if i == 0 else '')) ax2.set_ylim(1200) leg2 = ax2.legend(loc='upper right') for lh in leg2.legendHandles: lh.set_alpha(1) plt.suptitle('Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + '\\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)') plt.show() Model comparisons This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate.quad(lambda t: gauss(t, a_dynesty, xc_dynesty, w_dynesty, y0_dynesty), 159.1, 162.2)[0] / c_0 peak_integral_err_dynesty = np.sqrt(2 * np.pi * ((w_dynesty * aerr_dynesty)** 2 + (a_dynesty * werr_dynesty) ** 2 )) / c_0 percent_err_dynesty = 100*peak_integral_err_dynesty/peak_integral_dynesty print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_dynesty, peak_integral_err_dynesty) + 'Percentage error = {}%'.format(percent_err_dynesty)) Total counts = 22968.854117562147 \u00b1 224.93466521968398 counts Percentage error = 0.9793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error around 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"Gamma-Ray Spectroscopy with PyMC3 and dynesty"},{"location":"PyMC3_GRS/PyMC3_GRS/#using-pymc3-and-dynesty-to-fit-gaussian-curves-to-photopeaks-in-a-gamma-ray-spectrum","text":"A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133.","title":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum"},{"location":"PyMC3_GRS/PyMC3_GRS/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import pymc3 as pm print('PyMC3 version: {}'.format(pm.__version__)) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print('dynesty version: {}'.format(dynesty.__version__)) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1","title":"Useful imports"},{"location":"PyMC3_GRS/PyMC3_GRS/#viewing-the-data","text":"The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\". This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open(cali_dir, 'r') as file: calibration = file.read().splitlines() calibration = list(map(float, calibration)) c_0 = calibration[0] c_2 = calibration[2] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open(spectra_dir, 'r') as file: counts = [int(j) for j in file] counts = counts[2:] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np.array(range(len(counts))) # Bins for gamma-ray spectrum # Plot the spectrum plt.figure(figsize=(15,5)) plt.plot(xrange, counts, 'b') plt.fill(xrange, counts, 'b', alpha= 0.4) plt.xlabel('Energy / MCA Channels') plt.ylabel('Counts') plt.title('Gamma-Ray Spectrum of a sample of Ba-133') plt.yscale('log') plt.xlim(540, 3500) plt.show() The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks(counts, height=1300, distance=100)[0][3:] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Lets plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [c_0*np.array(range(peak-ranger, peak+ranger)) + c_2 for peak in peaks] peaks_y = [counts[peak-ranger:peak+ranger] for peak in peaks] # Plot selected peaks from gamma-ray spectrum fig, axs = plt.subplots(2,3, figsize=(12,7)) for i in range(2): for j in range(3): ind = 3*i + j axs[i,j].plot(peaks_x[ind], peaks_y[ind], 'b') axs[i, j].fill(peaks_x[ind], peaks_y[ind], 'b', alpha=0.2) if i == 1: axs[i,j].set_xlabel('Energy / KeV') if j == 0: axs[i,j].set_ylabel('Counts') fig.suptitle('Photopeaks produced by a sample of Ba-133', y=0.95) plt.show()","title":"Viewing the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#the-model","text":"The decays that cause the photopeaks in a GRS have a descrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature, rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian fuction to be used: def gauss(x, a, xc, w, y0): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a*np.exp(-(x-xc)**2/(2*w**2))+y0","title":"The model"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-pymc3","text":"Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [pm.Model() for i in range(len(peaks))] a_guesses = [23000., 900., 6100., 13800., 39800., 5300.] xc_guesses = [81., 161., 276.5, 303., 356., 384.] y0_guesses = [1700., 1350., 300., 300., 250., 50.]","title":"Modelling with PyMC3"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data","text":"Next, I'll use the above guesses to initialise each model. PyMC3 requires the used to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probabillity distribution functions. This is done within the scope of each model defined above, using the \"with\" statement: for i in range(len(peaks)): with gauss_models[i]: # set prior parameters # amplitude a_mu = a_guesses[i] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[i] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses[i] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Uniform('Standard Deviation', lower=w_min, upper=w_max) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # Expected value of outcome mu = gauss(peaks_x[i], a_model, xc_model, w_model, y0_model) # Poisson likelihood of observations Y_obs = pm.Poisson('Y_obs', mu=mu, observed=peaks_y[i]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging.getLogger('pymc3') logger.setLevel(logging.ERROR) # perform sampling traces = [] t0 = time() for i in range(len(peaks)): with gauss_models[i]: traces.append(pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True)) t1 = time() timepymc3 = t1-t0 # time taken to sample all of the photopeaks print('{} seconds ({} seconds per peak) taken to run PyMC3 sampling.'.format(timepymc3, timepymc3/6)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling.","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data-alternate-method","text":"The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probabillity distribution in log-space, meaning the standard deviation has an equal probabillity of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using simillar code as before, but with the new likelihood. gauss_model_alt = pm.Model() with gauss_model_alt: # set prior parameters # amplitude a_mu = a_guesses[1] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[1] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses[1] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = -1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Normal('Standard Deviation', mu=w_mu, sd=w_sig) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # set uniform prior sigma_model = pm.Uniform('Noise', lower=sigma_min, upper=sigma_max) # Expected value of outcome mu = gauss(peaks_x[1], a_model, xc_model, w_model, y0_model) # Normal likelihood of observations with noise Y_obs = pm.Normal('Y_obs', mu=mu, sd=10 ** sigma_model, observed=peaks_y[1]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time() with gauss_model_alt: trace_alt = pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True) t1_alt = time() timepymc3_alt = t1_alt-t0_alt print('{} seconds taken to run PyMC3 alternate sampling.'.format(timepymc3_alt)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np.vstack((trace_alt['Amplitude'], trace_alt['Peak Energy'], trace_alt['Standard Deviation'], trace_alt['Background Counts'], trace_alt['Noise'])).T # mean and standard deviation error of each parameter a_alt, a_err_alt = np.mean(samples_alt[:,0]), np.std(samples_alt[:,0]) xc_alt, xc_err_alt = np.mean(samples_alt[:,1]), np.std(samples_alt[:,1]) w_alt, w_err_alt = np.mean(samples_alt[:,2]), np.std(samples_alt[:,2]) y0_alt, y0_err_alt = np.mean(samples_alt[:,3]), np.std(samples_alt[:,3]) sigma_alt, sigma_err_alt = np.mean(samples_alt[:,4]), np.std(samples_alt[:,4]) # print values print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_alt, a_err_alt) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_alt, xc_err_alt) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_alt, w_err_alt) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_alt, y0_err_alt) + ' Noise = {} \\u00B1 {} counts'.format(sigma_alt, sigma_err_alt)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts","title":"Sampling the data - Alternate method"},{"location":"PyMC3_GRS/PyMC3_GRS/#results","text":"Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [np.vstack((trace['Amplitude'], trace['Peak Energy'], trace['Standard Deviation'], trace['Background Counts'])).T for trace in traces] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range(len(peaks))] for ind in range(len(peaks)): resdict[ind]['a_mu'] = np.mean(all_pymc3_samples[ind][:, 0]) resdict[ind]['a_sig'] = np.std(all_pymc3_samples[ind][:, 0]) resdict[ind]['xc_mu'] = np.mean(all_pymc3_samples[ind][:, 1]) resdict[ind]['xc_sig'] = np.std(all_pymc3_samples[ind][:, 1]) resdict[ind]['w_mu'] = np.mean(all_pymc3_samples[ind][:, 2]) resdict[ind]['w_sig'] = np.std(all_pymc3_samples[ind][:, 2]) resdict[ind]['y0_mu'] = np.mean(all_pymc3_samples[ind][:, 3]) resdict[ind]['y0_sig'] = np.std(all_pymc3_samples[ind][:, 3]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts(samples, labels, **kwargs): \"\"\" Function to plot posteriors using corner.py and scipy's gaussian KDE function. \"\"\" fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) plt.subplots_adjust(wspace=0.2, hspace=0.2) # plot KDE smoothed version of distributions for axidx, samps in zip([0, 5, 10, 15], samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 100) fig.axes[axidx].plot(xvals, kde(xvals), color=\"firebrick\") # create corner plot for peak with noisiest data labels = [r'Amplitude', r'Peak Energy', r'Standard Deviation', r'Background Counts'] corner_plot_samples = all_pymc3_samples[1] plotposts(corner_plot_samples, labels) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a, a_err = resdict[1]['a_mu'], resdict[1]['a_sig'] xc, xc_err = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w, w_err = resdict[1]['w_mu'], resdict[1]['w_sig'] y0, y0_err = resdict[1]['y0_mu'], resdict[1]['y0_sig'] print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a, a_err) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc, xc_err) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w, w_err) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0, y0_err)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior","text":"Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig, axs = plt.subplots(2, 3, figsize=(12, 7)) for i in range(2): for j in range(3): ind = 3 * i + j a = resdict[ind]['a_mu'] xc = resdict[ind]['xc_mu'] w = resdict[ind]['w_mu'] y0 = resdict[ind]['y0_mu'] x = peaks_x[ind] y = peaks_y[ind] # plot original data axs[i, j].plot(x, y, 'b.', alpha=1, label=('Original Data' if all(num == 0 for num in [i,j]) else '')) # plot fitted curve over the data xsmooth = np.linspace(x[0], x[-1], len(x) * 100) axs[i, j].plot(xsmooth, gauss(xsmooth, a, xc, w, y0), 'k:', alpha=1, label=('Fitted Model' if all(num == 0 for num in [i,j]) else '')) if i == 1: axs[i, j].set_xlabel('Energy / keV') if j == 0: axs[i, j].set_ylabel('Counts') leg = fig.legend(loc='lower right', numpoints=1) for lh in leg.legendHandles: lh.set_alpha(1) fig.suptitle('Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling') plt.show() Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3, xc_samps_pymc3, w_samps_pymc3, y0_samps_pymc3 = ([] for i in range(4)) for ind in range(len(peaks)): a_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 0], size=n_fits)) xc_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 1], size=n_fits)) w_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 2], size=n_fits)) y0_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 3], size=n_fits)) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x[ind] xsmooth = np.linspace(x[0], x[-1], len(x) * 100) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4)) for i in range(n_fits): ax1.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.01, linewidth=2) ax2.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.02, linewidth=2) ax1.set_ylabel('Counts') ax1.set_xlabel('Energy / keV') ax2.set_xlim(159.4,161.8) ax2.set_ylim(1800, 2250) ax2.set_xlabel('Energy / keV') fig.suptitle('Posterior predictive plot for a photopeak from a sample of Ba-133') plt.show()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons","text":"Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak: # parameter means and standard deviations of peak a_pymc3, aerr_pymc3 = resdict[1]['a_mu'], resdict[1]['a_sig'] xc_pymc3, xcerr_pymc3 = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w_pymc3, werr_pymc3 = resdict[1]['w_mu'], resdict[1]['w_sig'] y0_pymc3, y0err_pymc3 = resdict[1]['y0_mu'], resdict[1]['y0_sig'] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate.quad(lambda t: gauss(t, a_pymc3, xc_pymc3, w_pymc3, y0_pymc3), 159.1, 162.2)[0] / c_0 peak_integral_err = np.sqrt(2 * np.pi * ((w_pymc3 * aerr_pymc3) ** 2 + (a_pymc3 * werr_pymc3) ** 2 )) / c_0 percent_err = 100*peak_integral_err/peak_integral print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral, peak_integral_err) + 'Percentage error = {}%'.format(percent_err)) Total counts = 22933.86204613347 \u00b1 215.58699451020811 counts Percentage error = 0.9400378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate.quad(lambda t: gauss(t, a_alt, xc_alt, w_alt, y0_alt), 159.1, 162.2)[0] / c_0 peak_integral_err_alt = np.sqrt(2 * np.pi * ((w_alt * a_err_alt) ** 2 + (a_alt * w_err_alt) ** 2 )) / c_0 percent_err_alt = 100*peak_integral_err_alt/peak_integral_alt print('Alternate total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_alt, peak_integral_err_alt) + 'Alternate percentage error = {}%'.format(percent_err_alt)) Alternate total counts = 22943.751210742565 \u00b1 216.76313608385547 counts Alternate percentage error = 0.9447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-dynesty","text":"Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they can do a better job. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here.","title":"Modelling with dynesty"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data_1","text":"Using dynesty is slightly more complicated than PyMC3. Nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform(theta): # unpack the transformed parameters a_t, xc_t, w_t, y0_t = theta # define our prior guesses for each parameter a_mu, a_sig = a_guesses[1], 50. xc_mu, xc_sig = xc_guesses[1], 1. w_mu, w_sig = 0.7, 0.3 y0_mu, y0_sig = y0_guesses[1], 30. # convert back to a = a_mu + a_sig*ndtri(a_t) xc = xc_mu + xc_sig*ndtri(xc_t) w = w_mu + w_sig*ndtri(w_t) y0 = y0_mu + y0_sig*ndtri(y0_t) return a,xc,w,y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike(theta): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like, xc_like, w_like, y0_like = theta # expected value lmbda = np.array(gauss(peaks_x[1], a_like, xc_like, w_like, y0_like)) n = len(peaks_y[1]) a = np.sum(gammaln(np.array(peaks_y[1])+1)) b = np.sum(np.array(peaks_y[1]) * np.log(lmbda)) return -np.sum(lmbda) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points, sampling algorithm, sampling method, and a stopping criterion. Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler(loglike, priortransform, nparam, bound='multi', sample='unif', nlive=1000) t0_dynesty = time() sampler.run_nested(dlogz=stop, print_progress=False) t1_dynesty = time() print('{} seconds taken to run dynesty sampling'.format(t1_dynesty-t0_dynesty)) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler.results print(results.summary()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#results_1","text":"To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np.exp(results['logwt'] - results['logz'][-1]) samples = results.samples dynesty_samples = resample_equal(samples, weights) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a, xc, w, y0 = [dynesty_samples[:,i] for i in range(4)] a_dynesty, aerr_dynesty = np.mean(a), np.std(a) xc_dynesty, xcerr_dynesty = np.mean(xc), np.std(xc) w_dynesty, werr_dynesty = np.mean(w), np.std(w) y0_dynesty, y0err_dynesty = np.mean(y0), np.std(y0) nfits = 300 a_samps_dynesty = np.random.normal(a_dynesty, aerr_dynesty, nfits) xc_samps_dynesty = np.random.normal(xc_dynesty, xcerr_dynesty, nfits) w_samps_dynesty = np.random.normal(w_dynesty, werr_dynesty, nfits) y0_samps_dynesty = np.random.normal(y0_dynesty, y0err_dynesty, nfits) print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_dynesty, aerr_dynesty) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_dynesty, xcerr_dynesty) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_dynesty, werr_dynesty) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_dynesty, y0err_dynesty)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior_1","text":"So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'b.') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'k:') ax1.set_ylim(1200) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01) ax2.set_ylim(1200) plt.suptitle('Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of\\n' + ' Ba-133, sampled by dynesty') plt.show() Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'k.',label='Data') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'b:',label='dynesty') ax1.plot(xsmooth, gauss(xsmooth,a_pymc3,xc_pymc3,w_pymc3,y0_pymc3),'r:',label='PyMC3') ax1.set_ylim(1200) leg1 = ax1.legend(loc='upper right') for lh in leg1.legendHandles: lh.set_alpha(1) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01, label=('dynesty' if i == 0 else '')) ax2.plot(xsmooth, gauss(xsmooth,a_samps_pymc3[1][i],xc_samps_pymc3[1][i], w_samps_pymc3[1][i],y0_samps_pymc3[1][i]),'r-',alpha=0.01, label=('PyMC3' if i == 0 else '')) ax2.set_ylim(1200) leg2 = ax2.legend(loc='upper right') for lh in leg2.legendHandles: lh.set_alpha(1) plt.suptitle('Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + '\\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)') plt.show()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons_1","text":"This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate.quad(lambda t: gauss(t, a_dynesty, xc_dynesty, w_dynesty, y0_dynesty), 159.1, 162.2)[0] / c_0 peak_integral_err_dynesty = np.sqrt(2 * np.pi * ((w_dynesty * aerr_dynesty)** 2 + (a_dynesty * werr_dynesty) ** 2 )) / c_0 percent_err_dynesty = 100*peak_integral_err_dynesty/peak_integral_dynesty print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_dynesty, peak_integral_err_dynesty) + 'Percentage error = {}%'.format(percent_err_dynesty)) Total counts = 22968.854117562147 \u00b1 224.93466521968398 counts Percentage error = 0.9793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error around 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"Model comparisons"},{"location":"Sunspots2/Sunspots2/","text":"Using Zeus and Nestle to model the sunspot cycles A sunspot is an area on the surface of the sun that appears a lot darker than it's surroundings, caused by intense magnetic fields regulating a convection effect on the Sun's surface. Humanity has been tracking the average number of sunspots visible from Earth on the solar disk, and found that the sunspot number follows a cycle over roughly 11 years. In this example, I'll use the \"Zeus\" and \"Nestle\" samplers to fit a model describing the sunspot number to a single solar cycle. I'll then use this model to create a new model which predicts the properties of a solar cycle, given the properties of the previous cycle. Useful imports # numpy import numpy as np # pandas import pandas as pd # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import zeus print('Zeus version: {}'.format(zeus.__version__)) import nestle print('Nestle version: {}'.format(nestle.__version__)) # misc import logging from time import time Zeus version: 1.0.7 Nestle version: 0.2.0 Viewing the data Using Pandas, we can load the .csv file containing the average sunspot number over 24 solar cycles, since the 1750s. First I will plot the entire dataset to decide which cycles are of interest, however before plotting I'll first take the square root of the sunspot number. This is just to decrease the variance in the peak height, which will make everything a little easier when it comes to making a model that can predict the height of the next peak. dataframe = pd.read_csv(\"SN_m_tot_V2.0.csv\", sep=\";\", usecols=[2,3], header=None) dates = list(dataframe[2])[80:] ssn = list(dataframe[3])[80:] sqrtssn = np.sqrt(ssn) plt.figure(figsize=(16,3)) plt.plot(dates,sqrtssn) plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Average sunspot number from 1750-2020\") plt.show() I chose to look at the 4 solar cycles starting around the year 1924, as there seems to be a consistent (predictable!) change from peak to peak. Next, we need to take a closer look at those 4 solar cycles so that we can create a model. start = 2020 # data point corresponding the the start of the 1924 solar cycle period = 123 # average width of a solar cycle fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,3)) # plot region of interest ax1.plot(dates[start:start + 4*period],sqrtssn[start:start + 4*period]) ax1.set_xlabel(\"Date / years\") ax1.set_ylabel(\"Square Root of Sunspot Number\") ax1.set_title(\"Average sunspot number from 1924-1965\") # fragment the 4 solar cycles into 4 separate lists peaks = [sqrtssn[start + i*period:start + (i+1)*period] for i in range(4)] peaktimes = [dates[start + i*period:start + (i+1)*period] for i in range(4)] # plot a typical solar cycle ax2.plot(peaktimes[0],peaks[0]) ax2.set_xlabel(\"Date / years\") ax2.set_title(\"A typical solar cycle\") plt.show() The model We can create the model by splitting it into two parts: the ascending region, and the descending region. To define these regions, we need to know when the cycle begins, when the cycle peaks, and when the cycle ends. We'll call these \"t0\", \"tmax\", and \"t1\" respectively. Next, we need to know the amplitude of the peak, which we will call \"c\". Finally, we need 2 more parameters which will describe how much the ascending and descending regions curve, which we will call \"a1\", and \"a2\". Using all of these, we can define our model for a single solar cycle below: def cycle(times, c, a1, a2, t0, tmax, t1): # a1,a2 > 1 if a1 < 1: a1 = 1 if a2 < 1: a2 = 1 #t0 < tmax < t1 if t0 >= tmax: t0 = tmax - 1 if t1 <= tmax: t0 = tmax + 1 # sunspot number as a function of time ssn = [c*(1-((tmax-t)/(tmax-t0))**a1) if t < tmax else c*(1-((t-tmax)/(t1-tmax))**a2) for t in times] # ssn > 0 ssn_non_negative = [i if i > 0 else 0.1 for i in ssn] return ssn_non_negative I'll take the first solar cycle (starting 1924), and use some guesses for each parameter to overplot a model, just to give an idea of what the model will look like. # plot cycle data plt.plot(peaktimes[0],peaks[0],label=\"Original Data\") # plot an example model, with guessed parameters x = np.linspace(min(peaktimes[0]),max(peaktimes[0]),300) y = cycle(x,11,2.1,1.3,1923.5,1927.5,1935) plt.plot(x,y,label=\"Example Model\") plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"A typical solar cycle\") plt.legend() plt.show() Modelling with Zeus Zeus has a very simillar interface to the sampler \"emcee\", which uses an ensemble technique to obtain the prior distributions. The next step is defining the prior distributions for each parameter. I'll use a normal distribution for the peak amplitude \"c\", since it's pretty easy to eyeball. Aside from that, I'll be using uniform priors for every other parameter as they're a little trickier to guess. nens = 100 # number of ensemble points ndims = 6 # number of parameters # mean and standard deviation of normal parameter priors cmu,csig = 11,1 # lower and upper bounds of uniform parameter priors a1min,a1max = 1,3 a2min,a2max = 1,3 t0min,t0max = 1922,1925 tmaxmin,tmaxmax = 1926,1929 t1min,t1max = 1933,1936 param_priors = [] # normal prior on c param_priors.append(np.random.normal(cmu,csig,nens)) # uniform prior on a1 param_priors.append(np.random.uniform(a1min,a1max,nens)) # uniform prior on a2 param_priors.append(np.random.uniform(a2min,a2max,nens)) # uniform prior on t0 param_priors.append(np.random.uniform(t0min,t0max,nens)) # uniform prior on tmax param_priors.append(np.random.uniform(tmaxmin,tmaxmax,nens)) # uniform prior on t1 param_priors.append(np.random.uniform(t1min,t1max,nens)) param_samples = np.array(param_priors).T Next, we need to define a log prior, log likelihood, and log posterior. The log prior can be defined as below: def logprior(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range(len(param_priors)): # sum log priors from each parameter if i == 0: # normal priors lprior -= 0.5*((theta[i] - cmu) / csig)**2 else: # uniform priors # set bounds if i == 1: low, up = a1min, a1max elif i == 2: low, up = a2min, a2max elif i == 3: low, up = t0min,t0max elif i == 4: low, up = tmaxmin,tmaxmax else: low, up = t1min,t1max # parameter must be between bounds if low < theta[i] < up: pass else: lprior = -np.inf return lprior The log likelihood takes the form of a Poisson likelihood, since the sunspot counts are non-negative. using \"lmbda\" as the expected value of the cycle function, we can define the likelihood as below: def loglike(theta, times, obs): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # unpack parameters c_like, a1_like, a2_like, t0_like, tmax_like, t1_like = theta # expected value lmbda = np.array(cycle(times, c_like, a1_like, a2_like, t0_like, tmax_like, t1_like)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b Finally the log posterior is simply the sum of the log prior log likelihood: def logposterior(theta, times, obs): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta, times, obs) Sampling the data Now that we've defined everything we need, we can easily run the sampling process with Zeus. # create sampler using the first peak sampler = zeus.sampler(nens, ndims, logposterior, args=[peaktimes[0], peaks[0]]) nburn = 500 # burn-in points nsamples = 500 # points after burn-in time0 = time() sampler.run_mcmc(param_samples, nburn + nsamples) time1 = time() print(\"Time taken to sample first peak with Zeus: {} seconds\".format(time1-time0)) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:12<00:00, 13.84it/s] Time taken to sample first peak with Zeus: 72.26702880859375 seconds Results Collecting the samples after the sampling is equally simple, remembering to discard the burn-in samples at the start of the chain. We can create a corner plot, which shows the posteriors of each parameter along with contour plots describing how one parameter varies with any other: samples_zeus = sampler.get_chain(flat=True, discard=nburn) def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['c', 'a1', 'a2', 't0', 'tmax', 't1'] plotposts(samples_zeus, labels) Next, I'll find the means and standard deviation error on each parameter. I'll also choose random samples from the chains of each parameter, which I'll use to visualise the posteriors. param_mu = [np.mean(samples_zeus[:,i]) for i in range(6)] param_sig = [np.std(samples_zeus[:,i]) for i in range(6)] nfits = 300 param_samples = [np.random.choice(samples_zeus[:,i],nfits) for i in range(6)] post_samples = np.array(param_samples).T print(\"Parameters describing the square root SSN cycle starting 1923: \\n \\n\" + \" c = {} \\u00B1 {} square root counts\\n\".format(param_mu[0], param_sig[0]) + \" a1 = {} \\u00B1 {} \\n\".format(param_mu[1], param_sig[1]) + \" a2 = {} \\u00B1 {} \\n\".format(param_mu[2], param_sig[2]) + \" t0 = {} \\u00B1 {} years\\n\".format(param_mu[3], param_sig[3]) + \"tmax = {} \\u00B1 {} years\\n\".format(param_mu[4], param_sig[4]) + \" t1 = {} \\u00B1 {} years\\n\".format(param_mu[5], param_sig[5])) Parameters describing the square root SSN cycle starting 1923: c = 11.235859600042037 \u00b1 0.5215026875027372 square root counts a1 = 2.0512996667503938 \u00b1 0.5507731307368571 a2 = 1.3134491063165001 \u00b1 0.2501776677187202 t0 = 1923.4484201628645 \u00b1 0.3547836069094943 years tmax = 1927.5792309869332 \u00b1 0.6491606607901439 years t1 = 1935.0575941486698 \u00b1 0.31425239983590497 years Plotting the posterior Below I'll show two plots. The left plot will show the model produced using the mean parameters for the first peak. The right plot will show a posterior predictive plot, where the darker the colour of the plot, the higher the probabillty of the model passing through that region. fig, (ax1,ax2) = plt.subplots(1,2,figsize=(13,3)) # mean posterior plot ax1.plot(peaktimes[0], peaks[0]) x = np.linspace(min(peaktimes[0]), max(peaktimes[0]), 300) y = cycle(x,*param_mu) ax1.plot(x,y) ax1.set_xlabel(\"Date / years\") ax1.set_ylabel(\"square root counts\") ax1.set_title(\"Mean posterior plot for SSN cycle starting 1856\") # posterior predictive plot x = np.linspace(min(peaktimes[0]),max(peaktimes[0]),300) ax2.plot(peaktimes[0], peaks[0], label=\"Origina Data\") for i in range(len(post_samples)): params = post_samples[i] y = cycle(x,*params) ax2.plot(x,y,'orange',alpha=0.02, linewidth=3,label=\"Fitted Model\" if i == 0 else \"\") ax2.set_xlabel(\"Date / years\") ax2.set_title(\"Posterior predictive plot for SSN cycle starting 1856\") leg = ax2.legend() for lh in leg.legendHandles: lh.set_alpha(1) plt.show() Predicting the next solar cycles We now know the properties of the solar cycle between 1924-1934. Next, we want to know the properties of the next three cycles in our region of interest. We could just redefine our prior distributions, and run the sampling again for each peak. Alternatively, we could create a new model which takes the current solar cycle parameters, and uses them to predict the next solar cycle. This model will only, realisticly, be able to make long range predictions if the solar cycles evolve consistently, as mentiioned above. This way this model is put together is quite complicated. More details can be found here , but the brief is that we need 3 scaling parameters (y1,y2,y3), and 4 translation parameters (d0,d1,d2,d3). This model can be implemented as follows: def predict_cycle(theta_prev, d0, d1, d2, d3, y1, y2, y3): c_prev, a1, a2, t0_prev, tmax_prev, t1_prev = theta_prev # start of cycle found using slight deviation d0 t0 = t1_prev + d0 # current c found using current t0 and previous c,tmax if t0 <= tmax_prev: t0 = tmax_prev+5 c = y1*c_prev/(t0 - tmax_prev) + d1 # current tmax found using current t0,c tmax = t0 + y2*c + d2*(t0-param_mu[4])**0.9 # current t1 found using current tmax,c t1 = tmax + y3*c + d3*(t0-param_mu[4])**0.3 #a1,a2 unchanged # return new set of parameters theta_new = (c,a1,a2,t0,tmax,t1) return theta_new We can now try to guess the scaling and translational parameters, and using the set of parameters defining the first solar cycle, we can try to predict the second solar cycle between 1934-1944. I'll show this by plotting the data for the second solar cycle, and overplotting the predicted model. After some trial and error, I found the following prediction: # predict the next peak parameters, using current peak parameters params = predict_cycle(param_mu,-1,0,-0.4,0.2,7.5,0.5,0.5) # plot second solar cycle plt.plot(peaktimes[1],peaks[1],label=\"Original Data\") # overplot predicted model with new parameters x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) plt.plot(x,cycle(x,*params),label=\"Predicted model\") plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Average square root sunspot number between 1934-1944 \\n with overplotted predicted model\") plt.legend() plt.show() Using the above guesses, we can define some prior distributions. This is done in exactly the same way as above, however due to some hard limits on the parameters (such as a1,a2 being strictly greater than 1), I'll use uniform priors for all parameters. nens = 100 ndims = 7 d0min,d0max = -2.,0 d1min,d1max = -1.,1. d2min,d2max = -1.,0. d3min,d3max = 0.,0.4 y1min,y1max = 7.,8. y2min,y2max = 0.2,0.8 y3min,y3max = 0.25,0.75 predict_priors = [] # uniform prior on d0 predict_priors.append(np.random.uniform(d0min,d0max,nens)) # uniform prior on d1 predict_priors.append(np.random.uniform(d1min,d1max,nens)) # uniform prior on d2 predict_priors.append(np.random.uniform(d2min,d2max,nens)) # uniform prior on d3 predict_priors.append(np.random.uniform(d3min,d3max,nens)) # uniform prior on y1 predict_priors.append(np.random.uniform(y1min,y1max,nens)) # uniform prior on y2 predict_priors.append(np.random.uniform(y2min,y2max,nens)) # uniform prior on y3 predict_priors.append(np.random.uniform(y3min,y3max,nens)) priors = [(d0min,d0max),(d1min,d1max),(d2min,d2max),(d3min,d3max), (y1min,y1max),(y2min,y2max),(y3min,y3max)] predict_samples = np.array(predict_priors).T We need to create new log prior, log likelihood, and log posterior functions. This is done almost exactly as above, but with very slight variations, so I'll won't go into detail about the machinery in these functions. def loglike_predict(theta, times, obs): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle(param_mu, *theta) lmbda = np.array(cycle(times, *params_new)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b def logprior_predict(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range(len(predict_priors)): # sum log priors from each parameter low, up = priors[i][0],priors[i][1] # uniform prior for time parameters if low < theta[i] < up: pass else: lprior = -np.inf return lprior def logposterior_predict(theta, times, obs): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior_predict(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_predict(theta, times, obs) Running the sampler, using the same burn-in and chain lengths as above, we can tune the parameters so they predict the parameters of the second solar cycle well. We'll then assume that these parameters are the same for the third and fourth peaks, and see how good the quality of the fit is with increasing forecast time. sampler = zeus.sampler(nens, ndims, logposterior_predict, args=[peaktimes[1], peaks[1]]) nburn = 500 nsamples = 500 time0 = time() sampler.run_mcmc(predict_samples, nburn + nsamples) time1 = time() print(\"Time taken to sample predict function with Zeus: {} seconds\".format(time1-time0)) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:16<00:00, 13.10it/s] Time taken to sample predict function with Zeus: 76.33164262771606 seconds Predictions Out of curiosity, we can check the corner plot for the prediction model. Since the prediction model has every parameter interacting with eachother, we can expect a load of covariance between parameters. samples_zeus2 = sampler.get_chain(flat=True, discard=nburn) predict_param_mu = [np.mean(samples_zeus2[:,i]) for i in range(7)] predict_param_sig = [np.std(samples_zeus2[:,i]) for i in range(7)] nfits = 300 predict_param_samples = [np.random.choice(samples_zeus2[:,i],nfits) for i in range(7)] predict_post_samples = np.array(predict_param_samples).T labels = ['d0','d1','d2','d3','y1', 'y2', 'y3'] plotposts(samples_zeus2,labels) Lets check what predictions we can now make. The below code takes the set of parameters describing the first solar cycle, and uses it to predict the second solar cycle. The prediction will be plotted over the data, and then the program will move on to the next peak and repeat the process. Since we're making predictions on using predictions, over and over, we can expect the fit to get worse as we look deeper into the future. I'll also use the samples from the prediction model posteriors, and create a posterior predictive plot for each cycle. This should show the quality of the prediction, and how it evolves with increasing forecast time. fig,axs = plt.subplots(3,2,figsize=(10,10)) # predict second peak parameters # plot mean posterior plot new_param_mu = predict_cycle(param_mu,*predict_param_mu) axs[0,0].plot(peaktimes[1],peaks[1]) x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) axs[0,0].plot(x,cycle(x,*new_param_mu)) axs[0,0].set_ylabel(\"Square Root of Sunspot Number\") axs[0,0].set_title(\"Average sunspot number for the solar cycles\\n between 1933-1965\" + \" with overfitted\\n mean predicted models\") # posterior predictive plot axs[0,1].plot(peaktimes[1],peaks[1]) new_pred = [] for i in range(len(predict_post_samples)): pred = predict_cycle(param_mu,*predict_post_samples[i]) new_pred.append(pred) y = cycle(x,*pred) axs[0,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) axs[0,1].set_title(\"Average sunspot number for the solar cycles\\n between 1933-1965\" + \" with overfitted\\n posterior predictive plots\") # predict third peak parameters # plot mean posterior plot new_param_mu = predict_cycle(new_param_mu, *predict_param_mu) axs[1,0].plot(peaktimes[2],peaks[2]) x = np.linspace(min(peaktimes[2]),max(peaktimes[2]),300) axs[1,0].plot(x,cycle(x,*new_param_mu)) axs[1,0].set_ylabel(\"Square Root of Sunspot Number\") axs[1,0].set_xlabel(\"Date / years\") # posterior predictive plot axs[1,1].plot(peaktimes[2],peaks[2]) new_pred2 = [] for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred[i],*predict_param_mu) new_pred2.append(pred) y = cycle(x,*pred) axs[1,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) # predict fourth peak parameters # plot mean posterior plot new_param_mu = predict_cycle(new_param_mu, *predict_param_mu) axs[2,0].plot(peaktimes[3],peaks[3],label=\"Original Data\") x = np.linspace(min(peaktimes[3]),max(peaktimes[3]),300) axs[2,0].plot(x,cycle(x,*new_param_mu),label=\"Predicted Model\") axs[2,0].set_ylabel(\"Square Root of Sunspot Number\") axs[2,0].set_xlabel(\"Date / years\") axs[2,0].legend() # posterior predictive plot axs[2,1].plot(peaktimes[3],peaks[3]) for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred2[i],*predict_param_mu) y = cycle(x,*pred) axs[2,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) axs[2,1].set_xlabel(\"Date / years\") plt.show() Saying that our prediction model was fairly simplified, it actually does a pretty good job at predicting the solar cycle shape. The posterior plots show the predictions getting worse over time as expected, but even so the accuracy on the mean plots are within 20% for the majority of the cycles. Modelling with Nestle The predictions from this model only work if we know in advance that the next solar cycle will carry on the trend from the previous solar cycle. Since in reality it is very hard to say how the Sun will behave in advance, this model won't be all that useful. However, it did save us from having to individually sampling 3 solar cycles. To decide whether or not the time save is worth the loss in accuracy, I'll use \"Nestle\" to sample both the second solar cycle (between 1934-1945), and the prediction model for that same solar cycle. Nestle uses nested sampling, and so it produces a value of the marginalised evidence. Comparing these values will allow us to see how much accuracy we're sacrificing by predicting the solar cycle, instead of just sampling the next peak. Sampling the data To start, we'll assume that Zeus did a good enough job at sampling the first solar cycle, and jump straight to the second cycle (1934-1945). Lets define some new guesses for each parameter (c,a1,a2,t0,tmax,t1): # mean and standard deviation of normal parameter priors cmu,csig = 13,1 # lower and upper bounds of uniform parameter priors a1min,a1max = 1,3 a2min,a2max = 1,3 t0min,t0max = 1931,1935 tmaxmin,tmaxmax = 1936,1940 t1min,t1max = 1943,1947 param_priors = [(cmu,csig),(a1min,a1max),(a2min,a2max), (t0min,t0max),(tmaxmin,tmaxmax),(t1min,t1max)] We can reuse most of the stuff we had from before, except for the log prior functions. Nestle samples from a unit hypercube parameter space, so we need a function that transforms the priors back to their original space. This only requires a slight modification to our previous \"logprior\" function, making use of scipy's ndtri function. I'll also make a very minor change to the log likelihood function. Instead of taking the dates and sunspot counts as arguments, it will use the data for the second solar cycle by default. def prior_transform(theta): \"\"\" Function to transform the parameters from unit hypercube to their true form \"\"\" trans_params = [] # transform normal prior trans_params.append(param_priors[0][0] + param_priors[0][1]*ndtri(theta[0])) # transform uniform prior for i in range(1,6): mini, maxi = param_priors[i][0], param_priors[i][1] trans_params.append(theta[i]*(maxi-mini) + mini) return trans_params def loglike_nestle(theta): \"\"\" Function to return the log likelihood, with data fixed for solar cycle between 1934-1945 \"\"\" obs, times = peaks[1], peaktimes[1] # unpack parameters c_like, a1_like, a2_like, t0_like, tmax_like, t1_like = theta # expected value lmbda = np.array(cycle(times, c_like, a1_like, a2_like, t0_like, tmax_like, t1_like)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b We're now ready to run the sampling using Nestle: # set number of dimensions, live points, sampling method, and stopping criterion ndims = 6 nlive = 1024 method = 'multi' stop = 0.1 time0 = time() results_sample = nestle.sample(loglike_nestle, prior_transform, ndims, method=method, npoints=nlive, dlogz=stop) time1 = time() print(\"Time taken to sample second solar cycle with Nestle: {} seconds\".format(time1-time0)) Time taken to sample second solar cycle with Nestle: 18.07349967956543 seconds We'll now try the sampling again, but this time we'll use the parameters from the first solar cycle (found using Zeus), and sample the parameters of the prediction model. Start by guessing at the parameters for the prediction model: # reuse our guesses from Zeus d0min,d0max = -2.,0 d1min,d1max = -1.,1. d2min,d2max = -1.,0. d3min,d3max = 0.,0.4 y1min,y1max = 7.,8. y2min,y2max = 0.2,0.8 y3min,y3max = 0.25,0.75 predict_priors = [(d0min,d0max),(d1min,d1max),(d2min,d2max),(d3min,d3max), (y1min,y1max),(y2min,y2max),(y3min,y3max)] We have to tinker with out prediction model's likelihood and prior functions. This is done exactly as it was previously. def loglike_nestle_predict(theta): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" obs,times = peaks[1],peaktimes[1] # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle(param_mu, *theta) lmbda = np.array(cycle(times, *params_new)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b def prior_transform_predict(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" trans_priors = [] for i in range(len(predict_priors)): # sum log priors from each parameter mini, maxi = predict_priors[i][0],predict_priors[i][1] # uniform prior for time parameters trans_priors.append(theta[i]*(maxi-mini)+mini) return trans_priors Now that everything is set up, I'll run through the same sampling process, using the same hyper-parameters for fairness. # set number of dimensions, live points, sampling method, and stopping criterion ndims = 7 nlive = 1024 method = 'multi' stop = 0.1 time0 = time() results_predict = nestle.sample(loglike_nestle_predict, prior_transform_predict, ndims, method=method, npoints=nlive, dlogz=stop) time1 = time() print(\"Time taken to sample prediction model with Nestle: {} seconds\".format(time1-time0)) Time taken to sample prediction model with Nestle: 14.57199764251709 seconds Results Next, we find the log of the marginalised evidence provided by Nestle. This can be done as follows, using the information gain to estimate the error: logZ_sample = results_sample.logz logZerr_sample = np.sqrt(results_sample.h/nlive) logZ_predict = results_predict.logz logZerr_predict = np.sqrt(results_predict.h/nlive) print(\"log(Z) from sampling the solar cycle = {} \u00b1 {}\".format(logZ_sample, logZerr_sample)) print(\"log(Z) from predicting the solar cycle = {} \u00b1 {}\".format(logZ_predict, logZerr_predict)) log(Z) from sampling the solar cycle = -266.7678218108872 \u00b1 0.07474929979047762 log(Z) from predicting the solar cycle = -266.69101856955064 \u00b1 0.07202857357654911 The Bayes factor is a metric that describes how much more likely a model is to produce an observed data set. It's defined as the ratios between the marginalised evidences of the two models: K = np.exp(logZ_predict - logZ_sample) print(\"Bayes factor: {}\".format(K)) Bayes factor: 1.0798295896621817 This tells us that sampling via the prediction method rather than just sampling the peak doesn't come at a cost to the quality of the model. Since the methods produce simillar results, lets collect the samples from the prediction method to use for visualising our results. Since Nestle uses nested sampling, we have to resample with weights to obtain the posteriors. weights = results_predict.weights/np.max(results_predict.weights) mask = np.where(np.random.rand(len(weights)) < weights)[0] # collect posterior samples samples_nestle = results_predict.samples[mask,:] # collect posterior means predict_param_mu_nestle = [np.mean(samples_nestle[:,i]) for i in range(7)] # collect samples for posterior predictive plot nfits = 300 predict_param_samples_nestle = [np.random.choice(samples_nestle[:,i],nfits) for i in range(7)] predict_post_samples_nestle = np.array(predict_param_samples).T Plotting the posterior Let's start by making a comparison between the posteriors from Zeus and Nestle. Below I'll create two plots. The first will show the mean posterior predictions of the second solar cycle, from both Zeus and Nestle. The second plot will show the posterior predictive plots, for both samplers also. fig,(ax1,ax2) = plt.subplots(1,2,figsize=(13,4)) # plot mean posterior plot new_param_mu_zeus = predict_cycle(param_mu,*predict_param_mu) new_param_mu_nestle = predict_cycle(param_mu,*predict_param_mu_nestle) ax1.plot(peaktimes[1],peaks[1],label=\"Original Data\") x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) ax1.plot(x,cycle(x,*new_param_mu),\"orange\",label=\"Zeus Prediction\") ax1.plot(x,cycle(x,*new_param_mu_nestle),\"purple\",label=\"Nestle Prediction\") ax1.set_ylabel(\"Square Root of Sunspot Number\") ax1.set_xlabel(\"Date / years\") ax1.set_title(\"Average sunspot number for the solar cycles\\n between 1934-1945\" + \" with overfitted\\n mean predicted models\") ax1.legend() # posterior predictive plot ax2.plot(peaktimes[1],peaks[1]) for i in range(len(predict_post_samples)): pred = predict_cycle(param_mu,*predict_post_samples[i]) y = cycle(x,*pred) ax2.plot(x,y,\"orange\",alpha=0.04,linewidth=3) pred = predict_cycle(param_mu,*predict_post_samples_nestle[i]) y = cycle(x,*pred) ax2.plot(x,y,\"purple\",alpha=0.04,linewidth=3) ax1.set_ylabel(\"Date / years\") ax2.set_title(\"Average sunspot number for the solar cycles\\n between 1934-1945\" + \" with overfitted\\n posterior predictive plots\") plt.show() Whilst the posterior predictive plot looks a little messy, it tells us that the posterior predictives from Zeus and Nestle are difficult to tell apart, since they overlap so much. The mean posterior plot supports this, showing that the two samplers only start deviating on the descending limb of the cycle. Finally, lets plot a posterior predictive plot that shows the evolution of the prediction accuracy over time. Below is a plot that shows predictions for the entire range of 4 solar cycles between 1923 and 1965. plt.figure(figsize=(14,4)) # plot data between 1923 and 1965 plt.plot(dates[start:start + 4*period],sqrtssn[start:start + 4*period],label=\"Original Data\") # plot first cycle posterior predictive, using Zeus samples x = np.linspace(min(peaktimes[0])-20,max(peaktimes[0]),300) for i in range(len(post_samples)): params = post_samples[i] y = cycle(x,*params) plt.plot(x,y,\"purple\",alpha=0.02, linewidth=3,label=\"Fitted Model\" if i == 0 else \"\") # use first cycle posterior predictive to find second cycle posterior predictive new_pred = [] x = np.linspace(min(peaktimes[1])-20,max(peaktimes[1])+20,300) for i in range(len(predict_post_samples_nestle)): pred = predict_cycle(param_mu,*predict_post_samples_nestle[i]) new_pred.append(pred) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3,label=\"Predicted Model\" if i == 0 else \"\") # use second cycle posterior predictive to find third cycle posterior predictive x = np.linspace(min(peaktimes[2])-20,max(peaktimes[2])+20,300) new_pred2 = [] for i in range(len(predict_post_samples_nestle)): pred = predict_cycle(new_pred[i],*predict_param_mu_nestle) new_pred2.append(pred) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3) # use third cycle posterior predictive to find fourth cycle posterior predictive x = np.linspace(min(peaktimes[3])-20,max(peaktimes[3])+20,300) for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred2[i],*predict_param_mu_nestle) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3) plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Predictions of average sunspot number between 1934-1965,\\n\" + \" using the cycle between 1923-1934 to make predictions\") plt.xlim(1923,1967) plt.ylim(1) leg = plt.legend(loc=\"upper left\") for lh in leg.legendHandles: lh.set_alpha(1) plt.show()","title":"Solar Cycle Analysis with Zeus and Nestle"},{"location":"Sunspots2/Sunspots2/#using-zeus-and-nestle-to-model-the-sunspot-cycles","text":"A sunspot is an area on the surface of the sun that appears a lot darker than it's surroundings, caused by intense magnetic fields regulating a convection effect on the Sun's surface. Humanity has been tracking the average number of sunspots visible from Earth on the solar disk, and found that the sunspot number follows a cycle over roughly 11 years. In this example, I'll use the \"Zeus\" and \"Nestle\" samplers to fit a model describing the sunspot number to a single solar cycle. I'll then use this model to create a new model which predicts the properties of a solar cycle, given the properties of the previous cycle.","title":"Using Zeus and Nestle to model the sunspot cycles"},{"location":"Sunspots2/Sunspots2/#useful-imports","text":"# numpy import numpy as np # pandas import pandas as pd # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import zeus print('Zeus version: {}'.format(zeus.__version__)) import nestle print('Nestle version: {}'.format(nestle.__version__)) # misc import logging from time import time Zeus version: 1.0.7 Nestle version: 0.2.0","title":"Useful imports"},{"location":"Sunspots2/Sunspots2/#viewing-the-data","text":"Using Pandas, we can load the .csv file containing the average sunspot number over 24 solar cycles, since the 1750s. First I will plot the entire dataset to decide which cycles are of interest, however before plotting I'll first take the square root of the sunspot number. This is just to decrease the variance in the peak height, which will make everything a little easier when it comes to making a model that can predict the height of the next peak. dataframe = pd.read_csv(\"SN_m_tot_V2.0.csv\", sep=\";\", usecols=[2,3], header=None) dates = list(dataframe[2])[80:] ssn = list(dataframe[3])[80:] sqrtssn = np.sqrt(ssn) plt.figure(figsize=(16,3)) plt.plot(dates,sqrtssn) plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Average sunspot number from 1750-2020\") plt.show() I chose to look at the 4 solar cycles starting around the year 1924, as there seems to be a consistent (predictable!) change from peak to peak. Next, we need to take a closer look at those 4 solar cycles so that we can create a model. start = 2020 # data point corresponding the the start of the 1924 solar cycle period = 123 # average width of a solar cycle fig,(ax1,ax2) = plt.subplots(1,2,figsize=(12,3)) # plot region of interest ax1.plot(dates[start:start + 4*period],sqrtssn[start:start + 4*period]) ax1.set_xlabel(\"Date / years\") ax1.set_ylabel(\"Square Root of Sunspot Number\") ax1.set_title(\"Average sunspot number from 1924-1965\") # fragment the 4 solar cycles into 4 separate lists peaks = [sqrtssn[start + i*period:start + (i+1)*period] for i in range(4)] peaktimes = [dates[start + i*period:start + (i+1)*period] for i in range(4)] # plot a typical solar cycle ax2.plot(peaktimes[0],peaks[0]) ax2.set_xlabel(\"Date / years\") ax2.set_title(\"A typical solar cycle\") plt.show()","title":"Viewing the data"},{"location":"Sunspots2/Sunspots2/#the-model","text":"We can create the model by splitting it into two parts: the ascending region, and the descending region. To define these regions, we need to know when the cycle begins, when the cycle peaks, and when the cycle ends. We'll call these \"t0\", \"tmax\", and \"t1\" respectively. Next, we need to know the amplitude of the peak, which we will call \"c\". Finally, we need 2 more parameters which will describe how much the ascending and descending regions curve, which we will call \"a1\", and \"a2\". Using all of these, we can define our model for a single solar cycle below: def cycle(times, c, a1, a2, t0, tmax, t1): # a1,a2 > 1 if a1 < 1: a1 = 1 if a2 < 1: a2 = 1 #t0 < tmax < t1 if t0 >= tmax: t0 = tmax - 1 if t1 <= tmax: t0 = tmax + 1 # sunspot number as a function of time ssn = [c*(1-((tmax-t)/(tmax-t0))**a1) if t < tmax else c*(1-((t-tmax)/(t1-tmax))**a2) for t in times] # ssn > 0 ssn_non_negative = [i if i > 0 else 0.1 for i in ssn] return ssn_non_negative I'll take the first solar cycle (starting 1924), and use some guesses for each parameter to overplot a model, just to give an idea of what the model will look like. # plot cycle data plt.plot(peaktimes[0],peaks[0],label=\"Original Data\") # plot an example model, with guessed parameters x = np.linspace(min(peaktimes[0]),max(peaktimes[0]),300) y = cycle(x,11,2.1,1.3,1923.5,1927.5,1935) plt.plot(x,y,label=\"Example Model\") plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"A typical solar cycle\") plt.legend() plt.show()","title":"The model"},{"location":"Sunspots2/Sunspots2/#modelling-with-zeus","text":"Zeus has a very simillar interface to the sampler \"emcee\", which uses an ensemble technique to obtain the prior distributions. The next step is defining the prior distributions for each parameter. I'll use a normal distribution for the peak amplitude \"c\", since it's pretty easy to eyeball. Aside from that, I'll be using uniform priors for every other parameter as they're a little trickier to guess. nens = 100 # number of ensemble points ndims = 6 # number of parameters # mean and standard deviation of normal parameter priors cmu,csig = 11,1 # lower and upper bounds of uniform parameter priors a1min,a1max = 1,3 a2min,a2max = 1,3 t0min,t0max = 1922,1925 tmaxmin,tmaxmax = 1926,1929 t1min,t1max = 1933,1936 param_priors = [] # normal prior on c param_priors.append(np.random.normal(cmu,csig,nens)) # uniform prior on a1 param_priors.append(np.random.uniform(a1min,a1max,nens)) # uniform prior on a2 param_priors.append(np.random.uniform(a2min,a2max,nens)) # uniform prior on t0 param_priors.append(np.random.uniform(t0min,t0max,nens)) # uniform prior on tmax param_priors.append(np.random.uniform(tmaxmin,tmaxmax,nens)) # uniform prior on t1 param_priors.append(np.random.uniform(t1min,t1max,nens)) param_samples = np.array(param_priors).T Next, we need to define a log prior, log likelihood, and log posterior. The log prior can be defined as below: def logprior(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range(len(param_priors)): # sum log priors from each parameter if i == 0: # normal priors lprior -= 0.5*((theta[i] - cmu) / csig)**2 else: # uniform priors # set bounds if i == 1: low, up = a1min, a1max elif i == 2: low, up = a2min, a2max elif i == 3: low, up = t0min,t0max elif i == 4: low, up = tmaxmin,tmaxmax else: low, up = t1min,t1max # parameter must be between bounds if low < theta[i] < up: pass else: lprior = -np.inf return lprior The log likelihood takes the form of a Poisson likelihood, since the sunspot counts are non-negative. using \"lmbda\" as the expected value of the cycle function, we can define the likelihood as below: def loglike(theta, times, obs): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # unpack parameters c_like, a1_like, a2_like, t0_like, tmax_like, t1_like = theta # expected value lmbda = np.array(cycle(times, c_like, a1_like, a2_like, t0_like, tmax_like, t1_like)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b Finally the log posterior is simply the sum of the log prior log likelihood: def logposterior(theta, times, obs): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta, times, obs)","title":"Modelling with Zeus"},{"location":"Sunspots2/Sunspots2/#sampling-the-data","text":"Now that we've defined everything we need, we can easily run the sampling process with Zeus. # create sampler using the first peak sampler = zeus.sampler(nens, ndims, logposterior, args=[peaktimes[0], peaks[0]]) nburn = 500 # burn-in points nsamples = 500 # points after burn-in time0 = time() sampler.run_mcmc(param_samples, nburn + nsamples) time1 = time() print(\"Time taken to sample first peak with Zeus: {} seconds\".format(time1-time0)) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:12<00:00, 13.84it/s] Time taken to sample first peak with Zeus: 72.26702880859375 seconds","title":"Sampling the data"},{"location":"Sunspots2/Sunspots2/#results","text":"Collecting the samples after the sampling is equally simple, remembering to discard the burn-in samples at the start of the chain. We can create a corner plot, which shows the posteriors of each parameter along with contour plots describing how one parameter varies with any other: samples_zeus = sampler.get_chain(flat=True, discard=nburn) def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['c', 'a1', 'a2', 't0', 'tmax', 't1'] plotposts(samples_zeus, labels) Next, I'll find the means and standard deviation error on each parameter. I'll also choose random samples from the chains of each parameter, which I'll use to visualise the posteriors. param_mu = [np.mean(samples_zeus[:,i]) for i in range(6)] param_sig = [np.std(samples_zeus[:,i]) for i in range(6)] nfits = 300 param_samples = [np.random.choice(samples_zeus[:,i],nfits) for i in range(6)] post_samples = np.array(param_samples).T print(\"Parameters describing the square root SSN cycle starting 1923: \\n \\n\" + \" c = {} \\u00B1 {} square root counts\\n\".format(param_mu[0], param_sig[0]) + \" a1 = {} \\u00B1 {} \\n\".format(param_mu[1], param_sig[1]) + \" a2 = {} \\u00B1 {} \\n\".format(param_mu[2], param_sig[2]) + \" t0 = {} \\u00B1 {} years\\n\".format(param_mu[3], param_sig[3]) + \"tmax = {} \\u00B1 {} years\\n\".format(param_mu[4], param_sig[4]) + \" t1 = {} \\u00B1 {} years\\n\".format(param_mu[5], param_sig[5])) Parameters describing the square root SSN cycle starting 1923: c = 11.235859600042037 \u00b1 0.5215026875027372 square root counts a1 = 2.0512996667503938 \u00b1 0.5507731307368571 a2 = 1.3134491063165001 \u00b1 0.2501776677187202 t0 = 1923.4484201628645 \u00b1 0.3547836069094943 years tmax = 1927.5792309869332 \u00b1 0.6491606607901439 years t1 = 1935.0575941486698 \u00b1 0.31425239983590497 years","title":"Results"},{"location":"Sunspots2/Sunspots2/#plotting-the-posterior","text":"Below I'll show two plots. The left plot will show the model produced using the mean parameters for the first peak. The right plot will show a posterior predictive plot, where the darker the colour of the plot, the higher the probabillty of the model passing through that region. fig, (ax1,ax2) = plt.subplots(1,2,figsize=(13,3)) # mean posterior plot ax1.plot(peaktimes[0], peaks[0]) x = np.linspace(min(peaktimes[0]), max(peaktimes[0]), 300) y = cycle(x,*param_mu) ax1.plot(x,y) ax1.set_xlabel(\"Date / years\") ax1.set_ylabel(\"square root counts\") ax1.set_title(\"Mean posterior plot for SSN cycle starting 1856\") # posterior predictive plot x = np.linspace(min(peaktimes[0]),max(peaktimes[0]),300) ax2.plot(peaktimes[0], peaks[0], label=\"Origina Data\") for i in range(len(post_samples)): params = post_samples[i] y = cycle(x,*params) ax2.plot(x,y,'orange',alpha=0.02, linewidth=3,label=\"Fitted Model\" if i == 0 else \"\") ax2.set_xlabel(\"Date / years\") ax2.set_title(\"Posterior predictive plot for SSN cycle starting 1856\") leg = ax2.legend() for lh in leg.legendHandles: lh.set_alpha(1) plt.show()","title":"Plotting the posterior"},{"location":"Sunspots2/Sunspots2/#predicting-the-next-solar-cycles","text":"We now know the properties of the solar cycle between 1924-1934. Next, we want to know the properties of the next three cycles in our region of interest. We could just redefine our prior distributions, and run the sampling again for each peak. Alternatively, we could create a new model which takes the current solar cycle parameters, and uses them to predict the next solar cycle. This model will only, realisticly, be able to make long range predictions if the solar cycles evolve consistently, as mentiioned above. This way this model is put together is quite complicated. More details can be found here , but the brief is that we need 3 scaling parameters (y1,y2,y3), and 4 translation parameters (d0,d1,d2,d3). This model can be implemented as follows: def predict_cycle(theta_prev, d0, d1, d2, d3, y1, y2, y3): c_prev, a1, a2, t0_prev, tmax_prev, t1_prev = theta_prev # start of cycle found using slight deviation d0 t0 = t1_prev + d0 # current c found using current t0 and previous c,tmax if t0 <= tmax_prev: t0 = tmax_prev+5 c = y1*c_prev/(t0 - tmax_prev) + d1 # current tmax found using current t0,c tmax = t0 + y2*c + d2*(t0-param_mu[4])**0.9 # current t1 found using current tmax,c t1 = tmax + y3*c + d3*(t0-param_mu[4])**0.3 #a1,a2 unchanged # return new set of parameters theta_new = (c,a1,a2,t0,tmax,t1) return theta_new We can now try to guess the scaling and translational parameters, and using the set of parameters defining the first solar cycle, we can try to predict the second solar cycle between 1934-1944. I'll show this by plotting the data for the second solar cycle, and overplotting the predicted model. After some trial and error, I found the following prediction: # predict the next peak parameters, using current peak parameters params = predict_cycle(param_mu,-1,0,-0.4,0.2,7.5,0.5,0.5) # plot second solar cycle plt.plot(peaktimes[1],peaks[1],label=\"Original Data\") # overplot predicted model with new parameters x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) plt.plot(x,cycle(x,*params),label=\"Predicted model\") plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Average square root sunspot number between 1934-1944 \\n with overplotted predicted model\") plt.legend() plt.show() Using the above guesses, we can define some prior distributions. This is done in exactly the same way as above, however due to some hard limits on the parameters (such as a1,a2 being strictly greater than 1), I'll use uniform priors for all parameters. nens = 100 ndims = 7 d0min,d0max = -2.,0 d1min,d1max = -1.,1. d2min,d2max = -1.,0. d3min,d3max = 0.,0.4 y1min,y1max = 7.,8. y2min,y2max = 0.2,0.8 y3min,y3max = 0.25,0.75 predict_priors = [] # uniform prior on d0 predict_priors.append(np.random.uniform(d0min,d0max,nens)) # uniform prior on d1 predict_priors.append(np.random.uniform(d1min,d1max,nens)) # uniform prior on d2 predict_priors.append(np.random.uniform(d2min,d2max,nens)) # uniform prior on d3 predict_priors.append(np.random.uniform(d3min,d3max,nens)) # uniform prior on y1 predict_priors.append(np.random.uniform(y1min,y1max,nens)) # uniform prior on y2 predict_priors.append(np.random.uniform(y2min,y2max,nens)) # uniform prior on y3 predict_priors.append(np.random.uniform(y3min,y3max,nens)) priors = [(d0min,d0max),(d1min,d1max),(d2min,d2max),(d3min,d3max), (y1min,y1max),(y2min,y2max),(y3min,y3max)] predict_samples = np.array(predict_priors).T We need to create new log prior, log likelihood, and log posterior functions. This is done almost exactly as above, but with very slight variations, so I'll won't go into detail about the machinery in these functions. def loglike_predict(theta, times, obs): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle(param_mu, *theta) lmbda = np.array(cycle(times, *params_new)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b def logprior_predict(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range(len(predict_priors)): # sum log priors from each parameter low, up = priors[i][0],priors[i][1] # uniform prior for time parameters if low < theta[i] < up: pass else: lprior = -np.inf return lprior def logposterior_predict(theta, times, obs): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior_predict(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_predict(theta, times, obs) Running the sampler, using the same burn-in and chain lengths as above, we can tune the parameters so they predict the parameters of the second solar cycle well. We'll then assume that these parameters are the same for the third and fourth peaks, and see how good the quality of the fit is with increasing forecast time. sampler = zeus.sampler(nens, ndims, logposterior_predict, args=[peaktimes[1], peaks[1]]) nburn = 500 nsamples = 500 time0 = time() sampler.run_mcmc(predict_samples, nburn + nsamples) time1 = time() print(\"Time taken to sample predict function with Zeus: {} seconds\".format(time1-time0)) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:16<00:00, 13.10it/s] Time taken to sample predict function with Zeus: 76.33164262771606 seconds","title":"Predicting the next solar cycles"},{"location":"Sunspots2/Sunspots2/#predictions","text":"Out of curiosity, we can check the corner plot for the prediction model. Since the prediction model has every parameter interacting with eachother, we can expect a load of covariance between parameters. samples_zeus2 = sampler.get_chain(flat=True, discard=nburn) predict_param_mu = [np.mean(samples_zeus2[:,i]) for i in range(7)] predict_param_sig = [np.std(samples_zeus2[:,i]) for i in range(7)] nfits = 300 predict_param_samples = [np.random.choice(samples_zeus2[:,i],nfits) for i in range(7)] predict_post_samples = np.array(predict_param_samples).T labels = ['d0','d1','d2','d3','y1', 'y2', 'y3'] plotposts(samples_zeus2,labels) Lets check what predictions we can now make. The below code takes the set of parameters describing the first solar cycle, and uses it to predict the second solar cycle. The prediction will be plotted over the data, and then the program will move on to the next peak and repeat the process. Since we're making predictions on using predictions, over and over, we can expect the fit to get worse as we look deeper into the future. I'll also use the samples from the prediction model posteriors, and create a posterior predictive plot for each cycle. This should show the quality of the prediction, and how it evolves with increasing forecast time. fig,axs = plt.subplots(3,2,figsize=(10,10)) # predict second peak parameters # plot mean posterior plot new_param_mu = predict_cycle(param_mu,*predict_param_mu) axs[0,0].plot(peaktimes[1],peaks[1]) x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) axs[0,0].plot(x,cycle(x,*new_param_mu)) axs[0,0].set_ylabel(\"Square Root of Sunspot Number\") axs[0,0].set_title(\"Average sunspot number for the solar cycles\\n between 1933-1965\" + \" with overfitted\\n mean predicted models\") # posterior predictive plot axs[0,1].plot(peaktimes[1],peaks[1]) new_pred = [] for i in range(len(predict_post_samples)): pred = predict_cycle(param_mu,*predict_post_samples[i]) new_pred.append(pred) y = cycle(x,*pred) axs[0,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) axs[0,1].set_title(\"Average sunspot number for the solar cycles\\n between 1933-1965\" + \" with overfitted\\n posterior predictive plots\") # predict third peak parameters # plot mean posterior plot new_param_mu = predict_cycle(new_param_mu, *predict_param_mu) axs[1,0].plot(peaktimes[2],peaks[2]) x = np.linspace(min(peaktimes[2]),max(peaktimes[2]),300) axs[1,0].plot(x,cycle(x,*new_param_mu)) axs[1,0].set_ylabel(\"Square Root of Sunspot Number\") axs[1,0].set_xlabel(\"Date / years\") # posterior predictive plot axs[1,1].plot(peaktimes[2],peaks[2]) new_pred2 = [] for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred[i],*predict_param_mu) new_pred2.append(pred) y = cycle(x,*pred) axs[1,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) # predict fourth peak parameters # plot mean posterior plot new_param_mu = predict_cycle(new_param_mu, *predict_param_mu) axs[2,0].plot(peaktimes[3],peaks[3],label=\"Original Data\") x = np.linspace(min(peaktimes[3]),max(peaktimes[3]),300) axs[2,0].plot(x,cycle(x,*new_param_mu),label=\"Predicted Model\") axs[2,0].set_ylabel(\"Square Root of Sunspot Number\") axs[2,0].set_xlabel(\"Date / years\") axs[2,0].legend() # posterior predictive plot axs[2,1].plot(peaktimes[3],peaks[3]) for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred2[i],*predict_param_mu) y = cycle(x,*pred) axs[2,1].plot(x,y,\"orange\",alpha=0.04,linewidth=3) axs[2,1].set_xlabel(\"Date / years\") plt.show() Saying that our prediction model was fairly simplified, it actually does a pretty good job at predicting the solar cycle shape. The posterior plots show the predictions getting worse over time as expected, but even so the accuracy on the mean plots are within 20% for the majority of the cycles.","title":"Predictions"},{"location":"Sunspots2/Sunspots2/#modelling-with-nestle","text":"The predictions from this model only work if we know in advance that the next solar cycle will carry on the trend from the previous solar cycle. Since in reality it is very hard to say how the Sun will behave in advance, this model won't be all that useful. However, it did save us from having to individually sampling 3 solar cycles. To decide whether or not the time save is worth the loss in accuracy, I'll use \"Nestle\" to sample both the second solar cycle (between 1934-1945), and the prediction model for that same solar cycle. Nestle uses nested sampling, and so it produces a value of the marginalised evidence. Comparing these values will allow us to see how much accuracy we're sacrificing by predicting the solar cycle, instead of just sampling the next peak.","title":"Modelling with Nestle"},{"location":"Sunspots2/Sunspots2/#sampling-the-data_1","text":"To start, we'll assume that Zeus did a good enough job at sampling the first solar cycle, and jump straight to the second cycle (1934-1945). Lets define some new guesses for each parameter (c,a1,a2,t0,tmax,t1): # mean and standard deviation of normal parameter priors cmu,csig = 13,1 # lower and upper bounds of uniform parameter priors a1min,a1max = 1,3 a2min,a2max = 1,3 t0min,t0max = 1931,1935 tmaxmin,tmaxmax = 1936,1940 t1min,t1max = 1943,1947 param_priors = [(cmu,csig),(a1min,a1max),(a2min,a2max), (t0min,t0max),(tmaxmin,tmaxmax),(t1min,t1max)] We can reuse most of the stuff we had from before, except for the log prior functions. Nestle samples from a unit hypercube parameter space, so we need a function that transforms the priors back to their original space. This only requires a slight modification to our previous \"logprior\" function, making use of scipy's ndtri function. I'll also make a very minor change to the log likelihood function. Instead of taking the dates and sunspot counts as arguments, it will use the data for the second solar cycle by default. def prior_transform(theta): \"\"\" Function to transform the parameters from unit hypercube to their true form \"\"\" trans_params = [] # transform normal prior trans_params.append(param_priors[0][0] + param_priors[0][1]*ndtri(theta[0])) # transform uniform prior for i in range(1,6): mini, maxi = param_priors[i][0], param_priors[i][1] trans_params.append(theta[i]*(maxi-mini) + mini) return trans_params def loglike_nestle(theta): \"\"\" Function to return the log likelihood, with data fixed for solar cycle between 1934-1945 \"\"\" obs, times = peaks[1], peaktimes[1] # unpack parameters c_like, a1_like, a2_like, t0_like, tmax_like, t1_like = theta # expected value lmbda = np.array(cycle(times, c_like, a1_like, a2_like, t0_like, tmax_like, t1_like)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b We're now ready to run the sampling using Nestle: # set number of dimensions, live points, sampling method, and stopping criterion ndims = 6 nlive = 1024 method = 'multi' stop = 0.1 time0 = time() results_sample = nestle.sample(loglike_nestle, prior_transform, ndims, method=method, npoints=nlive, dlogz=stop) time1 = time() print(\"Time taken to sample second solar cycle with Nestle: {} seconds\".format(time1-time0)) Time taken to sample second solar cycle with Nestle: 18.07349967956543 seconds We'll now try the sampling again, but this time we'll use the parameters from the first solar cycle (found using Zeus), and sample the parameters of the prediction model. Start by guessing at the parameters for the prediction model: # reuse our guesses from Zeus d0min,d0max = -2.,0 d1min,d1max = -1.,1. d2min,d2max = -1.,0. d3min,d3max = 0.,0.4 y1min,y1max = 7.,8. y2min,y2max = 0.2,0.8 y3min,y3max = 0.25,0.75 predict_priors = [(d0min,d0max),(d1min,d1max),(d2min,d2max),(d3min,d3max), (y1min,y1max),(y2min,y2max),(y3min,y3max)] We have to tinker with out prediction model's likelihood and prior functions. This is done exactly as it was previously. def loglike_nestle_predict(theta): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" obs,times = peaks[1],peaktimes[1] # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle(param_mu, *theta) lmbda = np.array(cycle(times, *params_new)) n = len(obs) a = np.sum(gammaln(np.array(obs)+1)) b = np.sum(np.array(obs) * np.log(lmbda)) return -np.sum(lmbda) - a + b def prior_transform_predict(theta): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" trans_priors = [] for i in range(len(predict_priors)): # sum log priors from each parameter mini, maxi = predict_priors[i][0],predict_priors[i][1] # uniform prior for time parameters trans_priors.append(theta[i]*(maxi-mini)+mini) return trans_priors Now that everything is set up, I'll run through the same sampling process, using the same hyper-parameters for fairness. # set number of dimensions, live points, sampling method, and stopping criterion ndims = 7 nlive = 1024 method = 'multi' stop = 0.1 time0 = time() results_predict = nestle.sample(loglike_nestle_predict, prior_transform_predict, ndims, method=method, npoints=nlive, dlogz=stop) time1 = time() print(\"Time taken to sample prediction model with Nestle: {} seconds\".format(time1-time0)) Time taken to sample prediction model with Nestle: 14.57199764251709 seconds","title":"Sampling the data"},{"location":"Sunspots2/Sunspots2/#results_1","text":"Next, we find the log of the marginalised evidence provided by Nestle. This can be done as follows, using the information gain to estimate the error: logZ_sample = results_sample.logz logZerr_sample = np.sqrt(results_sample.h/nlive) logZ_predict = results_predict.logz logZerr_predict = np.sqrt(results_predict.h/nlive) print(\"log(Z) from sampling the solar cycle = {} \u00b1 {}\".format(logZ_sample, logZerr_sample)) print(\"log(Z) from predicting the solar cycle = {} \u00b1 {}\".format(logZ_predict, logZerr_predict)) log(Z) from sampling the solar cycle = -266.7678218108872 \u00b1 0.07474929979047762 log(Z) from predicting the solar cycle = -266.69101856955064 \u00b1 0.07202857357654911 The Bayes factor is a metric that describes how much more likely a model is to produce an observed data set. It's defined as the ratios between the marginalised evidences of the two models: K = np.exp(logZ_predict - logZ_sample) print(\"Bayes factor: {}\".format(K)) Bayes factor: 1.0798295896621817 This tells us that sampling via the prediction method rather than just sampling the peak doesn't come at a cost to the quality of the model. Since the methods produce simillar results, lets collect the samples from the prediction method to use for visualising our results. Since Nestle uses nested sampling, we have to resample with weights to obtain the posteriors. weights = results_predict.weights/np.max(results_predict.weights) mask = np.where(np.random.rand(len(weights)) < weights)[0] # collect posterior samples samples_nestle = results_predict.samples[mask,:] # collect posterior means predict_param_mu_nestle = [np.mean(samples_nestle[:,i]) for i in range(7)] # collect samples for posterior predictive plot nfits = 300 predict_param_samples_nestle = [np.random.choice(samples_nestle[:,i],nfits) for i in range(7)] predict_post_samples_nestle = np.array(predict_param_samples).T","title":"Results"},{"location":"Sunspots2/Sunspots2/#plotting-the-posterior_1","text":"Let's start by making a comparison between the posteriors from Zeus and Nestle. Below I'll create two plots. The first will show the mean posterior predictions of the second solar cycle, from both Zeus and Nestle. The second plot will show the posterior predictive plots, for both samplers also. fig,(ax1,ax2) = plt.subplots(1,2,figsize=(13,4)) # plot mean posterior plot new_param_mu_zeus = predict_cycle(param_mu,*predict_param_mu) new_param_mu_nestle = predict_cycle(param_mu,*predict_param_mu_nestle) ax1.plot(peaktimes[1],peaks[1],label=\"Original Data\") x = np.linspace(min(peaktimes[1]),max(peaktimes[1]),300) ax1.plot(x,cycle(x,*new_param_mu),\"orange\",label=\"Zeus Prediction\") ax1.plot(x,cycle(x,*new_param_mu_nestle),\"purple\",label=\"Nestle Prediction\") ax1.set_ylabel(\"Square Root of Sunspot Number\") ax1.set_xlabel(\"Date / years\") ax1.set_title(\"Average sunspot number for the solar cycles\\n between 1934-1945\" + \" with overfitted\\n mean predicted models\") ax1.legend() # posterior predictive plot ax2.plot(peaktimes[1],peaks[1]) for i in range(len(predict_post_samples)): pred = predict_cycle(param_mu,*predict_post_samples[i]) y = cycle(x,*pred) ax2.plot(x,y,\"orange\",alpha=0.04,linewidth=3) pred = predict_cycle(param_mu,*predict_post_samples_nestle[i]) y = cycle(x,*pred) ax2.plot(x,y,\"purple\",alpha=0.04,linewidth=3) ax1.set_ylabel(\"Date / years\") ax2.set_title(\"Average sunspot number for the solar cycles\\n between 1934-1945\" + \" with overfitted\\n posterior predictive plots\") plt.show() Whilst the posterior predictive plot looks a little messy, it tells us that the posterior predictives from Zeus and Nestle are difficult to tell apart, since they overlap so much. The mean posterior plot supports this, showing that the two samplers only start deviating on the descending limb of the cycle. Finally, lets plot a posterior predictive plot that shows the evolution of the prediction accuracy over time. Below is a plot that shows predictions for the entire range of 4 solar cycles between 1923 and 1965. plt.figure(figsize=(14,4)) # plot data between 1923 and 1965 plt.plot(dates[start:start + 4*period],sqrtssn[start:start + 4*period],label=\"Original Data\") # plot first cycle posterior predictive, using Zeus samples x = np.linspace(min(peaktimes[0])-20,max(peaktimes[0]),300) for i in range(len(post_samples)): params = post_samples[i] y = cycle(x,*params) plt.plot(x,y,\"purple\",alpha=0.02, linewidth=3,label=\"Fitted Model\" if i == 0 else \"\") # use first cycle posterior predictive to find second cycle posterior predictive new_pred = [] x = np.linspace(min(peaktimes[1])-20,max(peaktimes[1])+20,300) for i in range(len(predict_post_samples_nestle)): pred = predict_cycle(param_mu,*predict_post_samples_nestle[i]) new_pred.append(pred) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3,label=\"Predicted Model\" if i == 0 else \"\") # use second cycle posterior predictive to find third cycle posterior predictive x = np.linspace(min(peaktimes[2])-20,max(peaktimes[2])+20,300) new_pred2 = [] for i in range(len(predict_post_samples_nestle)): pred = predict_cycle(new_pred[i],*predict_param_mu_nestle) new_pred2.append(pred) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3) # use third cycle posterior predictive to find fourth cycle posterior predictive x = np.linspace(min(peaktimes[3])-20,max(peaktimes[3])+20,300) for i in range(len(predict_post_samples)): pred = predict_cycle(new_pred2[i],*predict_param_mu_nestle) y = cycle(x,*pred) plt.plot(x,y,\"orange\",alpha=0.04,linewidth=3) plt.xlabel(\"Date / years\") plt.ylabel(\"Square Root of Sunspot Number\") plt.title(\"Predictions of average sunspot number between 1934-1965,\\n\" + \" using the cycle between 1923-1934 to make predictions\") plt.xlim(1923,1967) plt.ylim(1) leg = plt.legend(loc=\"upper left\") for lh in leg.legendHandles: lh.set_alpha(1) plt.show()","title":"Plotting the posterior"}]}