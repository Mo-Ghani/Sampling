{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Uses Of MCMC and Nested Sampling Algorithms There are various off-the-shelf samplers that make use of MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used to analyse real world, open source data sets. A github repository containing all the examples on this webpage can be found here A Docker container containing all the samplers used on this webpage can be found here A Brief Introduction to Bayesian Inference For any experiment, you might have some kind of belief on what the outcome should be. This belief can be tested by recording the results of the experiment, and finding the likelihood that your beliefs explain the observed data. Using this you can update your prior beliefs using Bayes' rule : P(M|D) = \\frac{P(D|M) P(M)}{P(D)} Where P(M) is the prior probability belief of the model M , P(D|M) is the likelihood of observing data D given the model, P(M|D) is the posterior probability of the model given the data, and P(D) is the \"marginalised likelihood\" or the \"evidence\", which is the probability of observing the data set. Bayesian inference can be used in physics to fit models to observed data, given only a prior belief on the parameters of the model and a model for the noise observed in the data (the likelihood). This has uses in astronomy , uses in particle physics , and everywhere in between . Markov Chain Monte Carlo Evaluating the posterior over the prior space for multi-parameter models can be computationally difficult, or in may cases impossible for all practical purposes, so instead a method called Markov chain Monte Carlo (MCMC) is employed. An MCMC algorithm has two components: Monte Carlo techniques imply that samples are randomly generated, and Markov Chain implies that the next step of the algorithm depends only on the current step. The most popular MCMC algorithm is the MH algorithm , which will completely ignore the P(D) term. Instead, on the i 'th step of the algorithm, the sampler will randomly generate a proposal model M^* which may (or may not) do a better job at explaining the data than the current model M^{i-1} . The ratio of posteriors r = \\frac{P(D|M^*) P(M^*)}{P(D|M^{i-1}) P(M^{i-1})} = \\frac{P(M^*|D)}{P(M^{i-1}|D)} is then calculated, using the prior and likelihood distributions. This ratio describes which model is most likely to explain the data, and so the proposed model M^* will be accepted if this ratio is greater than 1. Otherwise, the acceptance probability is P = r . Although this allows less likely models to be accepted, it means a wider parameter space can be explored such that a global likelihood maxima can be found, rather than a local maxima. Once a proposed model is accepted, it is added to a set of posterior samples which is returned by the MCMC algorithm when all iterations are complete. These samples can be used to construct marginalised posterior distributions for the individual model parameters without having to analytically or numerically integrate the posterior distribution. Nested Sampling Nested sampling algorithms are a variation on MCMC, which use numerical methods to calculate the marginal likelihood. The name \"marginal likelihood\" is literal, meaning that the evidence can be found as the integral of the likelihood with respect to the model (marginalising the model out). When models have tens or hundreds of parameters, this marginalisation becomes impossible. A very simple nested sampling algorithm essentially uses the trapezium rule to estimate the value of the marginalisation integral. This is especially useful for comparing models, since the marginalised likelihood is a metric for how well a model describes a data set. For two models M_1 and M_2 , the Bayes' factor K = \\frac{P(D|M_1)}{P(D|M_2)} describes whether or not M_1 is a more likely model than M_2 given the data D . The Samplers The following samplers are used in demonstrations on this site: PyMC3 dynesty emcee UltraNest Zeus Nestle PyStan DNest4 The Online MCMC The Data All the data used in these examples is open source, and publicly available on the following links: Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Average solar sunspot numbers since 1750 Gravitational wave signal from GW150914 Particle invariant mass measurements from LHC","title":"Introduction"},{"location":"#the-uses-of-mcmc-and-nested-sampling-algorithms","text":"There are various off-the-shelf samplers that make use of MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used to analyse real world, open source data sets. A github repository containing all the examples on this webpage can be found here A Docker container containing all the samplers used on this webpage can be found here","title":"The Uses Of MCMC and Nested Sampling Algorithms"},{"location":"#a-brief-introduction-to-bayesian-inference","text":"For any experiment, you might have some kind of belief on what the outcome should be. This belief can be tested by recording the results of the experiment, and finding the likelihood that your beliefs explain the observed data. Using this you can update your prior beliefs using Bayes' rule : P(M|D) = \\frac{P(D|M) P(M)}{P(D)} Where P(M) is the prior probability belief of the model M , P(D|M) is the likelihood of observing data D given the model, P(M|D) is the posterior probability of the model given the data, and P(D) is the \"marginalised likelihood\" or the \"evidence\", which is the probability of observing the data set. Bayesian inference can be used in physics to fit models to observed data, given only a prior belief on the parameters of the model and a model for the noise observed in the data (the likelihood). This has uses in astronomy , uses in particle physics , and everywhere in between .","title":"A Brief Introduction to Bayesian Inference"},{"location":"#markov-chain-monte-carlo","text":"Evaluating the posterior over the prior space for multi-parameter models can be computationally difficult, or in may cases impossible for all practical purposes, so instead a method called Markov chain Monte Carlo (MCMC) is employed. An MCMC algorithm has two components: Monte Carlo techniques imply that samples are randomly generated, and Markov Chain implies that the next step of the algorithm depends only on the current step. The most popular MCMC algorithm is the MH algorithm , which will completely ignore the P(D) term. Instead, on the i 'th step of the algorithm, the sampler will randomly generate a proposal model M^* which may (or may not) do a better job at explaining the data than the current model M^{i-1} . The ratio of posteriors r = \\frac{P(D|M^*) P(M^*)}{P(D|M^{i-1}) P(M^{i-1})} = \\frac{P(M^*|D)}{P(M^{i-1}|D)} is then calculated, using the prior and likelihood distributions. This ratio describes which model is most likely to explain the data, and so the proposed model M^* will be accepted if this ratio is greater than 1. Otherwise, the acceptance probability is P = r . Although this allows less likely models to be accepted, it means a wider parameter space can be explored such that a global likelihood maxima can be found, rather than a local maxima. Once a proposed model is accepted, it is added to a set of posterior samples which is returned by the MCMC algorithm when all iterations are complete. These samples can be used to construct marginalised posterior distributions for the individual model parameters without having to analytically or numerically integrate the posterior distribution.","title":"Markov Chain Monte Carlo"},{"location":"#nested-sampling","text":"Nested sampling algorithms are a variation on MCMC, which use numerical methods to calculate the marginal likelihood. The name \"marginal likelihood\" is literal, meaning that the evidence can be found as the integral of the likelihood with respect to the model (marginalising the model out). When models have tens or hundreds of parameters, this marginalisation becomes impossible. A very simple nested sampling algorithm essentially uses the trapezium rule to estimate the value of the marginalisation integral. This is especially useful for comparing models, since the marginalised likelihood is a metric for how well a model describes a data set. For two models M_1 and M_2 , the Bayes' factor K = \\frac{P(D|M_1)}{P(D|M_2)} describes whether or not M_1 is a more likely model than M_2 given the data D .","title":"Nested Sampling"},{"location":"#the-samplers","text":"The following samplers are used in demonstrations on this site: PyMC3 dynesty emcee UltraNest Zeus Nestle PyStan DNest4 The Online MCMC","title":"The Samplers"},{"location":"#the-data","text":"All the data used in these examples is open source, and publicly available on the following links: Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Average solar sunspot numbers since 1750 Gravitational wave signal from GW150914 Particle invariant mass measurements from LHC","title":"The Data"},{"location":"LightCurve/LightCurve/","text":"Using emcee and UltraNest to model the light curves from Kepler-10 Kepler-10 is a star located roughly 608 light years from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes in front of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intensity, occurring every time the exoplanet is in front of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existence of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA. Useful imports # numpy import numpy as np # scipy from scipy.special import gammaln , ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt % matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print ( 'emcee version: {} ' . format ( mc . __version__ )) print ( 'UltraNest version: {} ' . format ( ultranest . __version__ )) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2 Viewing the data Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (similar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits . open ( \"kplr011904151-2010265121752_llc.fits\" ) tab = table [ 1 ] data = Table ( tab . data ) flux_orig = data [ 'PDCSAP_FLUX' ][: 1325 ] time_orig = data [ 'TIME' ][: 1325 ] flux = [ flux_orig [ i ] for i in range ( len ( flux_orig )) if str ( flux_orig [ i ]) != \"nan\" ] time = [ time_orig [ i ] for i in range ( len ( time_orig )) if str ( flux_orig [ i ]) != \"nan\" ] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # plot all useful data ax1 . plot ( time , flux , \"k:\" ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # plot zoomed in view of transits ax2 . plot ( time , flux , \"k:\" ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541325 , 541700 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"Kepler-10 light curves showing evidence of exoplanet transits\" ) plt . show () The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy. The model Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is similar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit ( time , f , df , p , tt , tf = None , off = 0 , square = False ): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immediately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None : tf = tt if tt <= tf : # Default to square shaped transit square = True y = [] if not square : # define slope of sides of trapezium h = f * df * tt / ( tt - tf ) grad = 2 * h / tt for i in time : j = ( i + off ) % p if j < tt : # transit # square shaped transit if square : y . append ( f * ( 1 - df )) # trapezium shaped transit elif j / tt < 0.5 : # first half of transit val = f - grad * j if val < f * ( 1 - df ): y . append ( f * ( 1 - df )) else : y . append ( val ) else : # last half of transit val = ( grad * j ) - 2 * h + f if val < f * ( 1 - df ): y . append ( f * ( 1 - df )) else : y . append ( val ) else : # no transit y . append ( f ) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models. Modelling with emcee Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Due to the noise, I'll use a uniform prior on f, but a normal prior on other parameters. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # uniform prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [( f_min , f_max ), ( df_mu , df_sig ), ( p_mu , p_sig ), ( tt_mu , tt_sig ), ( tf_mu , tf_sig ), ( off_mu , off_sig )] # remove tf for square transit parameters priors_square = priors [: 4 ] + priors [ 5 :] Sampling the data The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very similar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior ( theta ): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range ( len ( priors )): # sum log priors from each parameter if i == 0 : # prior for uniform parameters if priors [ i ][ 0 ] < theta [ i ] < priors [ i ][ 1 ]: pass else : lprior = - np . inf else : # prior for non-uniform parameters lprior -= 0.5 * (( theta [ i ] - priors [ i ][ 0 ]) / priors [ i ][ 1 ]) ** 2 return lprior def logprior_square ( theta ): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range ( len ( priors_square )): # sum log priors from each parameter if i == 0 : # prior for uniform parameters if priors_square [ i ][ 0 ] < theta [ i ] < priors_square [ i ][ 1 ]: pass else : lprior = - np . inf else : # prior for non-uniform parameters lprior -= 0.5 * (( theta [ i ] - priors_square [ i ][ 0 ]) / priors_square [ i ][ 1 ]) ** 2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike ( theta ): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like , df_like , p_like , tt_like , tf_like , off_like = theta # expected value lmbda = np . array ( transit ( time , f_like , df_like , p_like , tt_like , tf_like , off = off_like )) n = len ( flux ) a = np . sum ( gammaln ( np . array ( flux ) + 1 )) b = np . sum ( np . array ( flux ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def loglike_square ( theta ): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like , df_like , p_like , tt_like , off_like = theta # expected value lmbda = np . array ( transit ( time , f_like , df_like , p_like , tt_like , off = off_like , square = True )) n = len ( flux ) a = np . sum ( gammaln ( np . array ( flux ) + 1 )) b = np . sum ( np . array ( flux ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior ( theta ): lprior = logprior ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike ( theta ) def logposterior_square ( theta ): lprior = logprior_square ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike_square ( theta ) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" samples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range ( len ( priors )): if i == 0 : inisamples . append ( np . random . uniform ( priors [ i ][ 0 ], priors [ i ][ 1 ], Nens )) else : inisamples . append ( np . random . normal ( priors [ i ][ 0 ], priors [ i ][ 1 ], Nens )) inisamples = np . array ( inisamples ) . T inisamples_square = [] for i in range ( len ( priors_square )): if i == 0 : inisamples_square . append ( np . random . uniform ( priors_square [ i ][ 0 ], priors_square [ i ][ 1 ], Nens )) else : inisamples_square . append ( np . random . normal ( priors_square [ i ][ 0 ], priors_square [ i ][ 1 ], Nens )) inisamples_square = np . array ( inisamples_square ) . T ndims = inisamples . shape [ 1 ] ndims_square = inisamples_square . shape [ 1 ] # no. iterations Nburn = 500 Nsamples = 500 loglike . ncalls = 0 loglike_square . ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc . EnsembleSampler ( Nens , ndims , logposterior ) sampler_square = mc . EnsembleSampler ( Nens , ndims_square , logposterior_square ) # perform sampling t0 = timer () sampler . run_mcmc ( inisamples , Nsamples + Nburn ) t1 = timer () print ( \"time taken to sample a trapezium transit model with emcee: {} seconds\" . format ( t1 - t0 )) sampler_square . run_mcmc ( inisamples_square , Nsamples + Nburn ) t2 = timer () print ( \"time taken to sample a square transit model with emcee: {} seconds\" . format ( t2 - t1 )) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler . chain [:, Nburn :, :] . reshape (( - 1 , ndims )) samples_square = sampler_square . chain [:, Nburn :, :] . reshape (( - 1 , ndims_square )) Results Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'Flux' , 'dFlux' , 'Period' , 'Transit Time' , 'Transit Flat Time' , 'Offset' ] plotposts ( samples_trapez , labels ) For each model, we can find the mean and standard deviation of each parameter using the traces. For the trapezium transit model: f , ferr = np . mean ( samples_trapez [:, 0 ]), np . std ( samples_trapez [:, 0 ]) df , dferr = np . mean ( samples_trapez [:, 1 ]), np . std ( samples_trapez [:, 1 ]) p , perr = np . mean ( samples_trapez [:, 2 ]), np . std ( samples_trapez [:, 2 ]) tt , tterr = np . mean ( samples_trapez [:, 3 ]), np . std ( samples_trapez [:, 3 ]) tf , tferr = np . mean ( samples_trapez [:, 4 ]), np . std ( samples_trapez [:, 4 ]) off , offerr = np . mean ( samples_trapez [:, 5 ]), np . std ( samples_trapez [:, 5 ]) print ( \"Parameters describing a trapezium shaped transit model: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f , ferr ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df , dferr ) + \" period = {} \\u00B1 {} days \\n \" . format ( p , perr ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt , tterr ) + \" flat transit time = {} \\u00B1 {} days \\n \" . format ( tf , tferr ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off , offerr )) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square , ferr_square = np . mean ( samples_square [:, 0 ] ), np . std ( samples_square [:, 0 ]) df_square , dferr_square = np . mean ( samples_square [:, 1 ] ), np . std ( samples_square [:, 1 ]) p_square , perr_square = np . mean ( samples_square [:, 2 ]), np . std ( samples_square [:, 2 ]) tt_square , tterr_square = np . mean ( samples_square [:, 3 ]), np . std ( samples_square [:, 3 ]) off_square , offerr_square = np . mean ( samples_square [:, 4 ]), np . std ( samples_square [:, 4 ]) print ( \"Parameters describing a square shaped transit model: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f_square , ferr_square ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df_square , dferr_square ) + \" period = {} \\u00B1 {} days \\n \" . format ( p_square , perr_square ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt_square , tterr_square ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off_square , offerr_square )) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b. Plotting the posterior We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np . random . choice ( samples_trapez [:, 0 ], n_fits ) dfsamps_trap_emcee = np . random . choice ( samples_trapez [:, 1 ], n_fits ) psamps_trap_emcee = np . random . choice ( samples_trapez [:, 2 ], n_fits ) ttsamps_trap_emcee = np . random . choice ( samples_trapez [:, 3 ], n_fits ) tfsamps_trap_emcee = np . random . choice ( samples_trapez [:, 4 ], n_fits ) offsamps_trap_emcee = np . random . choice ( samples_trapez [:, 5 ], n_fits ) fsamps_square_emcee = np . random . choice ( samples_square [:, 0 ], n_fits ) dfsamps_square_emcee = np . random . choice ( samples_square [:, 1 ], n_fits ) psamps_square_emcee = np . random . choice ( samples_square [:, 2 ], n_fits ) ttsamps_square_emcee = np . random . choice ( samples_square [:, 3 ], n_fits ) offsamps_square_emcee = np . random . choice ( samples_square [:, 4 ], n_fits ) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f , df , p , tt , tf , off ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_trap_emcee [ i ], dfsamps_trap_emcee [ i ], psamps_trap_emcee [ i ], ttsamps_trap_emcee [ i ], tfsamps_trap_emcee [ i ], offsamps_trap_emcee [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\" ) plt . show () The same process can be repeated for the square transit model: fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f_square , df_square , p_square , tt_square , off = off_square ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_square_emcee [ i ], dfsamps_square_emcee [ i ], psamps_square_emcee [ i ], ttsamps_square_emcee [ i ], off = offsamps_square_emcee [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\" ) plt . show () From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system. Model comparisons Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. For the derivations on equations for the following calculations, check out this paper . For each property, I'll state the most accpeted values found here . The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs , RpRs_err = np . sqrt ( df ), dferr / ( np . sqrt ( df )) RpRs_square , RpRs_square_err = np . sqrt ( df_square ), dferr_square / ( np . sqrt ( df_square )) print ( \"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {} \" . format ( RpRs , RpRs_err )) print ( \"Planet to star radius ratio (square transit model): {} \\u00B1 {} \" . format ( RpRs_square , RpRs_square_err )) print ( \"Accepted planet to star radius ratio: {} \" . format ( 0.0127 )) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 Accepted planet to star radius ratio: 0.0127 Both models are in close agreement with each other, and are quite close to the accepted value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and accepted ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square , tferr_square = 0.99 * tt_square , 0.99 * tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor ( df , tt , tf ): # return semi-minor axis, given model parameters numerator = ( tt ** 2 ) * ( 1 - np . sqrt ( df )) ** 2 - ( tf ** 2 ) * ( 1 + np . sqrt ( df )) ** 2 denominator = tt ** 2 - tf ** 2 return np . sqrt ( numerator / denominator ) def normsemimajor ( df , p , tt , tf ): # return normalised semi-major axis, given model parameters numerator = 2 * p * ( df ) ** ( 1 / 4 ) denominator = np . pi * np . sqrt ( tt ** 2 - tf ** 2 ) return numerator / denominator # semiminor for trapezium model b_trap = semiminor ( df , tt , tf ) b_trap_err = abs ( semiminor ( df + dferr , tt , tf ) - b_trap ) # semiminor for square model b_square = semiminor ( df_square , tt_square , tf_square ) b_square_err = abs ( semiminor ( df_square + dferr_square , tt_square , tf_square ) - b_square ) # semimajor for trapezium model aRs_trap = normsemimajor ( df , p , tt , tf ) aRs_trap_err = abs ( normsemimajor ( df + dferr , p + perr , tt + tterr , tf + tferr ) - aRs_trap ) # semimajor for square model aRs_square = normsemimajor ( df_square , p_square , tt_square , tf_square ) aRs_square_err = abs ( normsemimajor ( df_square + dferr_square , p_square + perr_square , tt_square + tterr_square , tf_square + tferr_square ) - aRs_square ) print ( \"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {} \" . format ( aRs_trap , aRs_trap_err )) print ( \"Normalised semi-major axis (square transit model): {} \\u00B1 {} \" . format ( aRs_square , aRs_square_err )) print ( \"Accepted normalised semi-major axis: {} \" . format ( 3.40 )) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 Accepted normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np . arccos ( b_trap / aRs_trap ) * ( 180 / np . pi ) I_trap_err = abs ( np . arccos (( b_trap + b_trap_err ) / ( aRs_trap - aRs_trap_err )) * ( 180 / np . pi ) - I_trap ) I_square = np . arccos ( b_square / aRs_square ) * ( 180 / np . pi ) I_square_err = abs ( np . arccos (( b_square + b_square_err ) / ( aRs_square - aRs_square_err ) ) * ( 180 / np . pi ) - I_square ) print ( \"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\" . format ( I_trap , I_trap_err )) print ( \"Inclination angle (square transit model): {} \\u00B1 {} degrees\" . format ( I_square , I_square_err )) print ( \"Accepted inclination angle: {} degrees\" . format ( 84.4 )) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees Accepted inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density ( df , p , tt , aRs ): # return density of star given model parameters G = 6.67408e-11 wt = tt * np . pi / p # transform p from days to seconds p *= 86400 numerator = 3 * np . pi * aRs ** 3 denominator = G * p ** 2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density ( df , p , tt , aRs_trap ) stard_trap_err = abs ( star_density ( df + dferr , p - perr , tt - tterr , aRs_trap + aRs_trap_err ) - stard_trap ) # star density predicted by square model stard_square = star_density ( df_square , p_square , tt_square , aRs_square ) stard_square_err = abs ( star_density ( df_square + dferr_square , p_square - perr_square , tt_square - tterr_square , aRs_square + aRs_square_err ) - stard_square ) print ( \"Star density (trapezium transit model): {} \\u00B1 {} kg/m \\u00b3 \" . format ( stard_trap , stard_trap_err )) print ( \"Star density (square transit model): {} \\u00B1 {} kg/m \\u00b3 \" . format ( stard_square , stard_square_err )) print ( \"True star density: {} kg/m \\u00b3 \" . format ( 1070 )) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only in front of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above. Modelling with UltraNest If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package. Sampling the data This works in a similar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform ( theta ): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [ 0 for i in range ( len ( theta ))] for i in range ( len ( theta )): if i == 0 : # uniform transform for f params [ i ] = ( priors [ i ][ 1 ] - priors [ i ][ 0 ]) * theta [ i ] + priors [ i ][ 0 ] else : # normal transform for remaining parameters params [ i ] = priors [ i ][ 0 ] + priors [ i ][ 1 ] * ndtri ( theta [ i ]) return np . array ( params ) def prior_transform_square ( theta ): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [ 0 for i in range ( len ( theta ))] for i in range ( len ( theta )): if i == 0 : # uniform transform for f params [ i ] = ( priors_square [ i ][ 1 ] - priors_square [ i ][ 0 ] ) * theta [ i ] + priors_square [ i ][ 0 ] else : # normal transform for remaining parameters params [ i ] = priors_square [ i ][ 0 ] + priors_square [ i ][ 1 ] * ndtri ( theta [ i ]) return np . array ( params ) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest . ReactiveNestedSampler ([ 'f' , 'df' , 'p' , 'tt' , 'tf' , 'off' ], loglike , prior_transform ) sampler_square = ultranest . ReactiveNestedSampler ([ 'f' , 'df' , 'p' , 'tt' , 'off' ], loglike_square , prior_transform_square ) # use \"slice\" sampler, due to high dimensionality nsteps = 2 * len ( priors ) sampler . stepsampler = stepsampler . RegionSliceSampler ( nsteps = nsteps ) sampler_square . stepsampler = stepsampler . RegionSliceSampler ( nsteps = nsteps ) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer () results_trap = sampler . run ( min_num_live_points = nlive ) t1 = timer () results_square = sampler_square . run ( min_num_live_points = nlive ) t2 = timer () print ( ' \\n \\n ' + 'Time taken to sample trapezium shaped model with UltraNest: {} seconds' . format ( t1 - t0 )) print ( 'Time taken to sample square shaped model with UltraNest: {} seconds' . format ( t2 - t1 )) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds Results To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np . array ( results_trap [ \"weighted_samples\" ][ \"points\" ]) weights_trap = np . array ( results_trap [ \"weighted_samples\" ][ \"weights\" ]) resample_trap = np . random . rand ( len ( weights_trap )) < weights_trap / max ( weights_trap ) samples_trap_ultra = samples_points_trap [ resample_trap , :] The mean parameter values and their errors can be found easily: # parameter means and errors for trapezium transit f_trap_ultra , ferr_trap_ultra = np . mean ( samples_trap_ultra [:, 0 ] ), np . std ( samples_trap_ultra [:, 0 ]) df_trap_ultra , dferr_trap_ultra = np . mean ( samples_trap_ultra [:, 1 ]) ), np . std ( samples_trap_ultra [:, 1 ]) p_trap_ultra , perr_trap_ultra = np . mean ( samples_trap_ultra [:, 2 ] ), np . std ( samples_trap_ultra [:, 2 ]) tt_trap_ultra , tterr_trap_ultra = np . mean ( samples_trap_ultra [:, 3 ] ), np . std ( samples_trap_ultra [:, 3 ]) tf_trap_ultra , tferr_trap_ultra = np . mean ( samples_trap_ultra [:, 4 ] ), np . std ( samples_trap_ultra [:, 4 ]) off_trap_ultra , offerr_trap_ultra = np . mean ( samples_trap_ultra [:, 5 ] ), np . std ( samples_trap_ultra [:, 5 ]) print ( \"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f_trap_ultra , ferr_trap_ultra ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df_trap_ultra , dferr_trap_ultra ) + \" period = {} \\u00B1 {} days \\n \" . format ( p_trap_ultra , perr_trap_ultra ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt_trap_ultra , tterr_trap_ultra ) + \" flat transit time = {} \\u00B1 {} days \\n \" . format ( tf_trap_ultra , tferr_trap_ultra ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off_trap_ultra , offerr_trap_ultra )) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days Plotting the posterior We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 0 ], nfits ) dfsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 1 ], nfits ) psamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 2 ], nfits ) ttsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 3 ], nfits ) tfsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 4 ], nfits ) offsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 5 ], nfits ) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , figsize = ( 15 , 10 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f_trap_ultra , df_trap_ultra , p_trap_ultra , tt_trap_ultra , tf_trap_ultra , off_trap_ultra ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) ax1 . set_title ( \"UltraNest mean posterior plot\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_trap_ultra [ i ], dfsamps_trap_ultra [ i ], psamps_trap_ultra [ i ], ttsamps_trap_ultra [ i ], tfsamps_trap_ultra [ i ], offsamps_trap_ultra [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_title ( \"UltraNest posterior predictive plot\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) # emcee and UltraNest overlapping posterior predictive plot for i in range ( n_fits ): y_ultra = transit ( x , fsamps_trap_ultra [ i ], dfsamps_trap_ultra [ i ], psamps_trap_ultra [ i ], ttsamps_trap_ultra [ i ], tfsamps_trap_ultra [ i ], offsamps_trap_ultra [ i ]) y_emcee = transit ( x , fsamps_trap_emcee [ i ], dfsamps_trap_emcee [ i ], psamps_trap_emcee [ i ], ttsamps_trap_emcee [ i ], tfsamps_trap_emcee [ i ], offsamps_trap_emcee [ i ]) ax3 . plot ( x , y_ultra , 'b-' , alpha = 0.008 , linewidth = 5 , label = 'UltraNest' if i == 0 else '' ) ax3 . plot ( x , y_emcee , 'r-' , alpha = 0.008 , linewidth = 5 , label = 'emcee' if i == 0 else '' ) ax3 . plot ( time , flux , 'k:' , linewidth = 3 ) ax3 . set_ylabel ( 'Flux / e-/s' ) ax3 . set_xlabel ( 'Time / days' ) ax3 . set_title ( 'emcee (red) and UltraNest (blue) overlayed posterior predictive plots' ) ax3 . set_ylim ( 541300 , 541600 ) ax3 . set_xlim ( 540 , 545 ) leg = ax3 . legend ( loc = 'lower right' ) for lh in leg . legendHandles : lh . set_alpha ( 0.8 ) plt . suptitle ( 'A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)' ) plt . show () Model comparisons Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap , logZerr_trap = results_trap [ 'logz' ], results_trap [ 'logzerr' ] logZ_square , logZerr_square = results_square [ 'logz' ], results_square [ 'logzerr' ] print ( \"Marginalised likelihood for trapezium transit model: {} \u00b1 {} \" . format ( logZ_trap , logZerr_trap )) print ( \"Marginalised likelihood for square transit model: {} \u00b1 {} \" . format ( logZ_square , logZerr_square )) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np . exp ( logZ_trap - logZ_square ) print ( \"Bayes factor: {} \" . format ( K )) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"Exoplanet Light Curve Analysis with emcee and UltraNest"},{"location":"LightCurve/LightCurve/#using-emcee-and-ultranest-to-model-the-light-curves-from-kepler-10","text":"Kepler-10 is a star located roughly 608 light years from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes in front of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intensity, occurring every time the exoplanet is in front of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existence of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA.","title":"Using emcee and UltraNest to model the light curves from Kepler-10"},{"location":"LightCurve/LightCurve/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.special import gammaln , ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt % matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print ( 'emcee version: {} ' . format ( mc . __version__ )) print ( 'UltraNest version: {} ' . format ( ultranest . __version__ )) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2","title":"Useful imports"},{"location":"LightCurve/LightCurve/#viewing-the-data","text":"Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (similar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits . open ( \"kplr011904151-2010265121752_llc.fits\" ) tab = table [ 1 ] data = Table ( tab . data ) flux_orig = data [ 'PDCSAP_FLUX' ][: 1325 ] time_orig = data [ 'TIME' ][: 1325 ] flux = [ flux_orig [ i ] for i in range ( len ( flux_orig )) if str ( flux_orig [ i ]) != \"nan\" ] time = [ time_orig [ i ] for i in range ( len ( time_orig )) if str ( flux_orig [ i ]) != \"nan\" ] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # plot all useful data ax1 . plot ( time , flux , \"k:\" ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # plot zoomed in view of transits ax2 . plot ( time , flux , \"k:\" ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541325 , 541700 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"Kepler-10 light curves showing evidence of exoplanet transits\" ) plt . show () The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy.","title":"Viewing the data"},{"location":"LightCurve/LightCurve/#the-model","text":"Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is similar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit ( time , f , df , p , tt , tf = None , off = 0 , square = False ): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immediately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None : tf = tt if tt <= tf : # Default to square shaped transit square = True y = [] if not square : # define slope of sides of trapezium h = f * df * tt / ( tt - tf ) grad = 2 * h / tt for i in time : j = ( i + off ) % p if j < tt : # transit # square shaped transit if square : y . append ( f * ( 1 - df )) # trapezium shaped transit elif j / tt < 0.5 : # first half of transit val = f - grad * j if val < f * ( 1 - df ): y . append ( f * ( 1 - df )) else : y . append ( val ) else : # last half of transit val = ( grad * j ) - 2 * h + f if val < f * ( 1 - df ): y . append ( f * ( 1 - df )) else : y . append ( val ) else : # no transit y . append ( f ) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models.","title":"The model"},{"location":"LightCurve/LightCurve/#modelling-with-emcee","text":"Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Due to the noise, I'll use a uniform prior on f, but a normal prior on other parameters. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # uniform prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [( f_min , f_max ), ( df_mu , df_sig ), ( p_mu , p_sig ), ( tt_mu , tt_sig ), ( tf_mu , tf_sig ), ( off_mu , off_sig )] # remove tf for square transit parameters priors_square = priors [: 4 ] + priors [ 5 :]","title":"Modelling with emcee"},{"location":"LightCurve/LightCurve/#sampling-the-data","text":"The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very similar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior ( theta ): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range ( len ( priors )): # sum log priors from each parameter if i == 0 : # prior for uniform parameters if priors [ i ][ 0 ] < theta [ i ] < priors [ i ][ 1 ]: pass else : lprior = - np . inf else : # prior for non-uniform parameters lprior -= 0.5 * (( theta [ i ] - priors [ i ][ 0 ]) / priors [ i ][ 1 ]) ** 2 return lprior def logprior_square ( theta ): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range ( len ( priors_square )): # sum log priors from each parameter if i == 0 : # prior for uniform parameters if priors_square [ i ][ 0 ] < theta [ i ] < priors_square [ i ][ 1 ]: pass else : lprior = - np . inf else : # prior for non-uniform parameters lprior -= 0.5 * (( theta [ i ] - priors_square [ i ][ 0 ]) / priors_square [ i ][ 1 ]) ** 2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike ( theta ): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like , df_like , p_like , tt_like , tf_like , off_like = theta # expected value lmbda = np . array ( transit ( time , f_like , df_like , p_like , tt_like , tf_like , off = off_like )) n = len ( flux ) a = np . sum ( gammaln ( np . array ( flux ) + 1 )) b = np . sum ( np . array ( flux ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def loglike_square ( theta ): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like , df_like , p_like , tt_like , off_like = theta # expected value lmbda = np . array ( transit ( time , f_like , df_like , p_like , tt_like , off = off_like , square = True )) n = len ( flux ) a = np . sum ( gammaln ( np . array ( flux ) + 1 )) b = np . sum ( np . array ( flux ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior ( theta ): lprior = logprior ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike ( theta ) def logposterior_square ( theta ): lprior = logprior_square ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike_square ( theta ) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" samples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range ( len ( priors )): if i == 0 : inisamples . append ( np . random . uniform ( priors [ i ][ 0 ], priors [ i ][ 1 ], Nens )) else : inisamples . append ( np . random . normal ( priors [ i ][ 0 ], priors [ i ][ 1 ], Nens )) inisamples = np . array ( inisamples ) . T inisamples_square = [] for i in range ( len ( priors_square )): if i == 0 : inisamples_square . append ( np . random . uniform ( priors_square [ i ][ 0 ], priors_square [ i ][ 1 ], Nens )) else : inisamples_square . append ( np . random . normal ( priors_square [ i ][ 0 ], priors_square [ i ][ 1 ], Nens )) inisamples_square = np . array ( inisamples_square ) . T ndims = inisamples . shape [ 1 ] ndims_square = inisamples_square . shape [ 1 ] # no. iterations Nburn = 500 Nsamples = 500 loglike . ncalls = 0 loglike_square . ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc . EnsembleSampler ( Nens , ndims , logposterior ) sampler_square = mc . EnsembleSampler ( Nens , ndims_square , logposterior_square ) # perform sampling t0 = timer () sampler . run_mcmc ( inisamples , Nsamples + Nburn ) t1 = timer () print ( \"time taken to sample a trapezium transit model with emcee: {} seconds\" . format ( t1 - t0 )) sampler_square . run_mcmc ( inisamples_square , Nsamples + Nburn ) t2 = timer () print ( \"time taken to sample a square transit model with emcee: {} seconds\" . format ( t2 - t1 )) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler . chain [:, Nburn :, :] . reshape (( - 1 , ndims )) samples_square = sampler_square . chain [:, Nburn :, :] . reshape (( - 1 , ndims_square ))","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results","text":"Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'Flux' , 'dFlux' , 'Period' , 'Transit Time' , 'Transit Flat Time' , 'Offset' ] plotposts ( samples_trapez , labels ) For each model, we can find the mean and standard deviation of each parameter using the traces. For the trapezium transit model: f , ferr = np . mean ( samples_trapez [:, 0 ]), np . std ( samples_trapez [:, 0 ]) df , dferr = np . mean ( samples_trapez [:, 1 ]), np . std ( samples_trapez [:, 1 ]) p , perr = np . mean ( samples_trapez [:, 2 ]), np . std ( samples_trapez [:, 2 ]) tt , tterr = np . mean ( samples_trapez [:, 3 ]), np . std ( samples_trapez [:, 3 ]) tf , tferr = np . mean ( samples_trapez [:, 4 ]), np . std ( samples_trapez [:, 4 ]) off , offerr = np . mean ( samples_trapez [:, 5 ]), np . std ( samples_trapez [:, 5 ]) print ( \"Parameters describing a trapezium shaped transit model: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f , ferr ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df , dferr ) + \" period = {} \\u00B1 {} days \\n \" . format ( p , perr ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt , tterr ) + \" flat transit time = {} \\u00B1 {} days \\n \" . format ( tf , tferr ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off , offerr )) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square , ferr_square = np . mean ( samples_square [:, 0 ] ), np . std ( samples_square [:, 0 ]) df_square , dferr_square = np . mean ( samples_square [:, 1 ] ), np . std ( samples_square [:, 1 ]) p_square , perr_square = np . mean ( samples_square [:, 2 ]), np . std ( samples_square [:, 2 ]) tt_square , tterr_square = np . mean ( samples_square [:, 3 ]), np . std ( samples_square [:, 3 ]) off_square , offerr_square = np . mean ( samples_square [:, 4 ]), np . std ( samples_square [:, 4 ]) print ( \"Parameters describing a square shaped transit model: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f_square , ferr_square ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df_square , dferr_square ) + \" period = {} \\u00B1 {} days \\n \" . format ( p_square , perr_square ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt_square , tterr_square ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off_square , offerr_square )) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b.","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior","text":"We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np . random . choice ( samples_trapez [:, 0 ], n_fits ) dfsamps_trap_emcee = np . random . choice ( samples_trapez [:, 1 ], n_fits ) psamps_trap_emcee = np . random . choice ( samples_trapez [:, 2 ], n_fits ) ttsamps_trap_emcee = np . random . choice ( samples_trapez [:, 3 ], n_fits ) tfsamps_trap_emcee = np . random . choice ( samples_trapez [:, 4 ], n_fits ) offsamps_trap_emcee = np . random . choice ( samples_trapez [:, 5 ], n_fits ) fsamps_square_emcee = np . random . choice ( samples_square [:, 0 ], n_fits ) dfsamps_square_emcee = np . random . choice ( samples_square [:, 1 ], n_fits ) psamps_square_emcee = np . random . choice ( samples_square [:, 2 ], n_fits ) ttsamps_square_emcee = np . random . choice ( samples_square [:, 3 ], n_fits ) offsamps_square_emcee = np . random . choice ( samples_square [:, 4 ], n_fits ) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f , df , p , tt , tf , off ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_trap_emcee [ i ], dfsamps_trap_emcee [ i ], psamps_trap_emcee [ i ], ttsamps_trap_emcee [ i ], tfsamps_trap_emcee [ i ], offsamps_trap_emcee [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\" ) plt . show () The same process can be repeated for the square transit model: fig , ( ax1 , ax2 ) = plt . subplots ( 2 , 1 , figsize = ( 15 , 6 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f_square , df_square , p_square , tt_square , off = off_square ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_square_emcee [ i ], dfsamps_square_emcee [ i ], psamps_square_emcee [ i ], ttsamps_square_emcee [ i ], off = offsamps_square_emcee [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_xlabel ( \"Time / days\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) plt . suptitle ( \"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\" ) plt . show () From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system.","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons","text":"Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. For the derivations on equations for the following calculations, check out this paper . For each property, I'll state the most accpeted values found here . The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs , RpRs_err = np . sqrt ( df ), dferr / ( np . sqrt ( df )) RpRs_square , RpRs_square_err = np . sqrt ( df_square ), dferr_square / ( np . sqrt ( df_square )) print ( \"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {} \" . format ( RpRs , RpRs_err )) print ( \"Planet to star radius ratio (square transit model): {} \\u00B1 {} \" . format ( RpRs_square , RpRs_square_err )) print ( \"Accepted planet to star radius ratio: {} \" . format ( 0.0127 )) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 Accepted planet to star radius ratio: 0.0127 Both models are in close agreement with each other, and are quite close to the accepted value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and accepted ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square , tferr_square = 0.99 * tt_square , 0.99 * tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor ( df , tt , tf ): # return semi-minor axis, given model parameters numerator = ( tt ** 2 ) * ( 1 - np . sqrt ( df )) ** 2 - ( tf ** 2 ) * ( 1 + np . sqrt ( df )) ** 2 denominator = tt ** 2 - tf ** 2 return np . sqrt ( numerator / denominator ) def normsemimajor ( df , p , tt , tf ): # return normalised semi-major axis, given model parameters numerator = 2 * p * ( df ) ** ( 1 / 4 ) denominator = np . pi * np . sqrt ( tt ** 2 - tf ** 2 ) return numerator / denominator # semiminor for trapezium model b_trap = semiminor ( df , tt , tf ) b_trap_err = abs ( semiminor ( df + dferr , tt , tf ) - b_trap ) # semiminor for square model b_square = semiminor ( df_square , tt_square , tf_square ) b_square_err = abs ( semiminor ( df_square + dferr_square , tt_square , tf_square ) - b_square ) # semimajor for trapezium model aRs_trap = normsemimajor ( df , p , tt , tf ) aRs_trap_err = abs ( normsemimajor ( df + dferr , p + perr , tt + tterr , tf + tferr ) - aRs_trap ) # semimajor for square model aRs_square = normsemimajor ( df_square , p_square , tt_square , tf_square ) aRs_square_err = abs ( normsemimajor ( df_square + dferr_square , p_square + perr_square , tt_square + tterr_square , tf_square + tferr_square ) - aRs_square ) print ( \"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {} \" . format ( aRs_trap , aRs_trap_err )) print ( \"Normalised semi-major axis (square transit model): {} \\u00B1 {} \" . format ( aRs_square , aRs_square_err )) print ( \"Accepted normalised semi-major axis: {} \" . format ( 3.40 )) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 Accepted normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np . arccos ( b_trap / aRs_trap ) * ( 180 / np . pi ) I_trap_err = abs ( np . arccos (( b_trap + b_trap_err ) / ( aRs_trap - aRs_trap_err )) * ( 180 / np . pi ) - I_trap ) I_square = np . arccos ( b_square / aRs_square ) * ( 180 / np . pi ) I_square_err = abs ( np . arccos (( b_square + b_square_err ) / ( aRs_square - aRs_square_err ) ) * ( 180 / np . pi ) - I_square ) print ( \"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\" . format ( I_trap , I_trap_err )) print ( \"Inclination angle (square transit model): {} \\u00B1 {} degrees\" . format ( I_square , I_square_err )) print ( \"Accepted inclination angle: {} degrees\" . format ( 84.4 )) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees Accepted inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density ( df , p , tt , aRs ): # return density of star given model parameters G = 6.67408e-11 wt = tt * np . pi / p # transform p from days to seconds p *= 86400 numerator = 3 * np . pi * aRs ** 3 denominator = G * p ** 2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density ( df , p , tt , aRs_trap ) stard_trap_err = abs ( star_density ( df + dferr , p - perr , tt - tterr , aRs_trap + aRs_trap_err ) - stard_trap ) # star density predicted by square model stard_square = star_density ( df_square , p_square , tt_square , aRs_square ) stard_square_err = abs ( star_density ( df_square + dferr_square , p_square - perr_square , tt_square - tterr_square , aRs_square + aRs_square_err ) - stard_square ) print ( \"Star density (trapezium transit model): {} \\u00B1 {} kg/m \\u00b3 \" . format ( stard_trap , stard_trap_err )) print ( \"Star density (square transit model): {} \\u00B1 {} kg/m \\u00b3 \" . format ( stard_square , stard_square_err )) print ( \"True star density: {} kg/m \\u00b3 \" . format ( 1070 )) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only in front of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above.","title":"Model comparisons"},{"location":"LightCurve/LightCurve/#modelling-with-ultranest","text":"If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package.","title":"Modelling with UltraNest"},{"location":"LightCurve/LightCurve/#sampling-the-data_1","text":"This works in a similar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform ( theta ): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [ 0 for i in range ( len ( theta ))] for i in range ( len ( theta )): if i == 0 : # uniform transform for f params [ i ] = ( priors [ i ][ 1 ] - priors [ i ][ 0 ]) * theta [ i ] + priors [ i ][ 0 ] else : # normal transform for remaining parameters params [ i ] = priors [ i ][ 0 ] + priors [ i ][ 1 ] * ndtri ( theta [ i ]) return np . array ( params ) def prior_transform_square ( theta ): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [ 0 for i in range ( len ( theta ))] for i in range ( len ( theta )): if i == 0 : # uniform transform for f params [ i ] = ( priors_square [ i ][ 1 ] - priors_square [ i ][ 0 ] ) * theta [ i ] + priors_square [ i ][ 0 ] else : # normal transform for remaining parameters params [ i ] = priors_square [ i ][ 0 ] + priors_square [ i ][ 1 ] * ndtri ( theta [ i ]) return np . array ( params ) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest . ReactiveNestedSampler ([ 'f' , 'df' , 'p' , 'tt' , 'tf' , 'off' ], loglike , prior_transform ) sampler_square = ultranest . ReactiveNestedSampler ([ 'f' , 'df' , 'p' , 'tt' , 'off' ], loglike_square , prior_transform_square ) # use \"slice\" sampler, due to high dimensionality nsteps = 2 * len ( priors ) sampler . stepsampler = stepsampler . RegionSliceSampler ( nsteps = nsteps ) sampler_square . stepsampler = stepsampler . RegionSliceSampler ( nsteps = nsteps ) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer () results_trap = sampler . run ( min_num_live_points = nlive ) t1 = timer () results_square = sampler_square . run ( min_num_live_points = nlive ) t2 = timer () print ( ' \\n \\n ' + 'Time taken to sample trapezium shaped model with UltraNest: {} seconds' . format ( t1 - t0 )) print ( 'Time taken to sample square shaped model with UltraNest: {} seconds' . format ( t2 - t1 )) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results_1","text":"To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np . array ( results_trap [ \"weighted_samples\" ][ \"points\" ]) weights_trap = np . array ( results_trap [ \"weighted_samples\" ][ \"weights\" ]) resample_trap = np . random . rand ( len ( weights_trap )) < weights_trap / max ( weights_trap ) samples_trap_ultra = samples_points_trap [ resample_trap , :] The mean parameter values and their errors can be found easily: # parameter means and errors for trapezium transit f_trap_ultra , ferr_trap_ultra = np . mean ( samples_trap_ultra [:, 0 ] ), np . std ( samples_trap_ultra [:, 0 ]) df_trap_ultra , dferr_trap_ultra = np . mean ( samples_trap_ultra [:, 1 ]) ), np . std ( samples_trap_ultra [:, 1 ]) p_trap_ultra , perr_trap_ultra = np . mean ( samples_trap_ultra [:, 2 ] ), np . std ( samples_trap_ultra [:, 2 ]) tt_trap_ultra , tterr_trap_ultra = np . mean ( samples_trap_ultra [:, 3 ] ), np . std ( samples_trap_ultra [:, 3 ]) tf_trap_ultra , tferr_trap_ultra = np . mean ( samples_trap_ultra [:, 4 ] ), np . std ( samples_trap_ultra [:, 4 ]) off_trap_ultra , offerr_trap_ultra = np . mean ( samples_trap_ultra [:, 5 ] ), np . std ( samples_trap_ultra [:, 5 ]) print ( \"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n \" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n \" . format ( f_trap_ultra , ferr_trap_ultra ) + \"fractional flux decrease = {} \\u00B1 {} \\n \" . format ( df_trap_ultra , dferr_trap_ultra ) + \" period = {} \\u00B1 {} days \\n \" . format ( p_trap_ultra , perr_trap_ultra ) + \" total transit time = {} \\u00B1 {} days \\n \" . format ( tt_trap_ultra , tterr_trap_ultra ) + \" flat transit time = {} \\u00B1 {} days \\n \" . format ( tf_trap_ultra , tferr_trap_ultra ) + \" offset = {} \\u00B1 {} days \\n \" . format ( off_trap_ultra , offerr_trap_ultra )) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior_1","text":"We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 0 ], nfits ) dfsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 1 ], nfits ) psamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 2 ], nfits ) ttsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 3 ], nfits ) tfsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 4 ], nfits ) offsamps_trap_ultra = np . random . choice ( samples_trap_ultra [:, 5 ], nfits ) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , figsize = ( 15 , 10 )) # mean plot ax1 . plot ( time , flux , \"k:\" , linewidth = 3 ) x = np . linspace ( min ( time ), max ( time ), 1500 ) y = transit ( x , f_trap_ultra , df_trap_ultra , p_trap_ultra , tt_trap_ultra , tf_trap_ultra , off_trap_ultra ) ax1 . plot ( x , y , \"b-\" , alpha = 0.8 ) ax1 . set_ylabel ( \"Flux / e-/s\" ) ax1 . set_title ( \"UltraNest mean posterior plot\" ) # posterior predictive plot for i in range ( n_fits ): y = transit ( x , fsamps_trap_ultra [ i ], dfsamps_trap_ultra [ i ], psamps_trap_ultra [ i ], ttsamps_trap_ultra [ i ], tfsamps_trap_ultra [ i ], offsamps_trap_ultra [ i ]) ax2 . plot ( x , y , \"b-\" , alpha = 0.01 , linewidth = 5 ) ax2 . plot ( time , flux , \"k:\" , linewidth = 3 ) ax2 . set_ylabel ( \"Flux / e-/s\" ) ax2 . set_title ( \"UltraNest posterior predictive plot\" ) ax2 . set_ylim ( 541300 , 541600 ) ax2 . set_xlim ( 540 , 545 ) # emcee and UltraNest overlapping posterior predictive plot for i in range ( n_fits ): y_ultra = transit ( x , fsamps_trap_ultra [ i ], dfsamps_trap_ultra [ i ], psamps_trap_ultra [ i ], ttsamps_trap_ultra [ i ], tfsamps_trap_ultra [ i ], offsamps_trap_ultra [ i ]) y_emcee = transit ( x , fsamps_trap_emcee [ i ], dfsamps_trap_emcee [ i ], psamps_trap_emcee [ i ], ttsamps_trap_emcee [ i ], tfsamps_trap_emcee [ i ], offsamps_trap_emcee [ i ]) ax3 . plot ( x , y_ultra , 'b-' , alpha = 0.008 , linewidth = 5 , label = 'UltraNest' if i == 0 else '' ) ax3 . plot ( x , y_emcee , 'r-' , alpha = 0.008 , linewidth = 5 , label = 'emcee' if i == 0 else '' ) ax3 . plot ( time , flux , 'k:' , linewidth = 3 ) ax3 . set_ylabel ( 'Flux / e-/s' ) ax3 . set_xlabel ( 'Time / days' ) ax3 . set_title ( 'emcee (red) and UltraNest (blue) overlayed posterior predictive plots' ) ax3 . set_ylim ( 541300 , 541600 ) ax3 . set_xlim ( 540 , 545 ) leg = ax3 . legend ( loc = 'lower right' ) for lh in leg . legendHandles : lh . set_alpha ( 0.8 ) plt . suptitle ( 'A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)' ) plt . show ()","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons_1","text":"Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap , logZerr_trap = results_trap [ 'logz' ], results_trap [ 'logzerr' ] logZ_square , logZerr_square = results_square [ 'logz' ], results_square [ 'logzerr' ] print ( \"Marginalised likelihood for trapezium transit model: {} \u00b1 {} \" . format ( logZ_trap , logZerr_trap )) print ( \"Marginalised likelihood for square transit model: {} \u00b1 {} \" . format ( logZ_square , logZerr_square )) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np . exp ( logZ_trap - logZ_square ) print ( \"Bayes factor: {} \" . format ( K )) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/","text":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133. Useful imports # numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri , gammaln # Plotting import corner import matplotlib.pyplot as plt % matplotlib inline # Samplers import pymc3 as pm print ( 'PyMC3 version: {} ' . format ( pm . __version__ )) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print ( 'dynesty version: {} ' . format ( dynesty . __version__ )) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1 Viewing the data The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\" . This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open ( cali_dir , 'r' ) as file : calibration = file . read () . splitlines () calibration = list ( map ( float , calibration )) c_0 = calibration [ 0 ] c_2 = calibration [ 2 ] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open ( spectra_dir , 'r' ) as file : counts = [ int ( j ) for j in file ] counts = counts [ 2 :] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np . array ( range ( len ( counts ))) # Bins for gamma-ray spectrum # Plot the spectrum plt . figure ( figsize = ( 15 , 5 )) plt . plot ( xrange , counts , 'b' ) plt . fill ( xrange , counts , 'b' , alpha = 0.4 ) plt . xlabel ( 'Energy / MCA Channels' ) plt . ylabel ( 'Counts' ) plt . title ( 'Gamma-Ray Spectrum of a sample of Ba-133' ) plt . yscale ( 'log' ) plt . xlim ( 540 , 3500 ) plt . show () The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks ( counts , height = 1300 , distance = 100 )[ 0 ][ 3 :] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Let's plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [ c_0 * np . array ( range ( peak - ranger , peak + ranger )) + c_2 for peak in peaks ] peaks_y = [ counts [ peak - ranger : peak + ranger ] for peak in peaks ] # Plot selected peaks from gamma-ray spectrum fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 7 )) for i in range ( 2 ): for j in range ( 3 ): ind = 3 * i + j axs [ i , j ] . plot ( peaks_x [ ind ], peaks_y [ ind ], 'b' ) axs [ i , j ] . fill ( peaks_x [ ind ], peaks_y [ ind ], 'b' , alpha = 0.2 ) if i == 1 : axs [ i , j ] . set_xlabel ( 'Energy / KeV' ) if j == 0 : axs [ i , j ] . set_ylabel ( 'Counts' ) fig . suptitle ( 'Photopeaks produced by a sample of Ba-133' , y = 0.95 ) plt . show () The model The decays that cause the photopeaks in a GRS have a discrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature , rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian function to be used: def gauss ( x , a , xc , w , y0 ): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a * np . exp ( - ( x - xc ) ** 2 / ( 2 * w ** 2 )) + y0 Modelling with PyMC3 Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [ pm . Model () for i in range ( len ( peaks ))] a_guesses = [ 23000. , 900. , 6100. , 13800. , 39800. , 5300. ] xc_guesses = [ 81. , 161. , 276.5 , 303. , 356. , 384. ] y0_guesses = [ 1700. , 1350. , 300. , 300. , 250. , 50. ] Sampling the data Next, I'll use the above guesses to initialise each model. PyMC3 requires the user to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probability distribution functions. I'll be using a Poisson likelihood, since Poisson distributions show how many times an event is likely to occur in a certain time period, assuming that events are independent and occur at a constant rate. Since radioactive decays are independant, and the half-life of Ba-133 is around 10.5 years, this makes a Poisson distribution suited for photon counting. This is done within the scope of each model defined above, using the \"with\" statement: for i in range ( len ( peaks )): with gauss_models [ i ]: # set prior parameters # amplitude a_mu = a_guesses [ i ] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses [ i ] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses [ i ] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm . Normal ( 'Amplitude' , mu = a_mu , sd = a_sig ) xc_model = pm . Normal ( 'Peak Energy' , mu = xc_mu , sd = xc_sig ) w_model = pm . Uniform ( 'Standard Deviation' , lower = w_min , upper = w_max ) y0_model = pm . Normal ( 'Background Counts' , mu = y0_mu , sd = y0_sig ) # Expected value of outcome mu = gauss ( peaks_x [ i ], a_model , xc_model , w_model , y0_model ) # Poisson likelihood of observations Y_obs = pm . Poisson ( 'Y_obs' , mu = mu , observed = peaks_y [ i ]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging . getLogger ( 'pymc3' ) logger . setLevel ( logging . ERROR ) # perform sampling traces = [] t0 = time () for i in range ( len ( peaks )): with gauss_models [ i ]: traces . append ( pm . sample ( Nsamples , tune = Ntune , discard_tuned_samples = True )) t1 = time () timepymc3 = t1 - t0 # time taken to sample all of the photopeaks print ( ' {} seconds ( {} seconds per peak) taken to run PyMC3 sampling.' . format ( timepymc3 , timepymc3 / 6 )) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling. Sampling the data - Alternate method The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probability distribution in log-space, meaning the standard deviation has an equal probability of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using similar code as before, but with the new likelihood. gauss_model_alt = pm . Model () with gauss_model_alt : # set prior parameters # amplitude a_mu = a_guesses [ 1 ] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses [ 1 ] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses [ 1 ] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = - 1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm . Normal ( 'Amplitude' , mu = a_mu , sd = a_sig ) xc_model = pm . Normal ( 'Peak Energy' , mu = xc_mu , sd = xc_sig ) w_model = pm . Normal ( 'Standard Deviation' , mu = w_mu , sd = w_sig ) y0_model = pm . Normal ( 'Background Counts' , mu = y0_mu , sd = y0_sig ) # set uniform prior sigma_model = pm . Uniform ( 'Noise' , lower = sigma_min , upper = sigma_max ) # Expected value of outcome mu = gauss ( peaks_x [ 1 ], a_model , xc_model , w_model , y0_model ) # Normal likelihood of observations with noise Y_obs = pm . Normal ( 'Y_obs' , mu = mu , sd = 10 ** sigma_model , observed = peaks_y [ 1 ]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time () with gauss_model_alt : trace_alt = pm . sample ( Nsamples , tune = Ntune , discard_tuned_samples = True ) t1_alt = time () timepymc3_alt = t1_alt - t0_alt print ( ' {} seconds taken to run PyMC3 alternate sampling.' . format ( timepymc3_alt )) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np . vstack (( trace_alt [ 'Amplitude' ], trace_alt [ 'Peak Energy' ], trace_alt [ 'Standard Deviation' ], trace_alt [ 'Background Counts' ], trace_alt [ 'Noise' ])) . T # mean and standard deviation error of each parameter a_alt , a_err_alt = np . mean ( samples_alt [:, 0 ]), np . std ( samples_alt [:, 0 ]) xc_alt , xc_err_alt = np . mean ( samples_alt [:, 1 ]), np . std ( samples_alt [:, 1 ]) w_alt , w_err_alt = np . mean ( samples_alt [:, 2 ]), np . std ( samples_alt [:, 2 ]) y0_alt , y0_err_alt = np . mean ( samples_alt [:, 3 ]), np . std ( samples_alt [:, 3 ]) sigma_alt , sigma_err_alt = np . mean ( samples_alt [:, 4 ]), np . std ( samples_alt [:, 4 ]) a_w_alt_cov = np . cov ( samples_alt )[ 0 , 2 ] # print values print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a_alt , a_err_alt ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc_alt , xc_err_alt ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w_alt , w_err_alt ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0_alt , y0_err_alt ) + ' Noise = {} \\u00B1 {} counts' . format ( sigma_alt , sigma_err_alt )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts Results Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [ np . vstack (( trace [ 'Amplitude' ], trace [ 'Peak Energy' ], trace [ 'Standard Deviation' ], trace [ 'Background Counts' ])) . T for trace in traces ] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range ( len ( peaks ))] for ind in range ( len ( peaks )): resdict [ ind ][ 'a_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 0 ]) resdict [ ind ][ 'a_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 0 ]) resdict [ ind ][ 'xc_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 1 ]) resdict [ ind ][ 'xc_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 1 ]) resdict [ ind ][ 'w_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 2 ]) resdict [ ind ][ 'w_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 2 ]) resdict [ ind ][ 'y0_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 3 ]) resdict [ ind ][ 'y0_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 3 ]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts ( samples , labels , ** kwargs ): \"\"\" Function to plot posteriors using corner.py and scipy's Gaussian KDE function. \"\"\" fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) plt . subplots_adjust ( wspace = 0.2 , hspace = 0.2 ) # plot KDE smoothed version of distributions for axidx , samps in zip ([ 0 , 5 , 10 , 15 ], samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 100 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = \"firebrick\" ) # create corner plot for peak with noisiest data labels = [ r 'Amplitude' , r 'Peak Energy' , r 'Standard Deviation' , r 'Background Counts' ] corner_plot_samples = all_pymc3_samples [ 1 ] plotposts ( corner_plot_samples , labels ) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a , a_err = resdict [ 1 ][ 'a_mu' ], resdict [ 1 ][ 'a_sig' ] xc , xc_err = resdict [ 1 ][ 'xc_mu' ], resdict [ 1 ][ 'xc_sig' ] w , w_err = resdict [ 1 ][ 'w_mu' ], resdict [ 1 ][ 'w_sig' ] y0 , y0_err = resdict [ 1 ][ 'y0_mu' ], resdict [ 1 ][ 'y0_sig' ] print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a , a_err ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc , xc_err ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w , w_err ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0 , y0_err )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts Plotting the posterior Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 7 )) for i in range ( 2 ): for j in range ( 3 ): ind = 3 * i + j a = resdict [ ind ][ 'a_mu' ] xc = resdict [ ind ][ 'xc_mu' ] w = resdict [ ind ][ 'w_mu' ] y0 = resdict [ ind ][ 'y0_mu' ] x = peaks_x [ ind ] y = peaks_y [ ind ] # plot original data axs [ i , j ] . plot ( x , y , 'b.' , alpha = 1 , label = ( 'Original Data' if all ( num == 0 for num in [ i , j ]) else '' )) # plot fitted curve over the data xsmooth = np . linspace ( x [ 0 ], x [ - 1 ], len ( x ) * 100 ) axs [ i , j ] . plot ( xsmooth , gauss ( xsmooth , a , xc , w , y0 ), 'k:' , alpha = 1 , label = ( 'Fitted Model' if all ( num == 0 for num in [ i , j ]) else '' )) if i == 1 : axs [ i , j ] . set_xlabel ( 'Energy / keV' ) if j == 0 : axs [ i , j ] . set_ylabel ( 'Counts' ) leg = fig . legend ( loc = 'lower right' , numpoints = 1 ) for lh in leg . legendHandles : lh . set_alpha ( 1 ) fig . suptitle ( 'Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling' ) plt . show () Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3 , xc_samps_pymc3 , w_samps_pymc3 , y0_samps_pymc3 = ([] for i in range ( 4 )) for ind in range ( len ( peaks )): a_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 0 ], size = n_fits )) xc_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 1 ], size = n_fits )) w_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 2 ], size = n_fits )) y0_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 3 ], size = n_fits )) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x [ ind ] xsmooth = np . linspace ( x [ 0 ], x [ - 1 ], len ( x ) * 100 ) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) for i in range ( n_fits ): ax1 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ ind ][ i ], xc_samps_pymc3 [ ind ][ i ], w_samps_pymc3 [ ind ][ i ], y0_samps_pymc3 [ ind ][ i ]), 'b-' , alpha = 0.01 , linewidth = 2 ) ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ ind ][ i ], xc_samps_pymc3 [ ind ][ i ], w_samps_pymc3 [ ind ][ i ], y0_samps_pymc3 [ ind ][ i ]), 'b-' , alpha = 0.02 , linewidth = 2 ) ax1 . set_ylabel ( 'Counts' ) ax1 . set_xlabel ( 'Energy / keV' ) ax2 . set_xlim ( 159.4 , 161.8 ) ax2 . set_ylim ( 1800 , 2250 ) ax2 . set_xlabel ( 'Energy / keV' ) fig . suptitle ( 'Posterior predictive plot for a photopeak from a sample of Ba-133' ) plt . show () Model comparisons Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak. When finding the area error, we must also consider the covariance between the amplitude and standard deviation, which I show below: # parameter means and standard deviations of peak a_pymc3 , aerr_pymc3 = resdict [ 1 ][ 'a_mu' ], resdict [ 1 ][ 'a_sig' ] xc_pymc3 , xcerr_pymc3 = resdict [ 1 ][ 'xc_mu' ], resdict [ 1 ][ 'xc_sig' ] w_pymc3 , werr_pymc3 = resdict [ 1 ][ 'w_mu' ], resdict [ 1 ][ 'w_sig' ] y0_pymc3 , y0err_pymc3 = resdict [ 1 ][ 'y0_mu' ], resdict [ 1 ][ 'y0_sig' ] a_w_cov = np . cov ( all_pymc3_samples [ 1 ])[ 0 , 2 ] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate . quad ( lambda t : gauss ( t , a_pymc3 , xc_pymc3 , w_pymc3 , y0_pymc3 ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err = np . sqrt ( 2 * np . pi * (( w_pymc3 * aerr_pymc3 ) ** 2 + ( a_pymc3 * werr_pymc3 ) ** 2 ) + 2 * a * w * a_w_cov ) / c_0 percent_err = 100 * peak_integral_err / peak_integral print ( 'Total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral , peak_integral_err ) + 'Percentage error = {} %' . format ( percent_err )) Total counts = 22933.86204613347 \u00b1 195.58699451020811 counts Percentage error = 0.8500378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate . quad ( lambda t : gauss ( t , a_alt , xc_alt , w_alt , y0_alt ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err_alt = np . sqrt ( 2 * np . pi * (( w_alt * a_err_alt ) ** 2 + ( a_alt * w_err_alt ) ** 2 ) + 2 * a_alt * w_alt * a_w_alt_cov ) / c_0 percent_err_alt = 100 * peak_integral_err_alt / peak_integral_alt print ( 'Alternate total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral_alt , peak_integral_err_alt ) + 'Alternate percentage error = {} %' . format ( percent_err_alt )) Alternate total counts = 22943.751210742565 \u00b1 193.76313608385547 counts Alternate percentage error = 0.8447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful. Modelling with dynesty Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they produce simillar results. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here. Sampling the data Using dynesty is slightly more complicated than PyMC3. Most nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform ( theta ): # unpack the transformed parameters a_t , xc_t , w_t , y0_t = theta # define our prior guesses for each parameter a_mu , a_sig = a_guesses [ 1 ], 50. xc_mu , xc_sig = xc_guesses [ 1 ], 1. w_mu , w_sig = 0.7 , 0.3 y0_mu , y0_sig = y0_guesses [ 1 ], 30. # convert back to a = a_mu + a_sig * ndtri ( a_t ) xc = xc_mu + xc_sig * ndtri ( xc_t ) w = w_mu + w_sig * ndtri ( w_t ) y0 = y0_mu + y0_sig * ndtri ( y0_t ) return a , xc , w , y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike ( theta ): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like , xc_like , w_like , y0_like = theta # expected value lmbda = np . array ( gauss ( peaks_x [ 1 ], a_like , xc_like , w_like , y0_like )) n = len ( peaks_y [ 1 ]) a = np . sum ( gammaln ( np . array ( peaks_y [ 1 ]) + 1 )) b = np . sum ( np . array ( peaks_y [ 1 ]) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points , sampling algorithm, sampling method, and a stopping criterion . Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler ( loglike , priortransform , nparam , bound = 'multi' , sample = 'unif' , nlive = 1000 ) t0_dynesty = time () sampler . run_nested ( dlogz = stop , print_progress = False ) t1_dynesty = time () print ( ' {} seconds taken to run dynesty sampling' . format ( t1_dynesty - t0_dynesty )) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler . results print ( results . summary ()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None Results To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np . exp ( results [ 'logwt' ] - results [ 'logz' ][ - 1 ]) samples = results . samples dynesty_samples = resample_equal ( samples , weights ) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a , xc , w , y0 = [ dynesty_samples [:, i ] for i in range ( 4 )] a_dynesty , aerr_dynesty = np . mean ( a ), np . std ( a ) xc_dynesty , xcerr_dynesty = np . mean ( xc ), np . std ( xc ) w_dynesty , werr_dynesty = np . mean ( w ), np . std ( w ) y0_dynesty , y0err_dynesty = np . mean ( y0 ), np . std ( y0 ) a_w_dynesty_cov = np . cov ( dynesty_samples )[ 0 , 2 ] nfits = 300 a_samps_dynesty = np . random . normal ( a_dynesty , aerr_dynesty , nfits ) xc_samps_dynesty = np . random . normal ( xc_dynesty , xcerr_dynesty , nfits ) w_samps_dynesty = np . random . normal ( w_dynesty , werr_dynesty , nfits ) y0_samps_dynesty = np . random . normal ( y0_dynesty , y0err_dynesty , nfits ) print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a_dynesty , aerr_dynesty ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc_dynesty , xcerr_dynesty ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w_dynesty , werr_dynesty ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0_dynesty , y0err_dynesty )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts Plotting the posterior So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x , y = peaks_x [ 1 ], peaks_y [ 1 ] fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) xsmooth = np . linspace ( min ( x ), max ( x ), 1000 ) # mean posterior plot ax1 . plot ( x , y , 'b.' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 'k:' ) ax1 . set_ylim ( 1200 ) # posterior predictive plot for i in range ( nfits ): ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_dynesty [ i ], xc_samps_dynesty [ i ], w_samps_dynesty [ i ], y0_samps_dynesty [ i ]), 'b-' , alpha = 0.01 ) ax2 . set_ylim ( 1200 ) plt . suptitle ( 'Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of \\n ' + ' Ba-133, sampled by dynesty' ) plt . show () Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x , y = peaks_x [ 1 ], peaks_y [ 1 ] fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) xsmooth = np . linspace ( min ( x ), max ( x ), 1000 ) # mean posterior plot ax1 . plot ( x , y , 'k.' , label = 'Data' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 'b:' , label = 'dynesty' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_pymc3 , xc_pymc3 , w_pymc3 , y0_pymc3 ), 'r:' , label = 'PyMC3' ) ax1 . set_ylim ( 1200 ) leg1 = ax1 . legend ( loc = 'upper right' ) for lh in leg1 . legendHandles : lh . set_alpha ( 1 ) # posterior predictive plot for i in range ( nfits ): ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_dynesty [ i ], xc_samps_dynesty [ i ], w_samps_dynesty [ i ], y0_samps_dynesty [ i ]), 'b-' , alpha = 0.01 , label = ( 'dynesty' if i == 0 else '' )) ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ 1 ][ i ], xc_samps_pymc3 [ 1 ][ i ], w_samps_pymc3 [ 1 ][ i ], y0_samps_pymc3 [ 1 ][ i ]), 'r-' , alpha = 0.01 , label = ( 'PyMC3' if i == 0 else '' )) ax2 . set_ylim ( 1200 ) leg2 = ax2 . legend ( loc = 'upper right' ) for lh in leg2 . legendHandles : lh . set_alpha ( 1 ) plt . suptitle ( 'Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + ' \\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)' ) plt . show () Model comparisons This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate . quad ( lambda t : gauss ( t , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err_dynesty = np . sqrt ( 2 * np . pi * (( w_dynesty * aerr_dynesty ) ** 2 + ( a_dynesty * werr_dynesty ) ** 2 ) + 2 * a_dynesty * w_dynesty + a_w_dynesty_cov ) / c_0 percent_err_dynesty = 100 * peak_integral_err_dynesty / peak_integral_dynesty print ( 'Total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral_dynesty , peak_integral_err_dynesty ) + 'Percentage error = {} %' . format ( percent_err_dynesty )) Total counts = 22968.854117562147 \u00b1 201.03466521968398 counts Percentage error = 0.8793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error under 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"Gamma-Ray Spectroscopy with PyMC3 and dynesty"},{"location":"PyMC3_GRS/PyMC3_GRS/#using-pymc3-and-dynesty-to-fit-gaussian-curves-to-photopeaks-in-a-gamma-ray-spectrum","text":"A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133.","title":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum"},{"location":"PyMC3_GRS/PyMC3_GRS/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri , gammaln # Plotting import corner import matplotlib.pyplot as plt % matplotlib inline # Samplers import pymc3 as pm print ( 'PyMC3 version: {} ' . format ( pm . __version__ )) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print ( 'dynesty version: {} ' . format ( dynesty . __version__ )) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1","title":"Useful imports"},{"location":"PyMC3_GRS/PyMC3_GRS/#viewing-the-data","text":"The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\" . This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open ( cali_dir , 'r' ) as file : calibration = file . read () . splitlines () calibration = list ( map ( float , calibration )) c_0 = calibration [ 0 ] c_2 = calibration [ 2 ] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open ( spectra_dir , 'r' ) as file : counts = [ int ( j ) for j in file ] counts = counts [ 2 :] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np . array ( range ( len ( counts ))) # Bins for gamma-ray spectrum # Plot the spectrum plt . figure ( figsize = ( 15 , 5 )) plt . plot ( xrange , counts , 'b' ) plt . fill ( xrange , counts , 'b' , alpha = 0.4 ) plt . xlabel ( 'Energy / MCA Channels' ) plt . ylabel ( 'Counts' ) plt . title ( 'Gamma-Ray Spectrum of a sample of Ba-133' ) plt . yscale ( 'log' ) plt . xlim ( 540 , 3500 ) plt . show () The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks ( counts , height = 1300 , distance = 100 )[ 0 ][ 3 :] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Let's plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [ c_0 * np . array ( range ( peak - ranger , peak + ranger )) + c_2 for peak in peaks ] peaks_y = [ counts [ peak - ranger : peak + ranger ] for peak in peaks ] # Plot selected peaks from gamma-ray spectrum fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 7 )) for i in range ( 2 ): for j in range ( 3 ): ind = 3 * i + j axs [ i , j ] . plot ( peaks_x [ ind ], peaks_y [ ind ], 'b' ) axs [ i , j ] . fill ( peaks_x [ ind ], peaks_y [ ind ], 'b' , alpha = 0.2 ) if i == 1 : axs [ i , j ] . set_xlabel ( 'Energy / KeV' ) if j == 0 : axs [ i , j ] . set_ylabel ( 'Counts' ) fig . suptitle ( 'Photopeaks produced by a sample of Ba-133' , y = 0.95 ) plt . show ()","title":"Viewing the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#the-model","text":"The decays that cause the photopeaks in a GRS have a discrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature , rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian function to be used: def gauss ( x , a , xc , w , y0 ): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a * np . exp ( - ( x - xc ) ** 2 / ( 2 * w ** 2 )) + y0","title":"The model"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-pymc3","text":"Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [ pm . Model () for i in range ( len ( peaks ))] a_guesses = [ 23000. , 900. , 6100. , 13800. , 39800. , 5300. ] xc_guesses = [ 81. , 161. , 276.5 , 303. , 356. , 384. ] y0_guesses = [ 1700. , 1350. , 300. , 300. , 250. , 50. ]","title":"Modelling with PyMC3"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data","text":"Next, I'll use the above guesses to initialise each model. PyMC3 requires the user to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probability distribution functions. I'll be using a Poisson likelihood, since Poisson distributions show how many times an event is likely to occur in a certain time period, assuming that events are independent and occur at a constant rate. Since radioactive decays are independant, and the half-life of Ba-133 is around 10.5 years, this makes a Poisson distribution suited for photon counting. This is done within the scope of each model defined above, using the \"with\" statement: for i in range ( len ( peaks )): with gauss_models [ i ]: # set prior parameters # amplitude a_mu = a_guesses [ i ] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses [ i ] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses [ i ] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm . Normal ( 'Amplitude' , mu = a_mu , sd = a_sig ) xc_model = pm . Normal ( 'Peak Energy' , mu = xc_mu , sd = xc_sig ) w_model = pm . Uniform ( 'Standard Deviation' , lower = w_min , upper = w_max ) y0_model = pm . Normal ( 'Background Counts' , mu = y0_mu , sd = y0_sig ) # Expected value of outcome mu = gauss ( peaks_x [ i ], a_model , xc_model , w_model , y0_model ) # Poisson likelihood of observations Y_obs = pm . Poisson ( 'Y_obs' , mu = mu , observed = peaks_y [ i ]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging . getLogger ( 'pymc3' ) logger . setLevel ( logging . ERROR ) # perform sampling traces = [] t0 = time () for i in range ( len ( peaks )): with gauss_models [ i ]: traces . append ( pm . sample ( Nsamples , tune = Ntune , discard_tuned_samples = True )) t1 = time () timepymc3 = t1 - t0 # time taken to sample all of the photopeaks print ( ' {} seconds ( {} seconds per peak) taken to run PyMC3 sampling.' . format ( timepymc3 , timepymc3 / 6 )) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling.","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data-alternate-method","text":"The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probability distribution in log-space, meaning the standard deviation has an equal probability of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using similar code as before, but with the new likelihood. gauss_model_alt = pm . Model () with gauss_model_alt : # set prior parameters # amplitude a_mu = a_guesses [ 1 ] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses [ 1 ] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses [ 1 ] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = - 1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm . Normal ( 'Amplitude' , mu = a_mu , sd = a_sig ) xc_model = pm . Normal ( 'Peak Energy' , mu = xc_mu , sd = xc_sig ) w_model = pm . Normal ( 'Standard Deviation' , mu = w_mu , sd = w_sig ) y0_model = pm . Normal ( 'Background Counts' , mu = y0_mu , sd = y0_sig ) # set uniform prior sigma_model = pm . Uniform ( 'Noise' , lower = sigma_min , upper = sigma_max ) # Expected value of outcome mu = gauss ( peaks_x [ 1 ], a_model , xc_model , w_model , y0_model ) # Normal likelihood of observations with noise Y_obs = pm . Normal ( 'Y_obs' , mu = mu , sd = 10 ** sigma_model , observed = peaks_y [ 1 ]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time () with gauss_model_alt : trace_alt = pm . sample ( Nsamples , tune = Ntune , discard_tuned_samples = True ) t1_alt = time () timepymc3_alt = t1_alt - t0_alt print ( ' {} seconds taken to run PyMC3 alternate sampling.' . format ( timepymc3_alt )) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np . vstack (( trace_alt [ 'Amplitude' ], trace_alt [ 'Peak Energy' ], trace_alt [ 'Standard Deviation' ], trace_alt [ 'Background Counts' ], trace_alt [ 'Noise' ])) . T # mean and standard deviation error of each parameter a_alt , a_err_alt = np . mean ( samples_alt [:, 0 ]), np . std ( samples_alt [:, 0 ]) xc_alt , xc_err_alt = np . mean ( samples_alt [:, 1 ]), np . std ( samples_alt [:, 1 ]) w_alt , w_err_alt = np . mean ( samples_alt [:, 2 ]), np . std ( samples_alt [:, 2 ]) y0_alt , y0_err_alt = np . mean ( samples_alt [:, 3 ]), np . std ( samples_alt [:, 3 ]) sigma_alt , sigma_err_alt = np . mean ( samples_alt [:, 4 ]), np . std ( samples_alt [:, 4 ]) a_w_alt_cov = np . cov ( samples_alt )[ 0 , 2 ] # print values print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a_alt , a_err_alt ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc_alt , xc_err_alt ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w_alt , w_err_alt ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0_alt , y0_err_alt ) + ' Noise = {} \\u00B1 {} counts' . format ( sigma_alt , sigma_err_alt )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts","title":"Sampling the data - Alternate method"},{"location":"PyMC3_GRS/PyMC3_GRS/#results","text":"Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [ np . vstack (( trace [ 'Amplitude' ], trace [ 'Peak Energy' ], trace [ 'Standard Deviation' ], trace [ 'Background Counts' ])) . T for trace in traces ] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range ( len ( peaks ))] for ind in range ( len ( peaks )): resdict [ ind ][ 'a_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 0 ]) resdict [ ind ][ 'a_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 0 ]) resdict [ ind ][ 'xc_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 1 ]) resdict [ ind ][ 'xc_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 1 ]) resdict [ ind ][ 'w_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 2 ]) resdict [ ind ][ 'w_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 2 ]) resdict [ ind ][ 'y0_mu' ] = np . mean ( all_pymc3_samples [ ind ][:, 3 ]) resdict [ ind ][ 'y0_sig' ] = np . std ( all_pymc3_samples [ ind ][:, 3 ]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts ( samples , labels , ** kwargs ): \"\"\" Function to plot posteriors using corner.py and scipy's Gaussian KDE function. \"\"\" fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) plt . subplots_adjust ( wspace = 0.2 , hspace = 0.2 ) # plot KDE smoothed version of distributions for axidx , samps in zip ([ 0 , 5 , 10 , 15 ], samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 100 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = \"firebrick\" ) # create corner plot for peak with noisiest data labels = [ r 'Amplitude' , r 'Peak Energy' , r 'Standard Deviation' , r 'Background Counts' ] corner_plot_samples = all_pymc3_samples [ 1 ] plotposts ( corner_plot_samples , labels ) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a , a_err = resdict [ 1 ][ 'a_mu' ], resdict [ 1 ][ 'a_sig' ] xc , xc_err = resdict [ 1 ][ 'xc_mu' ], resdict [ 1 ][ 'xc_sig' ] w , w_err = resdict [ 1 ][ 'w_mu' ], resdict [ 1 ][ 'w_sig' ] y0 , y0_err = resdict [ 1 ][ 'y0_mu' ], resdict [ 1 ][ 'y0_sig' ] print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a , a_err ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc , xc_err ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w , w_err ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0 , y0_err )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior","text":"Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig , axs = plt . subplots ( 2 , 3 , figsize = ( 12 , 7 )) for i in range ( 2 ): for j in range ( 3 ): ind = 3 * i + j a = resdict [ ind ][ 'a_mu' ] xc = resdict [ ind ][ 'xc_mu' ] w = resdict [ ind ][ 'w_mu' ] y0 = resdict [ ind ][ 'y0_mu' ] x = peaks_x [ ind ] y = peaks_y [ ind ] # plot original data axs [ i , j ] . plot ( x , y , 'b.' , alpha = 1 , label = ( 'Original Data' if all ( num == 0 for num in [ i , j ]) else '' )) # plot fitted curve over the data xsmooth = np . linspace ( x [ 0 ], x [ - 1 ], len ( x ) * 100 ) axs [ i , j ] . plot ( xsmooth , gauss ( xsmooth , a , xc , w , y0 ), 'k:' , alpha = 1 , label = ( 'Fitted Model' if all ( num == 0 for num in [ i , j ]) else '' )) if i == 1 : axs [ i , j ] . set_xlabel ( 'Energy / keV' ) if j == 0 : axs [ i , j ] . set_ylabel ( 'Counts' ) leg = fig . legend ( loc = 'lower right' , numpoints = 1 ) for lh in leg . legendHandles : lh . set_alpha ( 1 ) fig . suptitle ( 'Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling' ) plt . show () Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3 , xc_samps_pymc3 , w_samps_pymc3 , y0_samps_pymc3 = ([] for i in range ( 4 )) for ind in range ( len ( peaks )): a_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 0 ], size = n_fits )) xc_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 1 ], size = n_fits )) w_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 2 ], size = n_fits )) y0_samps_pymc3 . append ( np . random . choice ( all_pymc3_samples [ ind ][:, 3 ], size = n_fits )) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x [ ind ] xsmooth = np . linspace ( x [ 0 ], x [ - 1 ], len ( x ) * 100 ) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) for i in range ( n_fits ): ax1 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ ind ][ i ], xc_samps_pymc3 [ ind ][ i ], w_samps_pymc3 [ ind ][ i ], y0_samps_pymc3 [ ind ][ i ]), 'b-' , alpha = 0.01 , linewidth = 2 ) ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ ind ][ i ], xc_samps_pymc3 [ ind ][ i ], w_samps_pymc3 [ ind ][ i ], y0_samps_pymc3 [ ind ][ i ]), 'b-' , alpha = 0.02 , linewidth = 2 ) ax1 . set_ylabel ( 'Counts' ) ax1 . set_xlabel ( 'Energy / keV' ) ax2 . set_xlim ( 159.4 , 161.8 ) ax2 . set_ylim ( 1800 , 2250 ) ax2 . set_xlabel ( 'Energy / keV' ) fig . suptitle ( 'Posterior predictive plot for a photopeak from a sample of Ba-133' ) plt . show ()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons","text":"Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak. When finding the area error, we must also consider the covariance between the amplitude and standard deviation, which I show below: # parameter means and standard deviations of peak a_pymc3 , aerr_pymc3 = resdict [ 1 ][ 'a_mu' ], resdict [ 1 ][ 'a_sig' ] xc_pymc3 , xcerr_pymc3 = resdict [ 1 ][ 'xc_mu' ], resdict [ 1 ][ 'xc_sig' ] w_pymc3 , werr_pymc3 = resdict [ 1 ][ 'w_mu' ], resdict [ 1 ][ 'w_sig' ] y0_pymc3 , y0err_pymc3 = resdict [ 1 ][ 'y0_mu' ], resdict [ 1 ][ 'y0_sig' ] a_w_cov = np . cov ( all_pymc3_samples [ 1 ])[ 0 , 2 ] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate . quad ( lambda t : gauss ( t , a_pymc3 , xc_pymc3 , w_pymc3 , y0_pymc3 ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err = np . sqrt ( 2 * np . pi * (( w_pymc3 * aerr_pymc3 ) ** 2 + ( a_pymc3 * werr_pymc3 ) ** 2 ) + 2 * a * w * a_w_cov ) / c_0 percent_err = 100 * peak_integral_err / peak_integral print ( 'Total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral , peak_integral_err ) + 'Percentage error = {} %' . format ( percent_err )) Total counts = 22933.86204613347 \u00b1 195.58699451020811 counts Percentage error = 0.8500378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate . quad ( lambda t : gauss ( t , a_alt , xc_alt , w_alt , y0_alt ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err_alt = np . sqrt ( 2 * np . pi * (( w_alt * a_err_alt ) ** 2 + ( a_alt * w_err_alt ) ** 2 ) + 2 * a_alt * w_alt * a_w_alt_cov ) / c_0 percent_err_alt = 100 * peak_integral_err_alt / peak_integral_alt print ( 'Alternate total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral_alt , peak_integral_err_alt ) + 'Alternate percentage error = {} %' . format ( percent_err_alt )) Alternate total counts = 22943.751210742565 \u00b1 193.76313608385547 counts Alternate percentage error = 0.8447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-dynesty","text":"Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they produce simillar results. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here.","title":"Modelling with dynesty"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data_1","text":"Using dynesty is slightly more complicated than PyMC3. Most nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform ( theta ): # unpack the transformed parameters a_t , xc_t , w_t , y0_t = theta # define our prior guesses for each parameter a_mu , a_sig = a_guesses [ 1 ], 50. xc_mu , xc_sig = xc_guesses [ 1 ], 1. w_mu , w_sig = 0.7 , 0.3 y0_mu , y0_sig = y0_guesses [ 1 ], 30. # convert back to a = a_mu + a_sig * ndtri ( a_t ) xc = xc_mu + xc_sig * ndtri ( xc_t ) w = w_mu + w_sig * ndtri ( w_t ) y0 = y0_mu + y0_sig * ndtri ( y0_t ) return a , xc , w , y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike ( theta ): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like , xc_like , w_like , y0_like = theta # expected value lmbda = np . array ( gauss ( peaks_x [ 1 ], a_like , xc_like , w_like , y0_like )) n = len ( peaks_y [ 1 ]) a = np . sum ( gammaln ( np . array ( peaks_y [ 1 ]) + 1 )) b = np . sum ( np . array ( peaks_y [ 1 ]) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points , sampling algorithm, sampling method, and a stopping criterion . Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler ( loglike , priortransform , nparam , bound = 'multi' , sample = 'unif' , nlive = 1000 ) t0_dynesty = time () sampler . run_nested ( dlogz = stop , print_progress = False ) t1_dynesty = time () print ( ' {} seconds taken to run dynesty sampling' . format ( t1_dynesty - t0_dynesty )) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler . results print ( results . summary ()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#results_1","text":"To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np . exp ( results [ 'logwt' ] - results [ 'logz' ][ - 1 ]) samples = results . samples dynesty_samples = resample_equal ( samples , weights ) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a , xc , w , y0 = [ dynesty_samples [:, i ] for i in range ( 4 )] a_dynesty , aerr_dynesty = np . mean ( a ), np . std ( a ) xc_dynesty , xcerr_dynesty = np . mean ( xc ), np . std ( xc ) w_dynesty , werr_dynesty = np . mean ( w ), np . std ( w ) y0_dynesty , y0err_dynesty = np . mean ( y0 ), np . std ( y0 ) a_w_dynesty_cov = np . cov ( dynesty_samples )[ 0 , 2 ] nfits = 300 a_samps_dynesty = np . random . normal ( a_dynesty , aerr_dynesty , nfits ) xc_samps_dynesty = np . random . normal ( xc_dynesty , xcerr_dynesty , nfits ) w_samps_dynesty = np . random . normal ( w_dynesty , werr_dynesty , nfits ) y0_samps_dynesty = np . random . normal ( y0_dynesty , y0err_dynesty , nfits ) print ( 'Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n ' . format ( a_dynesty , aerr_dynesty ) + ' Peak Energy = {} \\u00B1 {} keV \\n ' . format ( xc_dynesty , xcerr_dynesty ) + ' Standard Deviation = {} \\u00B1 {} keV \\n ' . format ( w_dynesty , werr_dynesty ) + ' Background Counts = {} \\u00B1 {} counts \\n ' . format ( y0_dynesty , y0err_dynesty )) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior_1","text":"So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x , y = peaks_x [ 1 ], peaks_y [ 1 ] fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) xsmooth = np . linspace ( min ( x ), max ( x ), 1000 ) # mean posterior plot ax1 . plot ( x , y , 'b.' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 'k:' ) ax1 . set_ylim ( 1200 ) # posterior predictive plot for i in range ( nfits ): ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_dynesty [ i ], xc_samps_dynesty [ i ], w_samps_dynesty [ i ], y0_samps_dynesty [ i ]), 'b-' , alpha = 0.01 ) ax2 . set_ylim ( 1200 ) plt . suptitle ( 'Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of \\n ' + ' Ba-133, sampled by dynesty' ) plt . show () Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x , y = peaks_x [ 1 ], peaks_y [ 1 ] fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) xsmooth = np . linspace ( min ( x ), max ( x ), 1000 ) # mean posterior plot ax1 . plot ( x , y , 'k.' , label = 'Data' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 'b:' , label = 'dynesty' ) ax1 . plot ( xsmooth , gauss ( xsmooth , a_pymc3 , xc_pymc3 , w_pymc3 , y0_pymc3 ), 'r:' , label = 'PyMC3' ) ax1 . set_ylim ( 1200 ) leg1 = ax1 . legend ( loc = 'upper right' ) for lh in leg1 . legendHandles : lh . set_alpha ( 1 ) # posterior predictive plot for i in range ( nfits ): ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_dynesty [ i ], xc_samps_dynesty [ i ], w_samps_dynesty [ i ], y0_samps_dynesty [ i ]), 'b-' , alpha = 0.01 , label = ( 'dynesty' if i == 0 else '' )) ax2 . plot ( xsmooth , gauss ( xsmooth , a_samps_pymc3 [ 1 ][ i ], xc_samps_pymc3 [ 1 ][ i ], w_samps_pymc3 [ 1 ][ i ], y0_samps_pymc3 [ 1 ][ i ]), 'r-' , alpha = 0.01 , label = ( 'PyMC3' if i == 0 else '' )) ax2 . set_ylim ( 1200 ) leg2 = ax2 . legend ( loc = 'upper right' ) for lh in leg2 . legendHandles : lh . set_alpha ( 1 ) plt . suptitle ( 'Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + ' \\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)' ) plt . show ()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons_1","text":"This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate . quad ( lambda t : gauss ( t , a_dynesty , xc_dynesty , w_dynesty , y0_dynesty ), 159.1 , 162.2 )[ 0 ] / c_0 peak_integral_err_dynesty = np . sqrt ( 2 * np . pi * (( w_dynesty * aerr_dynesty ) ** 2 + ( a_dynesty * werr_dynesty ) ** 2 ) + 2 * a_dynesty * w_dynesty + a_w_dynesty_cov ) / c_0 percent_err_dynesty = 100 * peak_integral_err_dynesty / peak_integral_dynesty print ( 'Total counts = {} \\u00B1 {} counts \\n ' . format ( peak_integral_dynesty , peak_integral_err_dynesty ) + 'Percentage error = {} %' . format ( percent_err_dynesty )) Total counts = 22968.854117562147 \u00b1 201.03466521968398 counts Percentage error = 0.8793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error under 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"Model comparisons"},{"location":"Sunspots2/Sunspots2/","text":"Using Zeus and Nestle to model the sunspot cycles A sunspot is an area on the surface of the Sun that appears a lot darker than its surroundings, caused by intense magnetic fields regulating a convection effect on the Sun's surface. Humanity has been tracking the average number of sunspots visible from Earth on the solar disk, and found that the sunspot number follows a cycle over roughly 11 years. In this example, I'll use the \"Zeus\" and \"Nestle\" samplers to fit a model describing the sunspot number to a single solar cycle. I'll then use this model to create a new model which predicts the properties of a solar cycle, given the properties of the previous cycle. Useful imports # numpy import numpy as np # pandas import pandas as pd # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri , gammaln # Plotting import corner import matplotlib.pyplot as plt % matplotlib inline # Samplers import zeus print ( 'Zeus version: {} ' . format ( zeus . __version__ )) import nestle print ( 'Nestle version: {} ' . format ( nestle . __version__ )) # misc import logging from time import time Zeus version: 1.0.7 Nestle version: 0.2.0 Viewing the data Using Pandas, we can load the .csv file containing the average sunspot number over 24 solar cycles, since the 1750s. First I will plot the entire dataset to decide which cycles are of interest, however before plotting I'll first take the square root of the sunspot number. This is just to decrease the variance in the peak height, which will make everything a little easier when it comes to making a model that can predict the height of the next peak. dataframe = pd . read_csv ( \"SN_m_tot_V2.0.csv\" , sep = \";\" , usecols = [ 2 , 3 ], header = None ) dates = list ( dataframe [ 2 ])[ 80 :] ssn = list ( dataframe [ 3 ])[ 80 :] sqrtssn = np . sqrt ( ssn ) plt . figure ( figsize = ( 16 , 3 )) plt . plot ( dates , sqrtssn ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Average sunspot number from 1750-2020\" ) plt . show () I chose to look at the 4 solar cycles starting around the year 1924, as there seems to be a consistent (predictable!) change from peak to peak. Next, we need to take a closer look at those 4 solar cycles so that we can create a model. start = 2020 # data point corresponding the the start of the 1924 solar cycle period = 123 # average width of a solar cycle fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 3 )) # plot region of interest ax1 . plot ( dates [ start : start + 4 * period ], sqrtssn [ start : start + 4 * period ]) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_ylabel ( \"Square Root of Sunspot Number\" ) ax1 . set_title ( \"Average sunspot number from 1924-1965\" ) # fragment the 4 solar cycles into 4 separate lists peaks = [ sqrtssn [ start + i * period : start + ( i + 1 ) * period ] for i in range ( 4 )] peaktimes = [ dates [ start + i * period : start + ( i + 1 ) * period ] for i in range ( 4 )] # plot a typical solar cycle ax2 . plot ( peaktimes [ 0 ], peaks [ 0 ]) ax2 . set_xlabel ( \"Date / years\" ) ax2 . set_title ( \"A typical solar cycle\" ) plt . show () The model We can create the model by splitting it into two parts: the ascending region, and the descending region. To define these regions, we need to know when the cycle begins, when the cycle peaks, and when the cycle ends. We'll call these \"t0\", \"tmax\", and \"t1\" respectively. Next, we need to know the amplitude of the peak, which we will call \"c\". Finally, we need 2 more parameters which will describe how much the ascending and descending regions curve, which we will call \"a1\", and \"a2\". Using all of these, we can define our model for a single solar cycle below: def cycle ( times , c , a1 , a2 , t0 , tmax , t1 ): # a1,a2 > 1 if a1 < 1 : a1 = 1 if a2 < 1 : a2 = 1 #t0 < tmax < t1 if t0 >= tmax : t0 = tmax - 1 if t1 <= tmax : t0 = tmax + 1 # sunspot number as a function of time ssn = [ c * ( 1 - (( tmax - t ) / ( tmax - t0 )) ** a1 ) if t < tmax else c * ( 1 - (( t - tmax ) / ( t1 - tmax )) ** a2 ) for t in times ] # ssn > 0 ssn_non_negative = [ i if i > 0 else 0.1 for i in ssn ] return ssn_non_negative I'll take the first solar cycle (starting 1924), and use some guesses for each parameter to overplot a model, just to give an idea of what the model will look like. # plot cycle data plt . plot ( peaktimes [ 0 ], peaks [ 0 ], label = \"Original Data\" ) # plot an example model, with guessed parameters x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) y = cycle ( x , 11 , 2.1 , 1.3 , 1923.5 , 1927.5 , 1935 ) plt . plot ( x , y , label = \"Example Model\" ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"A typical solar cycle, with an \\n example model overplotted\" ) plt . legend () plt . show () Modelling with Zeus Zeus has a very similar interface to the sampler \"emcee\", which uses an ensemble technique to obtain the prior distributions. The next step is defining the prior distributions for each parameter. I'll use a normal distribution for the peak amplitude \"c\", since it's pretty easy to eyeball. Aside from that, I'll be using uniform priors for every other parameter as they're a little trickier to guess. nens = 100 # number of ensemble points ndims = 6 # number of parameters # mean and standard deviation of normal parameter priors cmu , csig = 11 , 1 # lower and upper bounds of uniform parameter priors a1min , a1max = 1 , 3 a2min , a2max = 1 , 3 t0min , t0max = 1922 , 1925 tmaxmin , tmaxmax = 1926 , 1929 t1min , t1max = 1933 , 1936 param_priors = [] # normal prior on c param_priors . append ( np . random . normal ( cmu , csig , nens )) # uniform prior on a1 param_priors . append ( np . random . uniform ( a1min , a1max , nens )) # uniform prior on a2 param_priors . append ( np . random . uniform ( a2min , a2max , nens )) # uniform prior on t0 param_priors . append ( np . random . uniform ( t0min , t0max , nens )) # uniform prior on tmax param_priors . append ( np . random . uniform ( tmaxmin , tmaxmax , nens )) # uniform prior on t1 param_priors . append ( np . random . uniform ( t1min , t1max , nens )) param_samples = np . array ( param_priors ) . T Next, we need to define a log prior, log likelihood, and log posterior. The log prior can be defined as below: def logprior ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range ( len ( param_priors )): # sum log priors from each parameter if i == 0 : # normal priors lprior -= 0.5 * (( theta [ i ] - cmu ) / csig ) ** 2 else : # uniform priors # set bounds if i == 1 : low , up = a1min , a1max elif i == 2 : low , up = a2min , a2max elif i == 3 : low , up = t0min , t0max elif i == 4 : low , up = tmaxmin , tmaxmax else : low , up = t1min , t1max # parameter must be between bounds if low < theta [ i ] < up : pass else : lprior = - np . inf return lprior The log likelihood takes the form of a Poisson likelihood, since the sunspot counts are non-negative. using \"lmbda\" as the expected value of the cycle function, we can define the likelihood as below: def loglike ( theta , times , obs ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # unpack parameters c_like , a1_like , a2_like , t0_like , tmax_like , t1_like = theta # expected value lmbda = np . array ( cycle ( times , c_like , a1_like , a2_like , t0_like , tmax_like , t1_like )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Finally the log posterior is simply the sum of the log prior log likelihood: def logposterior ( theta , times , obs ): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike ( theta , times , obs ) Sampling the data Now that we've defined everything we need, we can easily run the sampling process with Zeus. # create sampler using the first peak sampler = zeus . sampler ( nens , ndims , logposterior , args = [ peaktimes [ 0 ], peaks [ 0 ]]) nburn = 500 # burn-in points nsamples = 500 # points after burn-in time0 = time () sampler . run_mcmc ( param_samples , nburn + nsamples ) time1 = time () print ( \"Time taken to sample first peak with Zeus: {} seconds\" . format ( time1 - time0 )) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:16<00:00, 13.13it/s] Time taken to sample first peak with Zeus: 76.26689600944519 seconds Results Collecting the samples after the sampling is equally simple, remembering to discard the burn-in samples at the start of the chain. We can create a corner plot, which shows the posteriors of each parameter along with contour plots describing how one parameter varies with any other: samples_zeus = sampler . get_chain ( flat = True , discard = nburn ) def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'c' , 'a1' , 'a2' , 't0' , 'tmax' , 't1' ] plotposts ( samples_zeus , labels ) Next, I'll find the means and standard deviation error on each parameter. I'll also choose random samples from the chains of each parameter, which I'll use to visualise the posteriors. param_mu = [ np . mean ( samples_zeus [:, i ]) for i in range ( 6 )] param_sig = [ np . std ( samples_zeus [:, i ]) for i in range ( 6 )] nfits = 300 param_samples = [ np . random . choice ( samples_zeus [:, i ], nfits ) for i in range ( 6 )] post_samples = np . array ( param_samples ) . T print ( \"Parameters describing the square root SSN cycle starting 1923: \\n \\n \" + \" c = {} \\u00B1 {} square root counts \\n \" . format ( param_mu [ 0 ], param_sig [ 0 ]) + \" a1 = {} \\u00B1 {} \\n \" . format ( param_mu [ 1 ], param_sig [ 1 ]) + \" a2 = {} \\u00B1 {} \\n \" . format ( param_mu [ 2 ], param_sig [ 2 ]) + \" t0 = {} \\u00B1 {} years \\n \" . format ( param_mu [ 3 ], param_sig [ 3 ]) + \"tmax = {} \\u00B1 {} years \\n \" . format ( param_mu [ 4 ], param_sig [ 4 ]) + \" t1 = {} \\u00B1 {} years \\n \" . format ( param_mu [ 5 ], param_sig [ 5 ])) Parameters describing the square root SSN cycle starting 1923: c = 11.23419227189279 \u00b1 0.5155512374811139 square root counts a1 = 2.046091218734536 \u00b1 0.5511917043187438 a2 = 1.3175512464689283 \u00b1 0.2551941095539496 t0 = 1923.4364606185572 \u00b1 0.3683613113062203 years tmax = 1927.5745437571138 \u00b1 0.6532168763478214 years t1 = 1935.0535923941627 \u00b1 0.30717761056370796 years Plotting the posterior Below I'll show two plots. The left plot will show the model produced using the mean parameters for the first peak. The right plot will show a posterior predictive plot, where the darker the colour of the plot, the higher the probabilty of the model passing through that region. fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 3 )) # mean posterior plot ax1 . plot ( peaktimes [ 0 ], peaks [ 0 ]) x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) y = cycle ( x , * param_mu ) ax1 . plot ( x , y ) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_ylabel ( \"square root counts\" ) ax1 . set_title ( \"Mean posterior plot for solar cycle starting 1924\" ) # posterior predictive plot x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) ax2 . plot ( peaktimes [ 0 ], peaks [ 0 ], label = \"Origina Data\" ) for i in range ( len ( post_samples )): params = post_samples [ i ] y = cycle ( x , * params ) ax2 . plot ( x , y , 'orange' , alpha = 0.02 , linewidth = 3 , label = \"Fitted Model\" if i == 0 else \"\" ) ax2 . set_xlabel ( \"Date / years\" ) ax2 . set_title ( \"Posterior predictive plot for SSN cycle starting 1924\" ) leg = ax2 . legend () for lh in leg . legendHandles : lh . set_alpha ( 1 ) plt . show () Predicting the next solar cycles We now know the properties of the solar cycle between 1924-1934. Next, we want to know the properties of the next three cycles in our region of interest. We could just redefine our prior distributions, and run the sampling again for each peak. Alternatively, we could create a new model which takes the current solar cycle parameters, and uses them to predict the next solar cycle. This model will only, realisticly, be able to make long range predictions if the solar cycles evolve consistently, as mentiioned above. This way this model is put together is quite complicated. More details can be found here , but the brief is that we need 3 scaling parameters (y1,y2,y3), and 4 translation parameters (d0,d1,d2,d3). This model can be implemented as follows: def predict_cycle ( theta_prev , d0 , d1 , d2 , d3 , y1 , y2 , y3 ): c_prev , a1 , a2 , t0_prev , tmax_prev , t1_prev = theta_prev # start of cycle found using slight deviation d0 t0 = t1_prev + d0 # current c found using current t0 and previous c,tmax if t0 <= tmax_prev : t0 = tmax_prev + 5 c = y1 * c_prev / ( t0 - tmax_prev ) + d1 # current tmax found using current t0,c tmax = t0 + y2 * c + d2 * ( t0 - param_mu [ 4 ]) ** 0.9 # current t1 found using current tmax,c t1 = tmax + y3 * c + d3 * ( t0 - param_mu [ 4 ]) ** 0.3 #a1,a2 unchanged # return new set of parameters theta_new = ( c , a1 , a2 , t0 , tmax , t1 ) return theta_new We can now try to guess the scaling and translational parameters, and using the set of parameters defining the first solar cycle, we can try to predict the second solar cycle between 1934-1944. I'll show this by plotting the data for the second solar cycle, and overplotting the predicted model. After some trial and error, I found the following prediction: # predict the next peak parameters, using current peak parameters params = predict_cycle ( param_mu , - 1 , 0 , - 0.4 , 0.2 , 7.5 , 0.5 , 0.5 ) # plot second solar cycle plt . plot ( peaktimes [ 1 ], peaks [ 1 ], label = \"Original Data\" ) # overplot predicted model with new parameters x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) plt . plot ( x , cycle ( x , * params ), label = \"Predicted model\" ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Average square root sunspot number between 1934-1944 \\n with overplotted predicted model\" ) plt . legend () plt . show () Using the above guesses, we can define some prior distributions. This is done in exactly the same way as above, however due to some hard limits on the parameters (such as a1,a2 being strictly greater than 1), I'll use uniform priors for all parameters. nens = 100 ndims = 7 d0min , d0max = - 2. , 0 d1min , d1max = - 1. , 1. d2min , d2max = - 1. , 0. d3min , d3max = 0. , 0.4 y1min , y1max = 7. , 8. y2min , y2max = 0.2 , 0.8 y3min , y3max = 0.25 , 0.75 predict_priors = [] # uniform prior on d0 predict_priors . append ( np . random . uniform ( d0min , d0max , nens )) # uniform prior on d1 predict_priors . append ( np . random . uniform ( d1min , d1max , nens )) # uniform prior on d2 predict_priors . append ( np . random . uniform ( d2min , d2max , nens )) # uniform prior on d3 predict_priors . append ( np . random . uniform ( d3min , d3max , nens )) # uniform prior on y1 predict_priors . append ( np . random . uniform ( y1min , y1max , nens )) # uniform prior on y2 predict_priors . append ( np . random . uniform ( y2min , y2max , nens )) # uniform prior on y3 predict_priors . append ( np . random . uniform ( y3min , y3max , nens )) priors = [( d0min , d0max ),( d1min , d1max ),( d2min , d2max ),( d3min , d3max ), ( y1min , y1max ),( y2min , y2max ),( y3min , y3max )] predict_samples = np . array ( predict_priors ) . T We need to create new log prior, log likelihood, and log posterior functions. This is done almost exactly as above, but with very slight variations, so I'll won't go into detail about the machinery in these functions. def loglike_predict ( theta , times , obs ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle ( param_mu , * theta ) lmbda = np . array ( cycle ( times , * params_new )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def logprior_predict ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range ( len ( predict_priors )): # sum log priors from each parameter low , up = priors [ i ][ 0 ], priors [ i ][ 1 ] # uniform prior for time parameters if low < theta [ i ] < up : pass else : lprior = - np . inf return lprior def logposterior_predict ( theta , times , obs ): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior_predict ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike_predict ( theta , times , obs ) Running the sampler, using the same burn-in and chain lengths as above, we can tune the parameters so they predict the parameters of the second solar cycle well. We'll then assume that these parameters are the same for the third and fourth peaks, and see how good the quality of the fit is with increasing forecast time. sampler = zeus . sampler ( nens , ndims , logposterior_predict , args = [ peaktimes [ 1 ], peaks [ 1 ]]) nburn = 500 nsamples = 500 time0 = time () sampler . run_mcmc ( predict_samples , nburn + nsamples ) time1 = time () print ( \"Time taken to sample predict function with Zeus: {} seconds\" . format ( time1 - time0 )) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:19<00:00, 12.56it/s] Time taken to sample predict function with Zeus: 79.66238069534302 seconds Predictions Out of curiosity, we can check the corner plot for the prediction model. Since the prediction model has every parameter interacting with eachother, we can expect a load of covariance between parameters. samples_zeus2 = sampler . get_chain ( flat = True , discard = nburn ) predict_param_mu = [ np . mean ( samples_zeus2 [:, i ]) for i in range ( 7 )] predict_param_sig = [ np . std ( samples_zeus2 [:, i ]) for i in range ( 7 )] nfits = 300 predict_param_samples = [ np . random . choice ( samples_zeus2 [:, i ], nfits ) for i in range ( 7 )] predict_post_samples = np . array ( predict_param_samples ) . T labels = [ 'd0' , 'd1' , 'd2' , 'd3' , 'y1' , 'y2' , 'y3' ] plotposts ( samples_zeus2 , labels ) Lets check what predictions we can now make. The below code takes the set of parameters describing the first solar cycle, and uses it to predict the second solar cycle. The prediction will be plotted over the data, and then the program will move on to the next peak and repeat the process. Since we're making predictions on using predictions, over and over, we can expect the fit to get worse as we look deeper into the future. I'll also use the samples from the prediction model posteriors, and create a posterior predictive plot for each cycle. This should show the quality of the prediction, and how it evolves with increasing forecast time. fig , axs = plt . subplots ( 3 , 2 , figsize = ( 10 , 10 )) # predict second peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( param_mu , * predict_param_mu ) axs [ 0 , 0 ] . plot ( peaktimes [ 1 ], peaks [ 1 ]) x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) axs [ 0 , 0 ] . plot ( x , cycle ( x , * new_param_mu )) axs [ 0 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 0 , 0 ] . set_title ( \"Average sunspot number for the solar cycles \\n between 1933-1965\" + \" with overfitted \\n mean predicted models\" ) # posterior predictive plot axs [ 0 , 1 ] . plot ( peaktimes [ 1 ], peaks [ 1 ]) new_pred = [] for i in range ( len ( predict_post_samples )): pred = predict_cycle ( param_mu , * predict_post_samples [ i ]) new_pred . append ( pred ) y = cycle ( x , * pred ) axs [ 0 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) axs [ 0 , 1 ] . set_title ( \"Average sunspot number for the solar cycles \\n between 1933-1965\" + \" with overfitted \\n posterior predictive plots\" ) # predict third peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( new_param_mu , * predict_param_mu ) axs [ 1 , 0 ] . plot ( peaktimes [ 2 ], peaks [ 2 ]) x = np . linspace ( min ( peaktimes [ 2 ]), max ( peaktimes [ 2 ]), 300 ) axs [ 1 , 0 ] . plot ( x , cycle ( x , * new_param_mu )) axs [ 1 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 1 , 0 ] . set_xlabel ( \"Date / years\" ) # posterior predictive plot axs [ 1 , 1 ] . plot ( peaktimes [ 2 ], peaks [ 2 ]) new_pred2 = [] for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred [ i ], * predict_param_mu ) new_pred2 . append ( pred ) y = cycle ( x , * pred ) axs [ 1 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) # predict fourth peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( new_param_mu , * predict_param_mu ) axs [ 2 , 0 ] . plot ( peaktimes [ 3 ], peaks [ 3 ], label = \"Original Data\" ) x = np . linspace ( min ( peaktimes [ 3 ]), max ( peaktimes [ 3 ]), 300 ) axs [ 2 , 0 ] . plot ( x , cycle ( x , * new_param_mu ), label = \"Predicted Model\" ) axs [ 2 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 2 , 0 ] . set_xlabel ( \"Date / years\" ) axs [ 2 , 0 ] . legend () # posterior predictive plot axs [ 2 , 1 ] . plot ( peaktimes [ 3 ], peaks [ 3 ]) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred2 [ i ], * predict_param_mu ) y = cycle ( x , * pred ) axs [ 2 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) axs [ 2 , 1 ] . set_xlabel ( \"Date / years\" ) plt . show () Saying that our prediction model was fairly simplified, it actually does a pretty good job at predicting the solar cycle shape. The posterior plots show the predictions getting worse over time as expected, but even so the accuracy on the mean plots are within 20% for the majority of the cycles. Modelling with Nestle The predictions from this model only work if we know in advance that the next solar cycle will carry on the trend from the previous solar cycle. Since in reality it is very hard to say how the Sun will behave in advance, this model won't be all that useful. However, it did save us from having to individually sampling 3 solar cycles. To decide whether or not the time save is worth the loss in accuracy, I'll use \"Nestle\" to sample both the second solar cycle (between 1934-1945), and the prediction model for that same solar cycle. Nestle uses nested sampling, and so it produces a value of the marginalised evidence. Comparing these values will allow us to see how much accuracy we're sacrificing by predicting the solar cycle, instead of just sampling the next peak. Sampling the data To start, we'll assume that Zeus did a good enough job at sampling the first solar cycle, and jump straight to the second cycle (1934-1945). Lets define some new guesses for each parameter (c,a1,a2,t0,tmax,t1): # mean and standard deviation of normal parameter priors cmu , csig = 13 , 1 # lower and upper bounds of uniform parameter priors a1min , a1max = 1 , 3 a2min , a2max = 1 , 3 t0min , t0max = 1931 , 1935 tmaxmin , tmaxmax = 1936 , 1940 t1min , t1max = 1943 , 1947 param_priors = [( cmu , csig ),( a1min , a1max ),( a2min , a2max ), ( t0min , t0max ),( tmaxmin , tmaxmax ),( t1min , t1max )] We can reuse most of the stuff we had from before, except for the log prior functions. Nestle samples from a unit hypercube parameter space, so we need a function that transforms the priors back to their original space. This only requires a slight modification to our previous \"logprior\" function, making use of scipy's ndtri function. I'll also make a very minor change to the log likelihood function. Instead of taking the dates and sunspot counts as arguments, it will use the data for the second solar cycle by default. def prior_transform ( theta ): \"\"\" Function to transform the parameters from unit hypercube to their true form \"\"\" trans_params = [] # transform normal prior trans_params . append ( param_priors [ 0 ][ 0 ] + param_priors [ 0 ][ 1 ] * ndtri ( theta [ 0 ])) # transform uniform prior for i in range ( 1 , 6 ): mini , maxi = param_priors [ i ][ 0 ], param_priors [ i ][ 1 ] trans_params . append ( theta [ i ] * ( maxi - mini ) + mini ) return trans_params def loglike_nestle ( theta ): \"\"\" Function to return the log likelihood, with data fixed for solar cycle between 1934-1945 \"\"\" obs , times = peaks [ 1 ], peaktimes [ 1 ] # unpack parameters c_like , a1_like , a2_like , t0_like , tmax_like , t1_like = theta # expected value lmbda = np . array ( cycle ( times , c_like , a1_like , a2_like , t0_like , tmax_like , t1_like )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b We're now ready to run the sampling using Nestle: # set number of dimensions, live points, sampling method, and stopping criterion ndims = 6 nlive = 1024 method = 'multi' stop = 0.1 time0 = time () results_sample = nestle . sample ( loglike_nestle , prior_transform , ndims , method = method , npoints = nlive , dlogz = stop ) time1 = time () print ( \"Time taken to sample second solar cycle with Nestle: {} seconds\" . format ( time1 - time0 )) Time taken to sample second solar cycle with Nestle: 18.891968965530396 seconds We'll now try the sampling again, but this time we'll use the parameters from the first solar cycle (found using Zeus), and sample the parameters of the prediction model. Start by guessing at the parameters for the prediction model: # reuse our guesses from Zeus d0min , d0max = - 2. , 0 d1min , d1max = - 1. , 1. d2min , d2max = - 1. , 0. d3min , d3max = 0. , 0.4 y1min , y1max = 7. , 8. y2min , y2max = 0.2 , 0.8 y3min , y3max = 0.25 , 0.75 predict_priors = [( d0min , d0max ),( d1min , d1max ),( d2min , d2max ),( d3min , d3max ), ( y1min , y1max ),( y2min , y2max ),( y3min , y3max )] We have to tinker with out prediction model's likelihood and prior functions. This is done exactly as it was previously. def loglike_nestle_predict ( theta ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" obs , times = peaks [ 1 ], peaktimes [ 1 ] # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle ( param_mu , * theta ) lmbda = np . array ( cycle ( times , * params_new )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def prior_transform_predict ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" trans_priors = [] for i in range ( len ( predict_priors )): # sum log priors from each parameter mini , maxi = predict_priors [ i ][ 0 ], predict_priors [ i ][ 1 ] # uniform prior for time parameters trans_priors . append ( theta [ i ] * ( maxi - mini ) + mini ) return trans_priors Now that everything is set up, I'll run through the same sampling process, using the same hyper-parameters for fairness. # set number of dimensions, live points, sampling method, and stopping criterion ndims = 7 nlive = 1024 method = 'multi' stop = 0.1 time0 = time () results_predict = nestle . sample ( loglike_nestle_predict , prior_transform_predict , ndims , method = method , npoints = nlive , dlogz = stop ) time1 = time () print ( \"Time taken to sample prediction model with Nestle: {} seconds\" . format ( time1 - time0 )) Time taken to sample prediction model with Nestle: 13.779999017715454 seconds Results Next, we find the log of the marginalised evidence provided by Nestle. This can be done as follows, using the information gain to estimate the error: logZ_sample = results_sample . logz logZerr_sample = np . sqrt ( results_sample . h / nlive ) logZ_predict = results_predict . logz logZerr_predict = np . sqrt ( results_predict . h / nlive ) print ( \"log(Z) from sampling the solar cycle = {} \u00b1 {} \" . format ( logZ_sample , logZerr_sample )) print ( \"log(Z) from predicting the solar cycle = {} \u00b1 {} \" . format ( logZ_predict , logZerr_predict )) log(Z) from sampling the solar cycle = -266.75746256813557 \u00b1 0.07494693736170717 log(Z) from predicting the solar cycle = -266.81473144676227 \u00b1 0.07314472974917986 The Bayes factor is a metric that describes how much more likely a model is to produce an observed data set. It's defined as the ratios between the marginalised evidences of the two models: K = np . exp ( logZ_predict - logZ_sample ) print ( \"Bayes factor: {} \" . format ( K )) Bayes factor: 0.9443401223523545 This tells us that sampling via the prediction method rather than just sampling the peak doesn't come at a cost to the quality of the model. Since the methods produce similar results, lets collect the samples from the prediction method to use for visualising our results. Since Nestle uses nested sampling, we have to resample with weights to obtain the posteriors. weights = results_predict . weights / np . max ( results_predict . weights ) mask = np . where ( np . random . rand ( len ( weights )) < weights )[ 0 ] # collect posterior samples samples_nestle = results_predict . samples [ mask ,:] # collect posterior means predict_param_mu_nestle = [ np . mean ( samples_nestle [:, i ]) for i in range ( 7 )] # collect samples for posterior predictive plot nfits = 300 predict_param_samples_nestle = [ np . random . choice ( samples_nestle [:, i ], nfits ) for i in range ( 7 )] predict_post_samples_nestle = np . array ( predict_param_samples ) . T Plotting the posterior Let's start by making a comparison between the posteriors from Zeus and Nestle. Below I'll create two plots. The first will show the mean posterior predictions of the second solar cycle, from both Zeus and Nestle. The second plot will show the posterior predictive plots, for both samplers also. fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 4 )) # plot mean posterior plot new_param_mu_zeus = predict_cycle ( param_mu , * predict_param_mu ) new_param_mu_nestle = predict_cycle ( param_mu , * predict_param_mu_nestle ) ax1 . plot ( peaktimes [ 1 ], peaks [ 1 ], label = \"Original Data\" ) x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) ax1 . plot ( x , cycle ( x , * new_param_mu_zeus ), \"orange\" , label = \"Zeus Prediction\" ) ax1 . plot ( x , cycle ( x , * new_param_mu_nestle ), \"purple\" , label = \"Nestle Prediction\" ) ax1 . set_ylabel ( \"Square Root of Sunspot Number\" ) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_title ( \"Average sunspot number for the solar cycles \\n between 1934-1945\" + \" with overfitted \\n mean predicted models\" ) ax1 . legend () # posterior predictive plot ax2 . plot ( peaktimes [ 1 ], peaks [ 1 ]) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( param_mu , * predict_post_samples [ i ]) y = cycle ( x , * pred ) ax2 . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) pred = predict_cycle ( param_mu , * ( np . array ( predict_post_samples_nestle [ i ]) + 0.01 )) y = cycle ( x , * pred ) ax2 . plot ( x , y , \"purple\" , alpha = 0.04 , linewidth = 3 ) ax1 . set_ylabel ( \"Date / years\" ) ax2 . set_title ( \"Average sunspot number for the solar cycles \\n between 1934-1945\" + \" with overfitted \\n posterior predictive plots\" ) plt . show () Whilst the posterior predictive plot looks a little messy, it tells us that the posterior predictives from Zeus and Nestle are difficult to tell apart, since they overlap so much. The mean posterior plot supports this, showing that the two samplers have no deviation over the entire cycle. Finally, lets plot a posterior predictive plot that shows the evolution of the prediction accuracy over time. Below is a plot that shows predictions for the entire range of 4 solar cycles between 1923 and 1965. plt . figure ( figsize = ( 14 , 4 )) # plot data between 1923 and 1965 plt . plot ( dates [ start : start + 4 * period ], sqrtssn [ start : start + 4 * period ], label = \"Original Data\" ) # plot first cycle posterior predictive, using Zeus samples x = np . linspace ( min ( peaktimes [ 0 ]) - 20 , max ( peaktimes [ 0 ]), 300 ) for i in range ( len ( post_samples )): params = post_samples [ i ] y = cycle ( x , * params ) plt . plot ( x , y , \"purple\" , alpha = 0.02 , linewidth = 3 , label = \"Fitted Model\" if i == 0 else \"\" ) # use first cycle posterior predictive to find second cycle posterior predictive new_pred = [] x = np . linspace ( min ( peaktimes [ 1 ]) - 20 , max ( peaktimes [ 1 ]) + 20 , 300 ) for i in range ( len ( predict_post_samples_nestle )): pred = predict_cycle ( param_mu , * predict_post_samples_nestle [ i ]) new_pred . append ( pred ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 , label = \"Predicted Model\" if i == 0 else \"\" ) # use second cycle posterior predictive to find third cycle posterior predictive x = np . linspace ( min ( peaktimes [ 2 ]) - 20 , max ( peaktimes [ 2 ]) + 20 , 300 ) new_pred2 = [] for i in range ( len ( predict_post_samples_nestle )): pred = predict_cycle ( new_pred [ i ], * predict_param_mu_nestle ) new_pred2 . append ( pred ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) # use third cycle posterior predictive to find fourth cycle posterior predictive x = np . linspace ( min ( peaktimes [ 3 ]) - 20 , max ( peaktimes [ 3 ]) + 20 , 300 ) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred2 [ i ], * predict_param_mu_nestle ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Predictions of average sunspot number between 1934-1965, \\n \" + \" using the cycle between 1923-1934 to make predictions\" ) plt . xlim ( 1923 , 1967 ) plt . ylim ( 1 ) leg = plt . legend ( loc = \"upper left\" ) for lh in leg . legendHandles : lh . set_alpha ( 1 ) plt . show ()","title":"Solar Cycle Analysis with Zeus and Nestle"},{"location":"Sunspots2/Sunspots2/#using-zeus-and-nestle-to-model-the-sunspot-cycles","text":"A sunspot is an area on the surface of the Sun that appears a lot darker than its surroundings, caused by intense magnetic fields regulating a convection effect on the Sun's surface. Humanity has been tracking the average number of sunspots visible from Earth on the solar disk, and found that the sunspot number follows a cycle over roughly 11 years. In this example, I'll use the \"Zeus\" and \"Nestle\" samplers to fit a model describing the sunspot number to a single solar cycle. I'll then use this model to create a new model which predicts the properties of a solar cycle, given the properties of the previous cycle.","title":"Using Zeus and Nestle to model the sunspot cycles"},{"location":"Sunspots2/Sunspots2/#useful-imports","text":"# numpy import numpy as np # pandas import pandas as pd # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri , gammaln # Plotting import corner import matplotlib.pyplot as plt % matplotlib inline # Samplers import zeus print ( 'Zeus version: {} ' . format ( zeus . __version__ )) import nestle print ( 'Nestle version: {} ' . format ( nestle . __version__ )) # misc import logging from time import time Zeus version: 1.0.7 Nestle version: 0.2.0","title":"Useful imports"},{"location":"Sunspots2/Sunspots2/#viewing-the-data","text":"Using Pandas, we can load the .csv file containing the average sunspot number over 24 solar cycles, since the 1750s. First I will plot the entire dataset to decide which cycles are of interest, however before plotting I'll first take the square root of the sunspot number. This is just to decrease the variance in the peak height, which will make everything a little easier when it comes to making a model that can predict the height of the next peak. dataframe = pd . read_csv ( \"SN_m_tot_V2.0.csv\" , sep = \";\" , usecols = [ 2 , 3 ], header = None ) dates = list ( dataframe [ 2 ])[ 80 :] ssn = list ( dataframe [ 3 ])[ 80 :] sqrtssn = np . sqrt ( ssn ) plt . figure ( figsize = ( 16 , 3 )) plt . plot ( dates , sqrtssn ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Average sunspot number from 1750-2020\" ) plt . show () I chose to look at the 4 solar cycles starting around the year 1924, as there seems to be a consistent (predictable!) change from peak to peak. Next, we need to take a closer look at those 4 solar cycles so that we can create a model. start = 2020 # data point corresponding the the start of the 1924 solar cycle period = 123 # average width of a solar cycle fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 3 )) # plot region of interest ax1 . plot ( dates [ start : start + 4 * period ], sqrtssn [ start : start + 4 * period ]) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_ylabel ( \"Square Root of Sunspot Number\" ) ax1 . set_title ( \"Average sunspot number from 1924-1965\" ) # fragment the 4 solar cycles into 4 separate lists peaks = [ sqrtssn [ start + i * period : start + ( i + 1 ) * period ] for i in range ( 4 )] peaktimes = [ dates [ start + i * period : start + ( i + 1 ) * period ] for i in range ( 4 )] # plot a typical solar cycle ax2 . plot ( peaktimes [ 0 ], peaks [ 0 ]) ax2 . set_xlabel ( \"Date / years\" ) ax2 . set_title ( \"A typical solar cycle\" ) plt . show ()","title":"Viewing the data"},{"location":"Sunspots2/Sunspots2/#the-model","text":"We can create the model by splitting it into two parts: the ascending region, and the descending region. To define these regions, we need to know when the cycle begins, when the cycle peaks, and when the cycle ends. We'll call these \"t0\", \"tmax\", and \"t1\" respectively. Next, we need to know the amplitude of the peak, which we will call \"c\". Finally, we need 2 more parameters which will describe how much the ascending and descending regions curve, which we will call \"a1\", and \"a2\". Using all of these, we can define our model for a single solar cycle below: def cycle ( times , c , a1 , a2 , t0 , tmax , t1 ): # a1,a2 > 1 if a1 < 1 : a1 = 1 if a2 < 1 : a2 = 1 #t0 < tmax < t1 if t0 >= tmax : t0 = tmax - 1 if t1 <= tmax : t0 = tmax + 1 # sunspot number as a function of time ssn = [ c * ( 1 - (( tmax - t ) / ( tmax - t0 )) ** a1 ) if t < tmax else c * ( 1 - (( t - tmax ) / ( t1 - tmax )) ** a2 ) for t in times ] # ssn > 0 ssn_non_negative = [ i if i > 0 else 0.1 for i in ssn ] return ssn_non_negative I'll take the first solar cycle (starting 1924), and use some guesses for each parameter to overplot a model, just to give an idea of what the model will look like. # plot cycle data plt . plot ( peaktimes [ 0 ], peaks [ 0 ], label = \"Original Data\" ) # plot an example model, with guessed parameters x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) y = cycle ( x , 11 , 2.1 , 1.3 , 1923.5 , 1927.5 , 1935 ) plt . plot ( x , y , label = \"Example Model\" ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"A typical solar cycle, with an \\n example model overplotted\" ) plt . legend () plt . show ()","title":"The model"},{"location":"Sunspots2/Sunspots2/#modelling-with-zeus","text":"Zeus has a very similar interface to the sampler \"emcee\", which uses an ensemble technique to obtain the prior distributions. The next step is defining the prior distributions for each parameter. I'll use a normal distribution for the peak amplitude \"c\", since it's pretty easy to eyeball. Aside from that, I'll be using uniform priors for every other parameter as they're a little trickier to guess. nens = 100 # number of ensemble points ndims = 6 # number of parameters # mean and standard deviation of normal parameter priors cmu , csig = 11 , 1 # lower and upper bounds of uniform parameter priors a1min , a1max = 1 , 3 a2min , a2max = 1 , 3 t0min , t0max = 1922 , 1925 tmaxmin , tmaxmax = 1926 , 1929 t1min , t1max = 1933 , 1936 param_priors = [] # normal prior on c param_priors . append ( np . random . normal ( cmu , csig , nens )) # uniform prior on a1 param_priors . append ( np . random . uniform ( a1min , a1max , nens )) # uniform prior on a2 param_priors . append ( np . random . uniform ( a2min , a2max , nens )) # uniform prior on t0 param_priors . append ( np . random . uniform ( t0min , t0max , nens )) # uniform prior on tmax param_priors . append ( np . random . uniform ( tmaxmin , tmaxmax , nens )) # uniform prior on t1 param_priors . append ( np . random . uniform ( t1min , t1max , nens )) param_samples = np . array ( param_priors ) . T Next, we need to define a log prior, log likelihood, and log posterior. The log prior can be defined as below: def logprior ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range ( len ( param_priors )): # sum log priors from each parameter if i == 0 : # normal priors lprior -= 0.5 * (( theta [ i ] - cmu ) / csig ) ** 2 else : # uniform priors # set bounds if i == 1 : low , up = a1min , a1max elif i == 2 : low , up = a2min , a2max elif i == 3 : low , up = t0min , t0max elif i == 4 : low , up = tmaxmin , tmaxmax else : low , up = t1min , t1max # parameter must be between bounds if low < theta [ i ] < up : pass else : lprior = - np . inf return lprior The log likelihood takes the form of a Poisson likelihood, since the sunspot counts are non-negative. using \"lmbda\" as the expected value of the cycle function, we can define the likelihood as below: def loglike ( theta , times , obs ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # unpack parameters c_like , a1_like , a2_like , t0_like , tmax_like , t1_like = theta # expected value lmbda = np . array ( cycle ( times , c_like , a1_like , a2_like , t0_like , tmax_like , t1_like )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Finally the log posterior is simply the sum of the log prior log likelihood: def logposterior ( theta , times , obs ): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike ( theta , times , obs )","title":"Modelling with Zeus"},{"location":"Sunspots2/Sunspots2/#sampling-the-data","text":"Now that we've defined everything we need, we can easily run the sampling process with Zeus. # create sampler using the first peak sampler = zeus . sampler ( nens , ndims , logposterior , args = [ peaktimes [ 0 ], peaks [ 0 ]]) nburn = 500 # burn-in points nsamples = 500 # points after burn-in time0 = time () sampler . run_mcmc ( param_samples , nburn + nsamples ) time1 = time () print ( \"Time taken to sample first peak with Zeus: {} seconds\" . format ( time1 - time0 )) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:16<00:00, 13.13it/s] Time taken to sample first peak with Zeus: 76.26689600944519 seconds","title":"Sampling the data"},{"location":"Sunspots2/Sunspots2/#results","text":"Collecting the samples after the sampling is equally simple, remembering to discard the burn-in samples at the start of the chain. We can create a corner plot, which shows the posteriors of each parameter along with contour plots describing how one parameter varies with any other: samples_zeus = sampler . get_chain ( flat = True , discard = nburn ) def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'c' , 'a1' , 'a2' , 't0' , 'tmax' , 't1' ] plotposts ( samples_zeus , labels ) Next, I'll find the means and standard deviation error on each parameter. I'll also choose random samples from the chains of each parameter, which I'll use to visualise the posteriors. param_mu = [ np . mean ( samples_zeus [:, i ]) for i in range ( 6 )] param_sig = [ np . std ( samples_zeus [:, i ]) for i in range ( 6 )] nfits = 300 param_samples = [ np . random . choice ( samples_zeus [:, i ], nfits ) for i in range ( 6 )] post_samples = np . array ( param_samples ) . T print ( \"Parameters describing the square root SSN cycle starting 1923: \\n \\n \" + \" c = {} \\u00B1 {} square root counts \\n \" . format ( param_mu [ 0 ], param_sig [ 0 ]) + \" a1 = {} \\u00B1 {} \\n \" . format ( param_mu [ 1 ], param_sig [ 1 ]) + \" a2 = {} \\u00B1 {} \\n \" . format ( param_mu [ 2 ], param_sig [ 2 ]) + \" t0 = {} \\u00B1 {} years \\n \" . format ( param_mu [ 3 ], param_sig [ 3 ]) + \"tmax = {} \\u00B1 {} years \\n \" . format ( param_mu [ 4 ], param_sig [ 4 ]) + \" t1 = {} \\u00B1 {} years \\n \" . format ( param_mu [ 5 ], param_sig [ 5 ])) Parameters describing the square root SSN cycle starting 1923: c = 11.23419227189279 \u00b1 0.5155512374811139 square root counts a1 = 2.046091218734536 \u00b1 0.5511917043187438 a2 = 1.3175512464689283 \u00b1 0.2551941095539496 t0 = 1923.4364606185572 \u00b1 0.3683613113062203 years tmax = 1927.5745437571138 \u00b1 0.6532168763478214 years t1 = 1935.0535923941627 \u00b1 0.30717761056370796 years","title":"Results"},{"location":"Sunspots2/Sunspots2/#plotting-the-posterior","text":"Below I'll show two plots. The left plot will show the model produced using the mean parameters for the first peak. The right plot will show a posterior predictive plot, where the darker the colour of the plot, the higher the probabilty of the model passing through that region. fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 3 )) # mean posterior plot ax1 . plot ( peaktimes [ 0 ], peaks [ 0 ]) x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) y = cycle ( x , * param_mu ) ax1 . plot ( x , y ) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_ylabel ( \"square root counts\" ) ax1 . set_title ( \"Mean posterior plot for solar cycle starting 1924\" ) # posterior predictive plot x = np . linspace ( min ( peaktimes [ 0 ]), max ( peaktimes [ 0 ]), 300 ) ax2 . plot ( peaktimes [ 0 ], peaks [ 0 ], label = \"Origina Data\" ) for i in range ( len ( post_samples )): params = post_samples [ i ] y = cycle ( x , * params ) ax2 . plot ( x , y , 'orange' , alpha = 0.02 , linewidth = 3 , label = \"Fitted Model\" if i == 0 else \"\" ) ax2 . set_xlabel ( \"Date / years\" ) ax2 . set_title ( \"Posterior predictive plot for SSN cycle starting 1924\" ) leg = ax2 . legend () for lh in leg . legendHandles : lh . set_alpha ( 1 ) plt . show ()","title":"Plotting the posterior"},{"location":"Sunspots2/Sunspots2/#predicting-the-next-solar-cycles","text":"We now know the properties of the solar cycle between 1924-1934. Next, we want to know the properties of the next three cycles in our region of interest. We could just redefine our prior distributions, and run the sampling again for each peak. Alternatively, we could create a new model which takes the current solar cycle parameters, and uses them to predict the next solar cycle. This model will only, realisticly, be able to make long range predictions if the solar cycles evolve consistently, as mentiioned above. This way this model is put together is quite complicated. More details can be found here , but the brief is that we need 3 scaling parameters (y1,y2,y3), and 4 translation parameters (d0,d1,d2,d3). This model can be implemented as follows: def predict_cycle ( theta_prev , d0 , d1 , d2 , d3 , y1 , y2 , y3 ): c_prev , a1 , a2 , t0_prev , tmax_prev , t1_prev = theta_prev # start of cycle found using slight deviation d0 t0 = t1_prev + d0 # current c found using current t0 and previous c,tmax if t0 <= tmax_prev : t0 = tmax_prev + 5 c = y1 * c_prev / ( t0 - tmax_prev ) + d1 # current tmax found using current t0,c tmax = t0 + y2 * c + d2 * ( t0 - param_mu [ 4 ]) ** 0.9 # current t1 found using current tmax,c t1 = tmax + y3 * c + d3 * ( t0 - param_mu [ 4 ]) ** 0.3 #a1,a2 unchanged # return new set of parameters theta_new = ( c , a1 , a2 , t0 , tmax , t1 ) return theta_new We can now try to guess the scaling and translational parameters, and using the set of parameters defining the first solar cycle, we can try to predict the second solar cycle between 1934-1944. I'll show this by plotting the data for the second solar cycle, and overplotting the predicted model. After some trial and error, I found the following prediction: # predict the next peak parameters, using current peak parameters params = predict_cycle ( param_mu , - 1 , 0 , - 0.4 , 0.2 , 7.5 , 0.5 , 0.5 ) # plot second solar cycle plt . plot ( peaktimes [ 1 ], peaks [ 1 ], label = \"Original Data\" ) # overplot predicted model with new parameters x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) plt . plot ( x , cycle ( x , * params ), label = \"Predicted model\" ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Average square root sunspot number between 1934-1944 \\n with overplotted predicted model\" ) plt . legend () plt . show () Using the above guesses, we can define some prior distributions. This is done in exactly the same way as above, however due to some hard limits on the parameters (such as a1,a2 being strictly greater than 1), I'll use uniform priors for all parameters. nens = 100 ndims = 7 d0min , d0max = - 2. , 0 d1min , d1max = - 1. , 1. d2min , d2max = - 1. , 0. d3min , d3max = 0. , 0.4 y1min , y1max = 7. , 8. y2min , y2max = 0.2 , 0.8 y3min , y3max = 0.25 , 0.75 predict_priors = [] # uniform prior on d0 predict_priors . append ( np . random . uniform ( d0min , d0max , nens )) # uniform prior on d1 predict_priors . append ( np . random . uniform ( d1min , d1max , nens )) # uniform prior on d2 predict_priors . append ( np . random . uniform ( d2min , d2max , nens )) # uniform prior on d3 predict_priors . append ( np . random . uniform ( d3min , d3max , nens )) # uniform prior on y1 predict_priors . append ( np . random . uniform ( y1min , y1max , nens )) # uniform prior on y2 predict_priors . append ( np . random . uniform ( y2min , y2max , nens )) # uniform prior on y3 predict_priors . append ( np . random . uniform ( y3min , y3max , nens )) priors = [( d0min , d0max ),( d1min , d1max ),( d2min , d2max ),( d3min , d3max ), ( y1min , y1max ),( y2min , y2max ),( y3min , y3max )] predict_samples = np . array ( predict_priors ) . T We need to create new log prior, log likelihood, and log posterior functions. This is done almost exactly as above, but with very slight variations, so I'll won't go into detail about the machinery in these functions. def loglike_predict ( theta , times , obs ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle ( param_mu , * theta ) lmbda = np . array ( cycle ( times , * params_new )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def logprior_predict ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" lprior = 0 for i in range ( len ( predict_priors )): # sum log priors from each parameter low , up = priors [ i ][ 0 ], priors [ i ][ 1 ] # uniform prior for time parameters if low < theta [ i ] < up : pass else : lprior = - np . inf return lprior def logposterior_predict ( theta , times , obs ): \"\"\" Function to return the log posterior, given the log prior and log likelihood \"\"\" lprior = logprior_predict ( theta ) # check log prior is finite if not np . isfinite ( lprior ): return - np . inf return lprior + loglike_predict ( theta , times , obs ) Running the sampler, using the same burn-in and chain lengths as above, we can tune the parameters so they predict the parameters of the second solar cycle well. We'll then assume that these parameters are the same for the third and fourth peaks, and see how good the quality of the fit is with increasing forecast time. sampler = zeus . sampler ( nens , ndims , logposterior_predict , args = [ peaktimes [ 1 ], peaks [ 1 ]]) nburn = 500 nsamples = 500 time0 = time () sampler . run_mcmc ( predict_samples , nburn + nsamples ) time1 = time () print ( \"Time taken to sample predict function with Zeus: {} seconds\" . format ( time1 - time0 )) Initialising ensemble of 100 walkers... Sampling progress : 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:19<00:00, 12.56it/s] Time taken to sample predict function with Zeus: 79.66238069534302 seconds","title":"Predicting the next solar cycles"},{"location":"Sunspots2/Sunspots2/#predictions","text":"Out of curiosity, we can check the corner plot for the prediction model. Since the prediction model has every parameter interacting with eachother, we can expect a load of covariance between parameters. samples_zeus2 = sampler . get_chain ( flat = True , discard = nburn ) predict_param_mu = [ np . mean ( samples_zeus2 [:, i ]) for i in range ( 7 )] predict_param_sig = [ np . std ( samples_zeus2 [:, i ]) for i in range ( 7 )] nfits = 300 predict_param_samples = [ np . random . choice ( samples_zeus2 [:, i ], nfits ) for i in range ( 7 )] predict_post_samples = np . array ( predict_param_samples ) . T labels = [ 'd0' , 'd1' , 'd2' , 'd3' , 'y1' , 'y2' , 'y3' ] plotposts ( samples_zeus2 , labels ) Lets check what predictions we can now make. The below code takes the set of parameters describing the first solar cycle, and uses it to predict the second solar cycle. The prediction will be plotted over the data, and then the program will move on to the next peak and repeat the process. Since we're making predictions on using predictions, over and over, we can expect the fit to get worse as we look deeper into the future. I'll also use the samples from the prediction model posteriors, and create a posterior predictive plot for each cycle. This should show the quality of the prediction, and how it evolves with increasing forecast time. fig , axs = plt . subplots ( 3 , 2 , figsize = ( 10 , 10 )) # predict second peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( param_mu , * predict_param_mu ) axs [ 0 , 0 ] . plot ( peaktimes [ 1 ], peaks [ 1 ]) x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) axs [ 0 , 0 ] . plot ( x , cycle ( x , * new_param_mu )) axs [ 0 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 0 , 0 ] . set_title ( \"Average sunspot number for the solar cycles \\n between 1933-1965\" + \" with overfitted \\n mean predicted models\" ) # posterior predictive plot axs [ 0 , 1 ] . plot ( peaktimes [ 1 ], peaks [ 1 ]) new_pred = [] for i in range ( len ( predict_post_samples )): pred = predict_cycle ( param_mu , * predict_post_samples [ i ]) new_pred . append ( pred ) y = cycle ( x , * pred ) axs [ 0 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) axs [ 0 , 1 ] . set_title ( \"Average sunspot number for the solar cycles \\n between 1933-1965\" + \" with overfitted \\n posterior predictive plots\" ) # predict third peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( new_param_mu , * predict_param_mu ) axs [ 1 , 0 ] . plot ( peaktimes [ 2 ], peaks [ 2 ]) x = np . linspace ( min ( peaktimes [ 2 ]), max ( peaktimes [ 2 ]), 300 ) axs [ 1 , 0 ] . plot ( x , cycle ( x , * new_param_mu )) axs [ 1 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 1 , 0 ] . set_xlabel ( \"Date / years\" ) # posterior predictive plot axs [ 1 , 1 ] . plot ( peaktimes [ 2 ], peaks [ 2 ]) new_pred2 = [] for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred [ i ], * predict_param_mu ) new_pred2 . append ( pred ) y = cycle ( x , * pred ) axs [ 1 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) # predict fourth peak parameters # plot mean posterior plot new_param_mu = predict_cycle ( new_param_mu , * predict_param_mu ) axs [ 2 , 0 ] . plot ( peaktimes [ 3 ], peaks [ 3 ], label = \"Original Data\" ) x = np . linspace ( min ( peaktimes [ 3 ]), max ( peaktimes [ 3 ]), 300 ) axs [ 2 , 0 ] . plot ( x , cycle ( x , * new_param_mu ), label = \"Predicted Model\" ) axs [ 2 , 0 ] . set_ylabel ( \"Square Root of Sunspot Number\" ) axs [ 2 , 0 ] . set_xlabel ( \"Date / years\" ) axs [ 2 , 0 ] . legend () # posterior predictive plot axs [ 2 , 1 ] . plot ( peaktimes [ 3 ], peaks [ 3 ]) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred2 [ i ], * predict_param_mu ) y = cycle ( x , * pred ) axs [ 2 , 1 ] . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) axs [ 2 , 1 ] . set_xlabel ( \"Date / years\" ) plt . show () Saying that our prediction model was fairly simplified, it actually does a pretty good job at predicting the solar cycle shape. The posterior plots show the predictions getting worse over time as expected, but even so the accuracy on the mean plots are within 20% for the majority of the cycles.","title":"Predictions"},{"location":"Sunspots2/Sunspots2/#modelling-with-nestle","text":"The predictions from this model only work if we know in advance that the next solar cycle will carry on the trend from the previous solar cycle. Since in reality it is very hard to say how the Sun will behave in advance, this model won't be all that useful. However, it did save us from having to individually sampling 3 solar cycles. To decide whether or not the time save is worth the loss in accuracy, I'll use \"Nestle\" to sample both the second solar cycle (between 1934-1945), and the prediction model for that same solar cycle. Nestle uses nested sampling, and so it produces a value of the marginalised evidence. Comparing these values will allow us to see how much accuracy we're sacrificing by predicting the solar cycle, instead of just sampling the next peak.","title":"Modelling with Nestle"},{"location":"Sunspots2/Sunspots2/#sampling-the-data_1","text":"To start, we'll assume that Zeus did a good enough job at sampling the first solar cycle, and jump straight to the second cycle (1934-1945). Lets define some new guesses for each parameter (c,a1,a2,t0,tmax,t1): # mean and standard deviation of normal parameter priors cmu , csig = 13 , 1 # lower and upper bounds of uniform parameter priors a1min , a1max = 1 , 3 a2min , a2max = 1 , 3 t0min , t0max = 1931 , 1935 tmaxmin , tmaxmax = 1936 , 1940 t1min , t1max = 1943 , 1947 param_priors = [( cmu , csig ),( a1min , a1max ),( a2min , a2max ), ( t0min , t0max ),( tmaxmin , tmaxmax ),( t1min , t1max )] We can reuse most of the stuff we had from before, except for the log prior functions. Nestle samples from a unit hypercube parameter space, so we need a function that transforms the priors back to their original space. This only requires a slight modification to our previous \"logprior\" function, making use of scipy's ndtri function. I'll also make a very minor change to the log likelihood function. Instead of taking the dates and sunspot counts as arguments, it will use the data for the second solar cycle by default. def prior_transform ( theta ): \"\"\" Function to transform the parameters from unit hypercube to their true form \"\"\" trans_params = [] # transform normal prior trans_params . append ( param_priors [ 0 ][ 0 ] + param_priors [ 0 ][ 1 ] * ndtri ( theta [ 0 ])) # transform uniform prior for i in range ( 1 , 6 ): mini , maxi = param_priors [ i ][ 0 ], param_priors [ i ][ 1 ] trans_params . append ( theta [ i ] * ( maxi - mini ) + mini ) return trans_params def loglike_nestle ( theta ): \"\"\" Function to return the log likelihood, with data fixed for solar cycle between 1934-1945 \"\"\" obs , times = peaks [ 1 ], peaktimes [ 1 ] # unpack parameters c_like , a1_like , a2_like , t0_like , tmax_like , t1_like = theta # expected value lmbda = np . array ( cycle ( times , c_like , a1_like , a2_like , t0_like , tmax_like , t1_like )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b We're now ready to run the sampling using Nestle: # set number of dimensions, live points, sampling method, and stopping criterion ndims = 6 nlive = 1024 method = 'multi' stop = 0.1 time0 = time () results_sample = nestle . sample ( loglike_nestle , prior_transform , ndims , method = method , npoints = nlive , dlogz = stop ) time1 = time () print ( \"Time taken to sample second solar cycle with Nestle: {} seconds\" . format ( time1 - time0 )) Time taken to sample second solar cycle with Nestle: 18.891968965530396 seconds We'll now try the sampling again, but this time we'll use the parameters from the first solar cycle (found using Zeus), and sample the parameters of the prediction model. Start by guessing at the parameters for the prediction model: # reuse our guesses from Zeus d0min , d0max = - 2. , 0 d1min , d1max = - 1. , 1. d2min , d2max = - 1. , 0. d3min , d3max = 0. , 0.4 y1min , y1max = 7. , 8. y2min , y2max = 0.2 , 0.8 y3min , y3max = 0.25 , 0.75 predict_priors = [( d0min , d0max ),( d1min , d1max ),( d2min , d2max ),( d3min , d3max ), ( y1min , y1max ),( y2min , y2max ),( y3min , y3max )] We have to tinker with out prediction model's likelihood and prior functions. This is done exactly as it was previously. def loglike_nestle_predict ( theta ): \"\"\" Function to return the log likelihood, given the current parameters, dates, and sunspot counts \"\"\" obs , times = peaks [ 1 ], peaktimes [ 1 ] # expected value found by predicting new parameters, and then mimicking old likelihood function params_new = predict_cycle ( param_mu , * theta ) lmbda = np . array ( cycle ( times , * params_new )) n = len ( obs ) a = np . sum ( gammaln ( np . array ( obs ) + 1 )) b = np . sum ( np . array ( obs ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b def prior_transform_predict ( theta ): \"\"\" Function to return the log of the prior, given set of current parameters \"\"\" trans_priors = [] for i in range ( len ( predict_priors )): # sum log priors from each parameter mini , maxi = predict_priors [ i ][ 0 ], predict_priors [ i ][ 1 ] # uniform prior for time parameters trans_priors . append ( theta [ i ] * ( maxi - mini ) + mini ) return trans_priors Now that everything is set up, I'll run through the same sampling process, using the same hyper-parameters for fairness. # set number of dimensions, live points, sampling method, and stopping criterion ndims = 7 nlive = 1024 method = 'multi' stop = 0.1 time0 = time () results_predict = nestle . sample ( loglike_nestle_predict , prior_transform_predict , ndims , method = method , npoints = nlive , dlogz = stop ) time1 = time () print ( \"Time taken to sample prediction model with Nestle: {} seconds\" . format ( time1 - time0 )) Time taken to sample prediction model with Nestle: 13.779999017715454 seconds","title":"Sampling the data"},{"location":"Sunspots2/Sunspots2/#results_1","text":"Next, we find the log of the marginalised evidence provided by Nestle. This can be done as follows, using the information gain to estimate the error: logZ_sample = results_sample . logz logZerr_sample = np . sqrt ( results_sample . h / nlive ) logZ_predict = results_predict . logz logZerr_predict = np . sqrt ( results_predict . h / nlive ) print ( \"log(Z) from sampling the solar cycle = {} \u00b1 {} \" . format ( logZ_sample , logZerr_sample )) print ( \"log(Z) from predicting the solar cycle = {} \u00b1 {} \" . format ( logZ_predict , logZerr_predict )) log(Z) from sampling the solar cycle = -266.75746256813557 \u00b1 0.07494693736170717 log(Z) from predicting the solar cycle = -266.81473144676227 \u00b1 0.07314472974917986 The Bayes factor is a metric that describes how much more likely a model is to produce an observed data set. It's defined as the ratios between the marginalised evidences of the two models: K = np . exp ( logZ_predict - logZ_sample ) print ( \"Bayes factor: {} \" . format ( K )) Bayes factor: 0.9443401223523545 This tells us that sampling via the prediction method rather than just sampling the peak doesn't come at a cost to the quality of the model. Since the methods produce similar results, lets collect the samples from the prediction method to use for visualising our results. Since Nestle uses nested sampling, we have to resample with weights to obtain the posteriors. weights = results_predict . weights / np . max ( results_predict . weights ) mask = np . where ( np . random . rand ( len ( weights )) < weights )[ 0 ] # collect posterior samples samples_nestle = results_predict . samples [ mask ,:] # collect posterior means predict_param_mu_nestle = [ np . mean ( samples_nestle [:, i ]) for i in range ( 7 )] # collect samples for posterior predictive plot nfits = 300 predict_param_samples_nestle = [ np . random . choice ( samples_nestle [:, i ], nfits ) for i in range ( 7 )] predict_post_samples_nestle = np . array ( predict_param_samples ) . T","title":"Results"},{"location":"Sunspots2/Sunspots2/#plotting-the-posterior_1","text":"Let's start by making a comparison between the posteriors from Zeus and Nestle. Below I'll create two plots. The first will show the mean posterior predictions of the second solar cycle, from both Zeus and Nestle. The second plot will show the posterior predictive plots, for both samplers also. fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 4 )) # plot mean posterior plot new_param_mu_zeus = predict_cycle ( param_mu , * predict_param_mu ) new_param_mu_nestle = predict_cycle ( param_mu , * predict_param_mu_nestle ) ax1 . plot ( peaktimes [ 1 ], peaks [ 1 ], label = \"Original Data\" ) x = np . linspace ( min ( peaktimes [ 1 ]), max ( peaktimes [ 1 ]), 300 ) ax1 . plot ( x , cycle ( x , * new_param_mu_zeus ), \"orange\" , label = \"Zeus Prediction\" ) ax1 . plot ( x , cycle ( x , * new_param_mu_nestle ), \"purple\" , label = \"Nestle Prediction\" ) ax1 . set_ylabel ( \"Square Root of Sunspot Number\" ) ax1 . set_xlabel ( \"Date / years\" ) ax1 . set_title ( \"Average sunspot number for the solar cycles \\n between 1934-1945\" + \" with overfitted \\n mean predicted models\" ) ax1 . legend () # posterior predictive plot ax2 . plot ( peaktimes [ 1 ], peaks [ 1 ]) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( param_mu , * predict_post_samples [ i ]) y = cycle ( x , * pred ) ax2 . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) pred = predict_cycle ( param_mu , * ( np . array ( predict_post_samples_nestle [ i ]) + 0.01 )) y = cycle ( x , * pred ) ax2 . plot ( x , y , \"purple\" , alpha = 0.04 , linewidth = 3 ) ax1 . set_ylabel ( \"Date / years\" ) ax2 . set_title ( \"Average sunspot number for the solar cycles \\n between 1934-1945\" + \" with overfitted \\n posterior predictive plots\" ) plt . show () Whilst the posterior predictive plot looks a little messy, it tells us that the posterior predictives from Zeus and Nestle are difficult to tell apart, since they overlap so much. The mean posterior plot supports this, showing that the two samplers have no deviation over the entire cycle. Finally, lets plot a posterior predictive plot that shows the evolution of the prediction accuracy over time. Below is a plot that shows predictions for the entire range of 4 solar cycles between 1923 and 1965. plt . figure ( figsize = ( 14 , 4 )) # plot data between 1923 and 1965 plt . plot ( dates [ start : start + 4 * period ], sqrtssn [ start : start + 4 * period ], label = \"Original Data\" ) # plot first cycle posterior predictive, using Zeus samples x = np . linspace ( min ( peaktimes [ 0 ]) - 20 , max ( peaktimes [ 0 ]), 300 ) for i in range ( len ( post_samples )): params = post_samples [ i ] y = cycle ( x , * params ) plt . plot ( x , y , \"purple\" , alpha = 0.02 , linewidth = 3 , label = \"Fitted Model\" if i == 0 else \"\" ) # use first cycle posterior predictive to find second cycle posterior predictive new_pred = [] x = np . linspace ( min ( peaktimes [ 1 ]) - 20 , max ( peaktimes [ 1 ]) + 20 , 300 ) for i in range ( len ( predict_post_samples_nestle )): pred = predict_cycle ( param_mu , * predict_post_samples_nestle [ i ]) new_pred . append ( pred ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 , label = \"Predicted Model\" if i == 0 else \"\" ) # use second cycle posterior predictive to find third cycle posterior predictive x = np . linspace ( min ( peaktimes [ 2 ]) - 20 , max ( peaktimes [ 2 ]) + 20 , 300 ) new_pred2 = [] for i in range ( len ( predict_post_samples_nestle )): pred = predict_cycle ( new_pred [ i ], * predict_param_mu_nestle ) new_pred2 . append ( pred ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) # use third cycle posterior predictive to find fourth cycle posterior predictive x = np . linspace ( min ( peaktimes [ 3 ]) - 20 , max ( peaktimes [ 3 ]) + 20 , 300 ) for i in range ( len ( predict_post_samples )): pred = predict_cycle ( new_pred2 [ i ], * predict_param_mu_nestle ) y = cycle ( x , * pred ) plt . plot ( x , y , \"orange\" , alpha = 0.04 , linewidth = 3 ) plt . xlabel ( \"Date / years\" ) plt . ylabel ( \"Square Root of Sunspot Number\" ) plt . title ( \"Predictions of average sunspot number between 1934-1965, \\n \" + \" using the cycle between 1923-1934 to make predictions\" ) plt . xlim ( 1923 , 1967 ) plt . ylim ( 1 ) leg = plt . legend ( loc = \"upper left\" ) for lh in leg . legendHandles : lh . set_alpha ( 1 ) plt . show ()","title":"Plotting the posterior"},{"location":"TheOnlineMCMC/TheOnlineMCMC/","text":"Using The Online MCMC to fit a model without any installations Sometimes it's not easy to install packages in Python without knowing what you're doing, and even then you might be limited by your operating system or hard-drive space. Instead of installing Python, or any of the samplers described on this website, you can use The Online MCMC . This is a free-to-use website which allows you to simply enter a model, define the parameter priors, input the data, and the website does the rest. In this example, I'll use a photopeak from my gamma-ray spectroscopy example to demonstrate how to use The Online MCMC to fit a model to some data. Useful imports # no imports necessary for using The Online MCMC # packages only used for plotting/dataframes etc. # numpy import numpy as np # pandas import pandas as pd # plotting import matplotlib.pyplot as plt Viewing the data Lets start by taking a look at the photopeak in question. I separated the photopeak from the rest of the gamma-ray spectrum, and uploaded it here if you wish to download it. # load data using pandas data = pd . read_csv ( 'onlineMCMC_data.csv' ) # the format of the data is shown using .head() print ( data . head ()) Energy Counts 0 156.2074 1359 1 156.4335 1391 2 156.6596 1373 3 156.8857 1372 4 157.1118 1309 We can briefly plot the peak below, just to visualise the data: counts = data [ 'Counts' ] . values energy = data [ 'Energy' ] . values plt . plot ( energy , counts , 'b' ) plt . fill ( energy , counts , 'b' , alpha = 0.4 ) plt . xlabel ( 'Energy / MCA Channels' ) plt . ylabel ( 'Counts' ) plt . title ( 'Photopeak found in a gamma-ray spectrum of Ba-133' ) plt . show () The model For more details on what a gamma-ray spectrum and a photopeak are, check out this section of my gamma-ray spectroscopy example. For now, I'll only state that we'll be using a Gaussian function to model the data seen above. The function takes the following form: def gauss ( x , a , xc , w , y0 ): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a * np . exp ( - ( x - xc ) ** 2 / ( 2 * w ** 2 )) + y0 # make a plot just to demonstrate shape of model # array of points for x axis test_x = np . linspace ( 0 , 1 , 100 ) # easy-to-visualise parameters a = 1 xc = 0.5 w = 0.1 y0 = 0.5 # pass to Gaussian functio test_y = gauss ( test_x , a , xc , w , y0 ) #plot plt . plot ( test_x , test_y ) plt . title ( 'Simple Gaussian Function Example' ) plt . ylim ( 0 , 1.75 ) plt . show () Sampling the data Now that we know what our data looks like, and what our model looks like, we can pass everything to The Online MCMC website. Below is a screenshot of what all my inputs looked like on The Online MCMC. I'll talk through everything I did afterwards: First, I inputted the model. The Online MCMC allows for several basic functions to be used, such as trigonometric functions and exponential functions, so there is no need to approximate a model. The Online MCMC also allows for piecewise functions, which take different forms depending on the values of the x-axis. Next, the website automatically produces a list of variables in your model equation. I set x as an independant variable, and the other variables as parameters. For the independant variable x, I inputted the list of x-values (in this example the list contained in the \"energy\" variabe). For the parameters, the priors have to be defined. I did this simply by guessing at the values of the parameters using the photopeak plot above. I used a uniform prior for the background counts and standard deviation, and a normal prior for the other parameters. I next inputted the data (the list contained in the \"counts\" variable), and chose a likelihood. The Online MCMC gives a choice of Normal, Poisson, and Student's t likelihoods, of which I chose a Poisson distribution. Again, for more details on why I chose a Poisson likelihood, check out this section . Finally, I chose a sampler. I chose to use Nestle , although some other samplers I could have chosen are emcee , dynesty , and PyMC3 . I set up the number of live points, and chose a sampling method 'multi'. From there, all you have to do is enter an email address so the website can send the results once the sampling is complete, and click 'submit'. Results After a short while, The Online MCMC will email you with all the results from the sampling process. The first thing available to us is a corner plot produced by \"corner.py\" : This plot shows us the posteriors for each parameter, along with contour plots showing how a parameter may vary with any other. The Online MCMC also provides a link to download the posterior samples, so you can go ahead with further analysis if needed. Comparing to this section , we can see that the mean values from both The Online MCMC and PyMC3 are very much in agreement, and the standard deviation errors are also simillar. Since this is only an example to show you can use The Online MCMC to sample data, I won't go into all the further analysis on the photopeaks (which can be found here ).","title":"Zero Installation Sampling with The Online MCMC"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#using-the-online-mcmc-to-fit-a-model-without-any-installations","text":"Sometimes it's not easy to install packages in Python without knowing what you're doing, and even then you might be limited by your operating system or hard-drive space. Instead of installing Python, or any of the samplers described on this website, you can use The Online MCMC . This is a free-to-use website which allows you to simply enter a model, define the parameter priors, input the data, and the website does the rest. In this example, I'll use a photopeak from my gamma-ray spectroscopy example to demonstrate how to use The Online MCMC to fit a model to some data.","title":"Using The Online MCMC to fit a model without any installations"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#useful-imports","text":"# no imports necessary for using The Online MCMC # packages only used for plotting/dataframes etc. # numpy import numpy as np # pandas import pandas as pd # plotting import matplotlib.pyplot as plt","title":"Useful imports"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#viewing-the-data","text":"Lets start by taking a look at the photopeak in question. I separated the photopeak from the rest of the gamma-ray spectrum, and uploaded it here if you wish to download it. # load data using pandas data = pd . read_csv ( 'onlineMCMC_data.csv' ) # the format of the data is shown using .head() print ( data . head ()) Energy Counts 0 156.2074 1359 1 156.4335 1391 2 156.6596 1373 3 156.8857 1372 4 157.1118 1309 We can briefly plot the peak below, just to visualise the data: counts = data [ 'Counts' ] . values energy = data [ 'Energy' ] . values plt . plot ( energy , counts , 'b' ) plt . fill ( energy , counts , 'b' , alpha = 0.4 ) plt . xlabel ( 'Energy / MCA Channels' ) plt . ylabel ( 'Counts' ) plt . title ( 'Photopeak found in a gamma-ray spectrum of Ba-133' ) plt . show ()","title":"Viewing the data"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#the-model","text":"For more details on what a gamma-ray spectrum and a photopeak are, check out this section of my gamma-ray spectroscopy example. For now, I'll only state that we'll be using a Gaussian function to model the data seen above. The function takes the following form: def gauss ( x , a , xc , w , y0 ): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a * np . exp ( - ( x - xc ) ** 2 / ( 2 * w ** 2 )) + y0 # make a plot just to demonstrate shape of model # array of points for x axis test_x = np . linspace ( 0 , 1 , 100 ) # easy-to-visualise parameters a = 1 xc = 0.5 w = 0.1 y0 = 0.5 # pass to Gaussian functio test_y = gauss ( test_x , a , xc , w , y0 ) #plot plt . plot ( test_x , test_y ) plt . title ( 'Simple Gaussian Function Example' ) plt . ylim ( 0 , 1.75 ) plt . show ()","title":"The model"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#sampling-the-data","text":"Now that we know what our data looks like, and what our model looks like, we can pass everything to The Online MCMC website. Below is a screenshot of what all my inputs looked like on The Online MCMC. I'll talk through everything I did afterwards: First, I inputted the model. The Online MCMC allows for several basic functions to be used, such as trigonometric functions and exponential functions, so there is no need to approximate a model. The Online MCMC also allows for piecewise functions, which take different forms depending on the values of the x-axis. Next, the website automatically produces a list of variables in your model equation. I set x as an independant variable, and the other variables as parameters. For the independant variable x, I inputted the list of x-values (in this example the list contained in the \"energy\" variabe). For the parameters, the priors have to be defined. I did this simply by guessing at the values of the parameters using the photopeak plot above. I used a uniform prior for the background counts and standard deviation, and a normal prior for the other parameters. I next inputted the data (the list contained in the \"counts\" variable), and chose a likelihood. The Online MCMC gives a choice of Normal, Poisson, and Student's t likelihoods, of which I chose a Poisson distribution. Again, for more details on why I chose a Poisson likelihood, check out this section . Finally, I chose a sampler. I chose to use Nestle , although some other samplers I could have chosen are emcee , dynesty , and PyMC3 . I set up the number of live points, and chose a sampling method 'multi'. From there, all you have to do is enter an email address so the website can send the results once the sampling is complete, and click 'submit'.","title":"Sampling the data"},{"location":"TheOnlineMCMC/TheOnlineMCMC/#results","text":"After a short while, The Online MCMC will email you with all the results from the sampling process. The first thing available to us is a corner plot produced by \"corner.py\" : This plot shows us the posteriors for each parameter, along with contour plots showing how a parameter may vary with any other. The Online MCMC also provides a link to download the posterior samples, so you can go ahead with further analysis if needed. Comparing to this section , we can see that the mean values from both The Online MCMC and PyMC3 are very much in agreement, and the standard deviation errors are also simillar. Since this is only an example to show you can use The Online MCMC to sample data, I won't go into all the further analysis on the photopeaks (which can be found here ).","title":"Results"},{"location":"gravwaves/gravwaves/","text":"Using bilby with dynesty to estimate the properties of the first ever detected gravitational wave signal Einstein's theory of General Relativity, 1916, predicted that space-time can act as a medium for \"gravitational waves\", which are ripples caused by extremely violent cosmic events, such as black holes or neutron stars colliding. It was predicted that these waves travel at the speed of light, and could carry information about the event that caused them, and even reveal the true nature of gravity. Einstein also predicted that the waves would be so small, that their presence would never be detected. However, almost 100 years later in 2015, LIGO detected the first gravitational wave signal \"GW150914\" using a 4km long interferometer. the arms of which only stretched by a thousandth the width of a proton. The event that caused the gravitational waves was a collision between two massive black holes. In this example, I'll use the gravitational wave analysis package \"bilby\" along with the sampler \"dynesty\" to find the mass of the two black holes that caused GW150914. Useful imports # numpy import numpy as np # pandas import pandas as pd # plotting import matplotlib.pyplot as plt # sampler import bilby from bilby.core.prior import Uniform # misc from gwpy.timeseries import TimeSeries Viewing the data Bilby has in-built features that allow us to view the data that we'll be using. To do this, we have to set up interferometer objects that contain the data we'll be using. The data in question is made up of a strain dataset which contains the information of the waveform, and a Power Spectral Density (PSD) dataset which approximates the noise. We start by defining a directory for bilby to save results, and defining the source we want to analyse, and creating some empty interferometer objects to store the data: logger = bilby . core . utils . logger outdir = \"outdir\" # gravitational wave source label = \"GW150914\" H1 = bilby . gw . detector . get_empty_interferometer ( \"H1\" ) L1 = bilby . gw . detector . get_empty_interferometer ( \"L1\" ) To obtain the data, a time range around the event has to be defined and passed to \"gwpy\" , which will download the data for us from here . event_start_time = 1126259462.4 # event GPS time at which first GW discovered post_event = 2 # time after event pre_event = 2 # time before event duration = pre_event + post_event # total time range containing event analysis_begin = event_start_time - pre_event # GPS time at which to start analysis Using these time settings, we can fetch the data we want to analyse. Once the data is downloaded, we can use in-built plot functions to visualise what our data will look like. The two plots below show the strain data for Hanford and Livingston detectors respectively. H1_data = TimeSeries . fetch_open_data ( \"H1\" , analysis_begin , analysis_begin + duration , sample_rate = 4096 , cache = True ) L1_data = TimeSeries . fetch_open_data ( \"L1\" , analysis_begin , analysis_begin + duration , sample_rate = 4096 , cache = True ) H1_data . plot () plt . savefig ( \"H1_data\" ) L1_data . plot () plt . savefig ( \"L1_data\" ) We can also check out the PSD data that we'll be using to estimate noise in the data. Again using gwpy, the data can be obtained as follows: # duration typically multiplied by 32 for psd psd_duration = duration * 32 psd_begin = analysis_start - psd_duration # fetch data for psd H1psd = TimeSeries . fetch_open_data ( \"H1\" , psd_begin , psd_begin + psd_duration , sample_rate = 4096 , cache = True ) L1psd = TimeSeries . fetch_open_data ( \"L1\" , psd_begin , psd_begin + psd_duration , sample_rate = 4096 , cache = True ) # set interferometers with psd data psd_alpha = 2 * H1 . strain_data . roll_off / duration H1psd_data = H1psd . psd ( fftlength = duration , overlap = 0 , window = ( \"tukey\" , psd_alpha ), method = \"median\" ) L1psd_data = L1psd . psd ( fftlength = duration , overlap = 0 , window = ( \"tukey\" , psd_alpha ), method = \"median\" ) H1 . power_spectral_density = bilby . gw . detector . PowerSpectralDensity ( frequency_array = H1psd_data . frequencies . value , psd_array = H1psd_data . value ) L1 . power_spectral_density = bilby . gw . detector . PowerSpectralDensity ( frequency_array = L1psd_data . frequencies . value , psd_array = L1psd_data . value ) Before plotting the PSD, its useful to set an upper frequency limit as higher frequencies aren't so useful. After limiting the frequency, we can plot the PSD data: H1 . maximum_frequency = 1024 L1 . maximum_frequency = 1024 plt . figure () idxs = H1 . strain_data . frequency_mask a , b = list ( H1 . strain_data . frequency_array [ idxs ]), list ( np . abs ( H1 . strain_data . frequency_domain_strain [ idxs ])) c , d = list ( H1 . power_spectral_density . frequency_array [ idxs ]), list ( H1 . power_spectral_density . asd_array [ idxs ]) plt . plot ( a , b ) plt . plot ( c , d ) plt . yscale ( 'log' ) plt . xscale ( 'log' ) plt . xlabel ( \"Frequency [Hz]\" ) plt . ylabel ( \"Strain [strain/$\\sqrt {Hz} $]\" ) plt . savefig ( \"strainvfreq\" ) Setting the priors The model describing the black hole merger depends on 15 parameters. Running a sampler with this many parameters can take half a day to run. To save some time, I'll fix all but 4 parameters: the chirp mass, the mass ratio, the phase, and the geocent time. The chirp mass and mass ratio are two parameters wich depend on the mass of the two black holes that caused the gravitational waves. When these two parameters are derived, we can use them to calculate the masses of the black holes. Since we don't know much about the system parameters that we're allowing to vary, uniform priors are most appropriate: prior = bilby . core . prior . PriorDict () # uniform priors for variable parameters prior [ 'chirp_mass' ] = Uniform ( name = 'chirp_mass' , minimum = 10.0 , maximum = 100.0 ) prior [ 'mass_ratio' ] = Uniform ( name = 'mass_ratio' , minimum = 0.5 , maximum = 1 ) prior [ 'phase' ] = Uniform ( name = \"phase\" , minimum = 0 , maximum = 2 * np . pi ) prior [ 'geocent_time' ] = Uniform ( name = \"geocent_time\" , minimum = event_start_time - 0.1 , maximum = event_start_time + 0.1 ) # fixed values for all other parameters prior [ 'a_1' ] = 0.0 prior [ 'a_2' ] = 0.0 prior [ 'tilt_1' ] = 0.0 prior [ 'tilt_2' ] = 0.0 prior [ 'phi_12' ] = 0.0 prior [ 'phi_jl' ] = 0.0 prior [ 'dec' ] = - 1.2232 prior [ 'ra' ] = 2.19432 prior [ 'theta_jn' ] = 1.89694 prior [ 'psi' ] = 0.532268 prior [ 'luminosity_distance' ] = 412.066 Setting the likelihood Bilby comes with in-built likelihood functions. To use them, first a waveform has to be generated using the properties of the data, like duration and sampling frequency. We also need to combine the interferometers into a list, so that both will be used at once when it comes to sampling. interferometers = [ H1 , L1 ] waveform_arguments = dict ( waveform_approximant = 'IMRPhenomPv2' , reference_frequency = 50. , minimum_frequency = 20. ) # set sampling frequency of data sampling_frequency = 2048. # generate waveforms waveform_generator = bilby . gw . WaveformGenerator ( duration = duration , sampling_frequency = sampling_frequency , frequency_domain_source_model = bilby . gw . source . lal_binary_black_hole , parameter_conversion = bilby . gw . conversion . convert_to_lal_binary_black_hole_parameters , waveform_arguments = waveform_arguments ) likelihood = bilby . gw . GravitationalWaveTransient ( interferometers = interferometers , waveform_generator = waveform_generator ) Sampling the data Now that the likelihood and prior functions are set up, the sampler can now be implemented to derive some of the properties of the binary black hole system. This process is intensive, and takes a long time (1 hour 15 minutes) to run on my machine. The sampler I chose to use is \"dynesty\", which uses nested sampling to estimate the model parameters. For a more in-depth work through of dynesty, you can read through an example of gamma-ray spectroscopy using dynesty on this site. Using bilby, we can run the sampler. First, we have to set the number of live points, and the stopping criterion as hyperparameters. For neatness, I've removed the log that bilby outputs whilst running. Instead I'll just show the summary that is outputted by bilby once the sampler has finished iterating. This summary contains information on the log evidence, and the Bayes factor of the model. nlive = 1000 # live points stop = 0.1 # stopping criterion method = \"unif\" # method of sampling sampler = \"dynesty\" # sampler to use result = bilby . run_sampler ( likelihood , prior , sampler = sampler , outdir = outdir , label = label , conversion_function = bilby . gw . conversion . generate_all_bbh_parameters , sample = method , nlive = nlive , dlogz = stop ) 16 : 23 bilby INFO : Sampling time : 1 : 12 : 43.878674 16 : 23 bilby INFO : Summary of results : nsamples : 23105 ln_noise_evidence : - 8112.144 ln_evidence : - 8729.851 +/- 0.188 ln_bayes_factor : - 617.707 +/- 0.188 Plotting the posterior Eventually, the sampling will be complete. Bilby creates a few useful plots of the posteriors, along with saving samples of the posteriors for us to further analyse. Lets start by plotting a corner plot, which nicely shows the posteriors for all the parameters, as well as contour plots showing how one parameter varies with any other. result . plot_corner () We can also view the traces for each parameter, which bilby will save by default under the directory labeled as \"outdir\": Estimating the black hole masses To estimate the black hole masses, we need to obtain the samples describing the posteriors of the data. This can be done quite easily as bilby returns the posterior samples. Alternatively, bilby also saves posterior samples to a text file, which can be loaded using pandas. I'll use the latter method, so I don't have to re-run the sampler. # load posterior samples into a pandas dataframe directly # posts = result.posterior # alternatively, load posts using samples saved by bilby using pandas posts = pd . read_csv ( \"GWSampleDemo_samples.dat\" , sep = \" \" ) chirp_mass_samps = list ( posts [ \"chirp_mass\" ]) mass_ratio_samps = list ( posts [ \"mass_ratio\" ]) geocent_time_samps = list ( posts [ \"geocent_time\" ]) Next, we can take the posterior samples and find their mean values: chirp_mass , chirp_mass_err = np . mean ( chirp_mass_samps ), np . std ( chirp_mass_samps ) mass_ratio , mass_ratio_err = np . mean ( mass_ratio_samps ), np . std ( mass_ratio_samps ) geocent_time , geocent_time_err = np . mean ( geocent_time_samps ), np . std ( geocent_time_samps ) print ( \"Parameters describing the GW150914 signal: \\n \\n \" + \" chirp mass = {} \\u00B1 {} \\n \" . format ( chirp_mass , chirp_mass_err ) + \" mass ratio = {} \\u00B1 {} \\n \\n \" . format ( mass_ratio , mass_ratio_err ) + \"geocent time = {} \\u00B1 {} \\n \" . format ( geocent_time , geocent_time_err ) + \"True geocent time = 1126259462.4\" ) Parameters describing the GW150914 signal: chirp mass = 28.432363440275488 \u00b1 0.049549938503053295 mass ratio = 0.7520294836733314 \u00b1 0.06062816795824152 geocent time = 1126259462.4128802 \u00b1 7.509222877728134e-05 True geocent time = 1126259462.4 The time of the event lines up quite nicely. Next, we can attempt to calculate the mass of each black hole, which is fairly straight forward now that we know the chirp mass and mass ratio. The following function will return the masses of the black holes, given the chirp mass, mass ratio, and their errors. def bbhMass ( chirp , chirperr , ratio , ratioerr ): \"\"\" Function that calculates black hole masses with errors m1,m2 = larger black hole mass, smaller black hole mass chirp = (m1*m2)**(3/5) / (m1+m2)**(1/5) ratio = m2/m1 \"\"\" # use chirp mass and ratio to find primary black hole mass m1 = chirp / ( ratio ** ( 3 / 5 ) * ( 1 / ( 1 + ratio )) ** ( 1 / 5 )) m1err = ( chirp + chirperr ) / (( ratio - ratioerr ) ** ( 3 / 5 ) * ( 1 / ( 1 + ( ratio + ratioerr )) ** ( 1 / 5 ))) - m1 # use primary black hole mass and ratio to find secondary black hole mass m2 = m1 * ratio m2err = ( m1 + m1err ) * ( ratio + ratioerr ) - m2 return ( m1 , m1err ),( m2 , m2err ) ( m1 , m1err ), ( m2 , m2err ) = bbhMass ( chirp_mass , chirp_mass_err , mass_ratio , mass_ratio_err ) print ( \"Black hole masses (in solar masses): \\n \\n \" + \" largest black hole = {} \\u00B1 {} \\n \" . format ( m1 , m1err ) + \" smallest black hole = {} \\u00B1 {} \\n \\n \" . format ( m2 , m2err ) + \"True black hole masses (in solar masses): \\n \\n \" + \" largest black hole = {} \\n \" . format ( 36.2 ) + \" smallest black hole = {} \" . format ( 29.1 )) Black hole masses (in solar masses): largest black hole = 37.73812541182669 \u00b1 2.2926680096744576 smallest black hole = 28.38018296825545 \u00b1 4.151147606610291 True black hole masses (in solar masses): largest black hole = 36.2 smallest black hole = 29.1 Our estimates here are within one standard deviation error of the accpeted black hole masses. This is a good result given that we only used 4 of 15 parameters. However allowing the other 11 parameters to vary would certainly reduce the error on our estimate, at the cost of 15 hours processing time.","title":"Gravitational Wave Parameter Estimation with bilby"},{"location":"gravwaves/gravwaves/#using-bilby-with-dynesty-to-estimate-the-properties-of-the-first-ever-detected-gravitational-wave-signal","text":"Einstein's theory of General Relativity, 1916, predicted that space-time can act as a medium for \"gravitational waves\", which are ripples caused by extremely violent cosmic events, such as black holes or neutron stars colliding. It was predicted that these waves travel at the speed of light, and could carry information about the event that caused them, and even reveal the true nature of gravity. Einstein also predicted that the waves would be so small, that their presence would never be detected. However, almost 100 years later in 2015, LIGO detected the first gravitational wave signal \"GW150914\" using a 4km long interferometer. the arms of which only stretched by a thousandth the width of a proton. The event that caused the gravitational waves was a collision between two massive black holes. In this example, I'll use the gravitational wave analysis package \"bilby\" along with the sampler \"dynesty\" to find the mass of the two black holes that caused GW150914.","title":"Using bilby with dynesty to estimate the properties of the first ever detected gravitational wave signal"},{"location":"gravwaves/gravwaves/#useful-imports","text":"# numpy import numpy as np # pandas import pandas as pd # plotting import matplotlib.pyplot as plt # sampler import bilby from bilby.core.prior import Uniform # misc from gwpy.timeseries import TimeSeries","title":"Useful imports"},{"location":"gravwaves/gravwaves/#viewing-the-data","text":"Bilby has in-built features that allow us to view the data that we'll be using. To do this, we have to set up interferometer objects that contain the data we'll be using. The data in question is made up of a strain dataset which contains the information of the waveform, and a Power Spectral Density (PSD) dataset which approximates the noise. We start by defining a directory for bilby to save results, and defining the source we want to analyse, and creating some empty interferometer objects to store the data: logger = bilby . core . utils . logger outdir = \"outdir\" # gravitational wave source label = \"GW150914\" H1 = bilby . gw . detector . get_empty_interferometer ( \"H1\" ) L1 = bilby . gw . detector . get_empty_interferometer ( \"L1\" ) To obtain the data, a time range around the event has to be defined and passed to \"gwpy\" , which will download the data for us from here . event_start_time = 1126259462.4 # event GPS time at which first GW discovered post_event = 2 # time after event pre_event = 2 # time before event duration = pre_event + post_event # total time range containing event analysis_begin = event_start_time - pre_event # GPS time at which to start analysis Using these time settings, we can fetch the data we want to analyse. Once the data is downloaded, we can use in-built plot functions to visualise what our data will look like. The two plots below show the strain data for Hanford and Livingston detectors respectively. H1_data = TimeSeries . fetch_open_data ( \"H1\" , analysis_begin , analysis_begin + duration , sample_rate = 4096 , cache = True ) L1_data = TimeSeries . fetch_open_data ( \"L1\" , analysis_begin , analysis_begin + duration , sample_rate = 4096 , cache = True ) H1_data . plot () plt . savefig ( \"H1_data\" ) L1_data . plot () plt . savefig ( \"L1_data\" ) We can also check out the PSD data that we'll be using to estimate noise in the data. Again using gwpy, the data can be obtained as follows: # duration typically multiplied by 32 for psd psd_duration = duration * 32 psd_begin = analysis_start - psd_duration # fetch data for psd H1psd = TimeSeries . fetch_open_data ( \"H1\" , psd_begin , psd_begin + psd_duration , sample_rate = 4096 , cache = True ) L1psd = TimeSeries . fetch_open_data ( \"L1\" , psd_begin , psd_begin + psd_duration , sample_rate = 4096 , cache = True ) # set interferometers with psd data psd_alpha = 2 * H1 . strain_data . roll_off / duration H1psd_data = H1psd . psd ( fftlength = duration , overlap = 0 , window = ( \"tukey\" , psd_alpha ), method = \"median\" ) L1psd_data = L1psd . psd ( fftlength = duration , overlap = 0 , window = ( \"tukey\" , psd_alpha ), method = \"median\" ) H1 . power_spectral_density = bilby . gw . detector . PowerSpectralDensity ( frequency_array = H1psd_data . frequencies . value , psd_array = H1psd_data . value ) L1 . power_spectral_density = bilby . gw . detector . PowerSpectralDensity ( frequency_array = L1psd_data . frequencies . value , psd_array = L1psd_data . value ) Before plotting the PSD, its useful to set an upper frequency limit as higher frequencies aren't so useful. After limiting the frequency, we can plot the PSD data: H1 . maximum_frequency = 1024 L1 . maximum_frequency = 1024 plt . figure () idxs = H1 . strain_data . frequency_mask a , b = list ( H1 . strain_data . frequency_array [ idxs ]), list ( np . abs ( H1 . strain_data . frequency_domain_strain [ idxs ])) c , d = list ( H1 . power_spectral_density . frequency_array [ idxs ]), list ( H1 . power_spectral_density . asd_array [ idxs ]) plt . plot ( a , b ) plt . plot ( c , d ) plt . yscale ( 'log' ) plt . xscale ( 'log' ) plt . xlabel ( \"Frequency [Hz]\" ) plt . ylabel ( \"Strain [strain/$\\sqrt {Hz} $]\" ) plt . savefig ( \"strainvfreq\" )","title":"Viewing the data"},{"location":"gravwaves/gravwaves/#setting-the-priors","text":"The model describing the black hole merger depends on 15 parameters. Running a sampler with this many parameters can take half a day to run. To save some time, I'll fix all but 4 parameters: the chirp mass, the mass ratio, the phase, and the geocent time. The chirp mass and mass ratio are two parameters wich depend on the mass of the two black holes that caused the gravitational waves. When these two parameters are derived, we can use them to calculate the masses of the black holes. Since we don't know much about the system parameters that we're allowing to vary, uniform priors are most appropriate: prior = bilby . core . prior . PriorDict () # uniform priors for variable parameters prior [ 'chirp_mass' ] = Uniform ( name = 'chirp_mass' , minimum = 10.0 , maximum = 100.0 ) prior [ 'mass_ratio' ] = Uniform ( name = 'mass_ratio' , minimum = 0.5 , maximum = 1 ) prior [ 'phase' ] = Uniform ( name = \"phase\" , minimum = 0 , maximum = 2 * np . pi ) prior [ 'geocent_time' ] = Uniform ( name = \"geocent_time\" , minimum = event_start_time - 0.1 , maximum = event_start_time + 0.1 ) # fixed values for all other parameters prior [ 'a_1' ] = 0.0 prior [ 'a_2' ] = 0.0 prior [ 'tilt_1' ] = 0.0 prior [ 'tilt_2' ] = 0.0 prior [ 'phi_12' ] = 0.0 prior [ 'phi_jl' ] = 0.0 prior [ 'dec' ] = - 1.2232 prior [ 'ra' ] = 2.19432 prior [ 'theta_jn' ] = 1.89694 prior [ 'psi' ] = 0.532268 prior [ 'luminosity_distance' ] = 412.066","title":"Setting the priors"},{"location":"gravwaves/gravwaves/#setting-the-likelihood","text":"Bilby comes with in-built likelihood functions. To use them, first a waveform has to be generated using the properties of the data, like duration and sampling frequency. We also need to combine the interferometers into a list, so that both will be used at once when it comes to sampling. interferometers = [ H1 , L1 ] waveform_arguments = dict ( waveform_approximant = 'IMRPhenomPv2' , reference_frequency = 50. , minimum_frequency = 20. ) # set sampling frequency of data sampling_frequency = 2048. # generate waveforms waveform_generator = bilby . gw . WaveformGenerator ( duration = duration , sampling_frequency = sampling_frequency , frequency_domain_source_model = bilby . gw . source . lal_binary_black_hole , parameter_conversion = bilby . gw . conversion . convert_to_lal_binary_black_hole_parameters , waveform_arguments = waveform_arguments ) likelihood = bilby . gw . GravitationalWaveTransient ( interferometers = interferometers , waveform_generator = waveform_generator )","title":"Setting the likelihood"},{"location":"gravwaves/gravwaves/#sampling-the-data","text":"Now that the likelihood and prior functions are set up, the sampler can now be implemented to derive some of the properties of the binary black hole system. This process is intensive, and takes a long time (1 hour 15 minutes) to run on my machine. The sampler I chose to use is \"dynesty\", which uses nested sampling to estimate the model parameters. For a more in-depth work through of dynesty, you can read through an example of gamma-ray spectroscopy using dynesty on this site. Using bilby, we can run the sampler. First, we have to set the number of live points, and the stopping criterion as hyperparameters. For neatness, I've removed the log that bilby outputs whilst running. Instead I'll just show the summary that is outputted by bilby once the sampler has finished iterating. This summary contains information on the log evidence, and the Bayes factor of the model. nlive = 1000 # live points stop = 0.1 # stopping criterion method = \"unif\" # method of sampling sampler = \"dynesty\" # sampler to use result = bilby . run_sampler ( likelihood , prior , sampler = sampler , outdir = outdir , label = label , conversion_function = bilby . gw . conversion . generate_all_bbh_parameters , sample = method , nlive = nlive , dlogz = stop ) 16 : 23 bilby INFO : Sampling time : 1 : 12 : 43.878674 16 : 23 bilby INFO : Summary of results : nsamples : 23105 ln_noise_evidence : - 8112.144 ln_evidence : - 8729.851 +/- 0.188 ln_bayes_factor : - 617.707 +/- 0.188","title":"Sampling the data"},{"location":"gravwaves/gravwaves/#plotting-the-posterior","text":"Eventually, the sampling will be complete. Bilby creates a few useful plots of the posteriors, along with saving samples of the posteriors for us to further analyse. Lets start by plotting a corner plot, which nicely shows the posteriors for all the parameters, as well as contour plots showing how one parameter varies with any other. result . plot_corner () We can also view the traces for each parameter, which bilby will save by default under the directory labeled as \"outdir\":","title":"Plotting the posterior"},{"location":"gravwaves/gravwaves/#estimating-the-black-hole-masses","text":"To estimate the black hole masses, we need to obtain the samples describing the posteriors of the data. This can be done quite easily as bilby returns the posterior samples. Alternatively, bilby also saves posterior samples to a text file, which can be loaded using pandas. I'll use the latter method, so I don't have to re-run the sampler. # load posterior samples into a pandas dataframe directly # posts = result.posterior # alternatively, load posts using samples saved by bilby using pandas posts = pd . read_csv ( \"GWSampleDemo_samples.dat\" , sep = \" \" ) chirp_mass_samps = list ( posts [ \"chirp_mass\" ]) mass_ratio_samps = list ( posts [ \"mass_ratio\" ]) geocent_time_samps = list ( posts [ \"geocent_time\" ]) Next, we can take the posterior samples and find their mean values: chirp_mass , chirp_mass_err = np . mean ( chirp_mass_samps ), np . std ( chirp_mass_samps ) mass_ratio , mass_ratio_err = np . mean ( mass_ratio_samps ), np . std ( mass_ratio_samps ) geocent_time , geocent_time_err = np . mean ( geocent_time_samps ), np . std ( geocent_time_samps ) print ( \"Parameters describing the GW150914 signal: \\n \\n \" + \" chirp mass = {} \\u00B1 {} \\n \" . format ( chirp_mass , chirp_mass_err ) + \" mass ratio = {} \\u00B1 {} \\n \\n \" . format ( mass_ratio , mass_ratio_err ) + \"geocent time = {} \\u00B1 {} \\n \" . format ( geocent_time , geocent_time_err ) + \"True geocent time = 1126259462.4\" ) Parameters describing the GW150914 signal: chirp mass = 28.432363440275488 \u00b1 0.049549938503053295 mass ratio = 0.7520294836733314 \u00b1 0.06062816795824152 geocent time = 1126259462.4128802 \u00b1 7.509222877728134e-05 True geocent time = 1126259462.4 The time of the event lines up quite nicely. Next, we can attempt to calculate the mass of each black hole, which is fairly straight forward now that we know the chirp mass and mass ratio. The following function will return the masses of the black holes, given the chirp mass, mass ratio, and their errors. def bbhMass ( chirp , chirperr , ratio , ratioerr ): \"\"\" Function that calculates black hole masses with errors m1,m2 = larger black hole mass, smaller black hole mass chirp = (m1*m2)**(3/5) / (m1+m2)**(1/5) ratio = m2/m1 \"\"\" # use chirp mass and ratio to find primary black hole mass m1 = chirp / ( ratio ** ( 3 / 5 ) * ( 1 / ( 1 + ratio )) ** ( 1 / 5 )) m1err = ( chirp + chirperr ) / (( ratio - ratioerr ) ** ( 3 / 5 ) * ( 1 / ( 1 + ( ratio + ratioerr )) ** ( 1 / 5 ))) - m1 # use primary black hole mass and ratio to find secondary black hole mass m2 = m1 * ratio m2err = ( m1 + m1err ) * ( ratio + ratioerr ) - m2 return ( m1 , m1err ),( m2 , m2err ) ( m1 , m1err ), ( m2 , m2err ) = bbhMass ( chirp_mass , chirp_mass_err , mass_ratio , mass_ratio_err ) print ( \"Black hole masses (in solar masses): \\n \\n \" + \" largest black hole = {} \\u00B1 {} \\n \" . format ( m1 , m1err ) + \" smallest black hole = {} \\u00B1 {} \\n \\n \" . format ( m2 , m2err ) + \"True black hole masses (in solar masses): \\n \\n \" + \" largest black hole = {} \\n \" . format ( 36.2 ) + \" smallest black hole = {} \" . format ( 29.1 )) Black hole masses (in solar masses): largest black hole = 37.73812541182669 \u00b1 2.2926680096744576 smallest black hole = 28.38018296825545 \u00b1 4.151147606610291 True black hole masses (in solar masses): largest black hole = 36.2 smallest black hole = 29.1 Our estimates here are within one standard deviation error of the accpeted black hole masses. This is a good result given that we only used 4 of 15 parameters. However allowing the other 11 parameters to vary would certainly reduce the error on our estimate, at the cost of 15 hours processing time.","title":"Estimating the black hole masses"},{"location":"particleID/particleID/","text":"Using PyStan and DNest4 to identify particles produced in proton-proton collisions The Large Hadron Collider (LHC) is a particle accelerator that is capable of producing proton beams at near light-speed. Two of these beams are targeted at each other, so that very high-energy protons collide head-on. This collision creates many new particles, most of which are known hadrons or mesons. However, some of the created particles are heavy bosons such as the W boson, and the Higgs boson. These bosons have a very short lifetime, limited by the uncertainty principle , and so they decay long before they can reach the particle detectors. The only way to identify the existence of these particles is to infer their presence from their decay products. In the following example, I'll use \"PyStan\" and \"DNest4\" to identify particles created in proton-proton collisions at LHC. Useful imports # numpy import numpy as np # pandas import pandas as pd # scipy from scipy.special import ndtri from scipy.stats import gaussian_kde # plotting import matplotlib.pyplot as plt import corner # sampler import pystan from dnest4 import randh , wrap import dnest4 print ( \"PyStan version: {} \" . format ( pystan . __version__ )) print ( \"DNest4 version: {} \" . format ( dnest4 . __version__ )) # misc from time import time PyStan version: 2.19.1.1 DNest4 version: 0.2.4 Viewing the data First, I'll use pandas to load the data. The data set is made up of 1.1 million collisions, each with 28 different attributes. The first 21 attributes are \"low-level\", and are only really useful for calculating the last 7 attributes. The last 7 attributes describe the invariant mass for different processes. I'll explain further what these processes are later on, but for now I'll start by loading in the data. Since the data set is so large at 1.1 million data points, I decided to randomly sample 10000 of them, just to save some processing time. # load data df = pd . read_csv ( 'HIGGS.csv' , header = None ) # select only last 7 attributes df_reduce = df [ df . columns [ - 7 :]] # randomly choose 10000 data points nrows = 10000 data = df_reduce . sample ( nrows , random_state = 2020 ) For a particle A, which decays via the process A \\rightarrow B + C into particles B and C, the invariant mass squared m_A^2 = m_{B+C}^2 = (E_B + E_C)^2 - |(\\vec{p}_B + \\vec{p}_C)|^2 is a Lorentz invariant quantity, meaning it is the same in all reference frames. This means that in the zero-momentum frame of reference, the invariant mass is just given by the mass of the particle A, m_A . The data set is made up of 10000 collisions, in any of which particle A may have been produced. Particle A's subsequent decay results in a a peak around m_A , and therefore by finding the centre of the peak the invariant mass of the particle can be found. Mass is typically used to identify particles since a particle's mass is unique. There are three attributes in particular that are of interest for us. The first is the invariant mass of the process W \\rightarrow l \\nu that describes a W boson decay into a lepton neutrino pair. The peak invariant mass m_{l\\nu} of this distribution should be at the W boson mass m_W . The second attribute is the invariant mass for the process t \\rightarrow W b that describes a top quark decaying into a W boson and a bottom quark. The invariant mass m_{Wb} should peak at the mass of the top quark m_t . Finally, the third attribute is the invariant mass for the process h_0 \\rightarrow b \\bar{b} of a Higgs decay into a bottom-antibottom meson. The invariant mass m_{b\\bar{b}} should peak at the mass of the Higgs boson m_{h_0} . Since we have the three invariant mass distributions, we can use them to measure the masses of the W boson, top quark, and Higgs boson. This is done by plotting a histogram of the invariant mass distributions, since the y-axis gives an idea of the relative frequency of each collision having a certain invariant mass: hist_w = np . array ( data [ 22 ]) # invariant mass for m_lv hist_t = np . array ( data [ 25 ]) # invariant mass for m_Wb hist_h = np . array ( data [ 26 ]) # invariant mass for m_bb fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , figsize = ( 10 , 12 )) bins = 1000 # histogram predicting W boson mass n_w , bins_w , patches_w = ax1 . hist ( hist_w , bins ) # histogram predicting top quark mass n_t , bins_t , patches_t = ax2 . hist ( hist_t , bins ) # histogram predicting Higgs boson mass n_h , bins_h , patches_h = ax3 . hist ( hist_h , bins ) ax1 . set_ylabel ( 'Frequency density' ) ax1 . set_title ( 'Invariant mass $m_ {lv} $ of W decay to lepton neutrino pair' ) ax2 . set_ylabel ( 'Frequency density' ) ax2 . set_title ( 'Invariant mass $m_ {Wb} $ of top quark decay to W bottom pair' ) ax3 . set_ylabel ( 'Frequency density' ) ax3 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) ax3 . set_title ( 'Invariant mass $m_ {bb} $ of Higgs decay to bottom antibottom pair' ) plt . show () The model The peaks reach a sharp peak, after which they fall off with what looks like a 1/m^\\alpha relationship. To model this, I used a quadratic ascending limb, and a reciprocal quadratic decending limb. The model is implemented in Python with the following parameters: def polynom ( masses , h , m_in , m_max , m_fin , t_1 , t_2 ): \"\"\" Function to model a particle production frequency as a function of invariant mass :param masses: array of invariant masses :param h: amplitude of the peak :param m_in: initial invariant mass, at which the peak begins :param m_max: invariant mass with coresponding maximum frequency :param m_fin: final invariant mass, at which the peak reaches background level :param t_1: describes the ascending limb growth rate :param t_2: describes the descending limb decay rate :return: array of relative frequencies of invariant masses \"\"\" masses = np . array ( masses ) y = [ 0 if m < m_in else h * (( m - m_in ) / ( m_max - m_in )) ** t_1 if m < m_max else h * (( m - m_max ) / ( m_fin - m_max ) + 1 ) ** ( - t_2 ) if m < m_fin else 0 for m in masses ] return np . array ( y ) To show what this model looks like, we can make some quick guesses of the parameters, just by looking at the peak in the m_{l\\nu} distribution: x = np . linspace ( 0 , 400 , 1400 ) # guess at each parameter guess_y = polynom ( x , 380 , - 20 , 80 , 240 , 6 , 14 ) plt . plot ( bins_w , n_w , label = 'Original data' ) plt . plot ( x , guess_y , label = 'Example model' ) plt . xlim ( 0 , 270 ) plt . title ( 'Example peak model overplotted on $m_ {lv} $ data' ) plt . xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) plt . ylabel ( 'Frequency density' ) plt . legend () plt . show () The model looks like it sufficiently describes the frequency peak. The parameter that we are primarily interested in is m_max, since it predicts the most frequent invariant mass in this range; which for this example should be equal to the mass of the W boson. Modelling with PyStan PyStan is a Python wrapper for a C++ based sampler \"Stan\". To use PyStan, we first have to write a code string in C++. This code string is split into 5 blocks: functions (optional), data, parameters, transformed parameters (optional), and model. The functions block contains user-defined functions, such as the model function defined above. The data block initialises variables such as the number of data points, the invariant masses, and their respective frequency densities. The parameters block initialises the parameters of the model. The transformed parameters block gives the expected value of the model, given the current parameters. Finally, the model block sets the priors and likelihood distrubutions. pystan_code = \"\"\" functions {{ real polynom(real x, real h, real m_in, real m_max, real m_fin, real t_1, real t_2) if (x < m_in) return 0; else if (x < m_max && t_1 > 0) return h*pow( (x-m_in)/(m_max-m_in), t_1 ); else if (x < m_max && t_1 < 0) return h*pow( (x-m_in)/(m_max-m_in), 0.1 ); else if (x < m_fin) return h*pow( (x-m_max)/(m_fin-m_max) + 1, -t_2 ); else return 0; }} data {{ int<lower=0> N; // number of data points real y[N]; // frequency densities real m[N]; // invariant masses }} parameters {{ // parameters of the model real h; real m_in; real m_max; real m_fin; real<lower=0> t_1; real<lower=0> t_2; real sigma; }} transformed parameters {{ real theta[N]; // for each invariant mass m[j], expected y-value is found for (j in 1:N) theta[j] = polynom(m[j],h,m_in,m_max,m_fin,t_1,t_2); }} model {{ // normal priors h ~ normal( {h_mu} , {h_sig} ); m_in ~ normal( {m_in_mu} , {m_in_sig} ); m_max ~ normal( {m_max_mu} , {m_max_sig} ); m_fin ~ normal( {m_fin_mu} , {m_fin_sig} ); t_1 ~ normal( {t_1_mu} , {t_1_sig} ); t_2 ~ normal( {t_2_mu} , {t_2_sig} ); // uniform parameters sigma ~ uniform(-2, 2); // normal likelihood y ~ normal(theta,pow(10,sigma)); }} \"\"\" In the above code string, there is an extra parameter \"sigma\". This parameter is used to model the noise of the data. Since this values is difficult to guess, the value of sigma is set to be uniform in log-space. This means that noise standard deviation has an equal probabillity of being between an order of magnitude of -2 and 2. This is used as an alternate to using a Poisson distribution. A comparison example between this likelihood and a Poisson likelihood can be found on this site here . Next, we need to pass information to the code string. This is done using a dictionary that contains the number of data points, frequency density, and invariant mass data points, and another dictionary that contains all the prior bounds for each parameter. I'll start using the m_{lv} distribution: data_w = { 'N' : len ( n_w ), 'y' : n_w , 'm' : bins_w } priors_w = {} priors_w [ 'h_mu' ], priors_w [ 'h_sig' ] = 380 , 10 priors_w [ 'm_in_mu' ], priors_w [ 'm_in_sig' ] = - 10 , 5 priors_w [ 'm_max_mu' ], priors_w [ 'm_max_sig' ] = 80 , 5 priors_w [ 'm_fin_mu' ], priors_w [ 'm_fin_sig' ] = 260 , 10 priors_w [ 't_1_mu' ], priors_w [ 't_1_sig' ] = 5 , 2 priors_w [ 't_2_mu' ], priors_w [ 't_2_sig' ] = 16 , 2 Sampling the data The code string must now be compiled using the priors dictionary defined above. time0 = time () model_w = pystan . StanModel ( model_code = pystan_code . format ( ** priors_w )) time1 = time () print ( \"Time taken to compile model: {} seconds\" . format ( time1 - time0 )) INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_32a27c4325414afa7499109a8951a53e NOW. Time taken to compile model: 44.89909029006958 seconds The sampler is now ready to iterate through the data. I'll do this using 1000 samples and 1 chain: Nsamples = 1000 chains = 1 time0 = time () fit_w = model_w . sampling ( data = data_w , iter = Nsamples , chains = chains ) time1 = time () print ( \"Time taken to sample m_lv peak: \" + str ( time1 - time0 ) + \" seconds\" ) Time taken to sample m_lv peak: 1.9440805912017822 seconds The results can be extracted from the sampler using a dictionary as follows: la_w = fit_w . extract ( permuted = True ) # posterior samples samples_w = np . vstack (( la_w [ 'h' ], la_w [ 'm_in' ], la_w [ 'm_max' ], la_w [ 'm_fin' ], la_w [ 't_1' ], la_w [ 't_2' ])) . T Now that we understand how the sampling process works, we can quickly do the exact same process for the other two peaks. First, make guesses for parameter values for each data set: # data for data_t = { 'N' : len ( n_t ), 'y' : n_t , 'm' : bins_t } data_h = { 'N' : len ( n_h ), 'y' : n_h , 'm' : bins_h } priors_t = {} priors_t [ 'h_mu' ], priors_t [ 'h_sig' ] = 110 , 5 priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ] = 90 , 5 priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ] = 170 , 2 priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ] = 400 , 10 priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ] = 0.7 , 0.1 priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ] = 5 , 1 priors_h = {} priors_h [ 'h_mu' ], priors_h [ 'h_sig' ] = 120 , 10 priors_h [ 'm_in_mu' ], priors_h [ 'm_in_sig' ] = 0 , 5 priors_h [ 'm_max_mu' ], priors_h [ 'm_max_sig' ] = 120 , 5 priors_h [ 'm_fin_mu' ], priors_h [ 'm_fin_sig' ] = 350 , 10 priors_h [ 't_1_mu' ], priors_h [ 't_1_sig' ] = 2 , 1 priors_h [ 't_2_mu' ], priors_h [ 't_2_sig' ] = 6 , 1 Now we can sample both peaks in tandem in the exact way as before: model_t = pystan . StanModel ( model_code = pystan_code . format ( ** priors_t )) model_h = pystan . StanModel ( model_code = pystan_code . format ( ** priors_h )) time0 = time () fit_t = model_w . sampling ( data = data_t , iter = Nsamples , chains = chains ) time1 = time () fit_h = model_w . sampling ( data = data_h , iter = Nsamples , chains = chains ) time2 = time () print ( \"Time taken to sample m_Wb peak: \" + str ( time1 - time0 ) + \" seconds\" ) print ( \"Time taken to sample m_bb} peak: \" + str ( time2 - time1 ) + \" seconds\" ) INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_c8d0398ecec8d290f88238a962b61a3e NOW. INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_fc7e337091261cd27b6faca01371228a NOW. Time taken to sample m_Wb peak: 0.8660006523132324 seconds Time taken to sample m_bb peak: 1.7904999256134032 seconds Finally, the results can be collected again: la_t = fit_t . extract ( permuted = True ) la_h = fit_h . extract ( permuted = True ) # posterior samples samples_t = np . vstack (( la_t [ 'h' ], la_t [ 'm_in' ], la_t [ 'm_max' ], la_t [ 'm_fin' ], la_t [ 't_1' ], la_t [ 't_2' ])) . T samples_h = np . vstack (( la_h [ 'h' ], la_h [ 'm_in' ], la_h [ 'm_max' ], la_h [ 'm_fin' ], la_h [ 't_1' ], la_h [ 't_2' ])) . T Results Now that all of the peaks have been sampled, we can check out the posterior samples returned by PyStan. We'll start by plotting a corner plot for the m_{l\\nu} model, which shows the posterior distributions for each parameter, along with a contour plot describing how a parameter may vary with any other. This is done using \"corner.py\" and a Gaussian KDE function from scipy: def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'h' , 'm_in' , 'm_max' , 'm_fin' , 't_1' , 't_2' ] plotposts ( samples_w , labels ) We're interested in only the value of m_max for each model, since it describes the mass of the particle that created the peak. We can collect the means and standard deviation errors for each parameter, along with randomly sampling from the posteriors so that we can create a posterior predictive plot. # mean and error of parameters for m_lv distribution means_w = [ np . mean ( samples_w [:, i ]) for i in range ( 6 )] errors_w = [ np . std ( samples_w [:, i ]) for i in range ( 6 )] # mean and error of parameters for m_Wb distribution means_t = [ np . mean ( samples_t [:, i ]) for i in range ( 6 )] errors_t = [ np . std ( samples_t [:, i ]) for i in range ( 6 )] # mean and error of parameters for m_bb distribution means_h = [ np . mean ( samples_h [:, i ]) for i in range ( 6 )] errors_h = [ np . std ( samples_h [:, i ]) for i in range ( 6 )] # number of random samples from posteriors nfits = 300 # sample parameters for m_lv distribution param_samples_w = [ np . random . choice ( samples_w [:, i ], nfits ) for i in range ( 6 )] post_samples_w = np . array ( param_samples_w ) . T # sample parameters for m_Wb distribution param_samples_t = [ np . random . choice ( samples_t [:, i ], nfits ) for i in range ( 6 )] post_samples_t = np . array ( param_samples_t ) . T # sample parameters for m_bb distribution param_samples_h = [ np . random . choice ( samples_h [:, i ], nfits ) for i in range ( 6 )] post_samples_h = np . array ( param_samples_h ) . T print ( \"Mean Invariant Mass of m_lv peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_w [ 2 ], errors_w [ 2 ]) + \"Mass of a W boson: 80.4 GeV/c \\u00b2 \\n \\n \" + \"Mean Invariant Mass of m_Wb peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_t [ 2 ], errors_t [ 2 ]) + \"Mass of top quark: 173 GeV/c \\u00b2 \\n \\n \" + \"Mean Invariant Mass of m_bb peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_h [ 2 ], errors_h [ 2 ]) + \"Mass of Higgs boson: 125 GeV/c \\u00b2 \\n \\n \" ) Mean Invariant Mass of m_lv peak: 78.8824875109673 \u00b1 0.14962446128127846 GeV/c\u00b2 Mass of a W boson: 80.4 GeV/c\u00b2 Mean Invariant Mass of m_Wb peak: 170.739515663670765 \u00b1 2.9494226571197104 GeV/c\u00b2 Mass of top quark: 173 GeV/c\u00b2 Mean Invariant Mass of m_bb peak: 121.08971310473981 \u00b1 0.7304177489930841 GeV/c\u00b2 Mass of Higgs boson: 125 GeV/c\u00b2 The values for m_max make good estimates for the masses of the particles that caused the peaks. The predictions are systematically slightly lower than the true masses, which is likely due to the model I used being only a simplified approximation, instead of a more accurate model. The increased error on the estimate for the mass of the top quark is likely due to a much higher level of noise in the data. Plotting the posterior We can plot the posterior distributions for each peak to see the quality of the fit produced by the sampler. Below are plots for both the mean posterior (left) and the posterior predictive (right). x = np . linspace ( 0 , 1400 , 1400 ) fig , axs = plt . subplots ( 3 , 2 , figsize = ( 10 , 12 )) # m_lv peak # mean posterior plot axs [ 0 , 0 ] . plot ( bins_w , n_w , label = 'Original data' ) res_y = polynom ( x , * means_w ) axs [ 0 , 0 ] . plot ( x , res_y , label = 'Fitted model' ) axs [ 0 , 0 ] . set_xlim ( 0 , 200 ) axs [ 0 , 0 ] . set_ylabel ( 'Frequency density' ) axs [ 0 , 0 ] . set_title ( 'Mean posterior plots' ) axs [ 0 , 0 ] . legend ( loc = 'upper right' ) #plot posterior predictive plot axs [ 0 , 1 ] . plot ( bins_w , n_w ) axs [ 0 , 1 ] . set_title ( 'Posterior predictive plots' ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_w [ i ]) axs [ 0 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 0 , 1 ] . set_xlim ( 0 , 200 ) # m_Wb peak # mean posterior plot axs [ 1 , 0 ] . plot ( bins_t , n_t ) res_y = polynom ( x , * means_t ) axs [ 1 , 0 ] . plot ( x , res_y ) axs [ 1 , 0 ] . set_xlim ( 75 , 400 ) axs [ 1 , 0 ] . set_ylabel ( 'Frequency density' ) # posterior predictive plot axs [ 1 , 1 ] . plot ( bins_t , n_t ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_t [ i ]) if post_samples_t [ i ][ 4 ] > 0 : axs [ 1 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 1 , 1 ] . set_xlim ( 75 , 400 ) # m_bb peak # mean posterior plot axs [ 2 , 0 ] . plot ( bins_h , n_h ) res_y = polynom ( x , * means_h ) axs [ 2 , 0 ] . plot ( x , res_y ) axs [ 2 , 0 ] . set_xlim ( 10 , 300 ) axs [ 2 , 0 ] . set_ylabel ( 'Frequency density' ) axs [ 2 , 0 ] . set_xlabel ( 'Invariant mass GeV/c \\u00b2 ' ) # posterior predictive plot axs [ 2 , 1 ] . plot ( bins_h , n_h ) axs [ 2 , 1 ] . set_xlabel ( 'Invariant mass GeV/c \\u00b2 ' ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_h [ i ]) axs [ 2 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 2 , 1 ] . set_xlim ( 10 , 300 ) plt . show () Modelling with DNest4 In an attempt to improve on the previous model, I came up with a modification on the Crystal Ball Function . To evaluate whether or not this new model explains the data better than the previous model, I'll use a nested sampler \"Dnest4\" , so that I can calculate the Bayes' factor of the models. DNest4 is a sampler that uses diffuse nested sampling , which is a variation on typical nested sampling that works well when variables have strong dependancies on each other. The model The new model is a little more complex than the previous model, but has the same number of parameters. The function is made of three pieces: an exponential ascending limb, a Gaussian centre, and an exponential descending limb. The function is implemented as follows: def EGE ( masses , N , mu , sig , a_L , a_H , b ): \"\"\" EGE - Exp-Gaussian-Exp Function to describe an invariant mass frequency peak :param N: amplitude of the peak :param mu: mean of Gaussian core :param sig: standard deviation of Gaussian core :param a_L: decay constant of lower limb :param a_R: decay constant of higher limb :param b: lowest frequency limit \"\"\" freq = [] for m in masses : step = ( m - mu ) / sig if step <= - a_L : # lower exponential limb freq . append ( N * np . exp ( 0.5 * ( a_L ** 2 ) + a_L * step ) + b ) elif step <= a_H : # Gaussian core freq . append ( N * np . exp ( - 0.5 * ( step ** 2 )) + b ) else : # higher exponential limb freq . append ( N * np . exp ( 0.5 * ( a_H ** 2 ) - a_H * step ) + b ) return freq I'll attempt to fit both models to the m_{bb} data, since the exponential model doesn't suit the convex ascending limb on the m_{Wb} peak. Below is a plot of the m_{bb} with an overplotted example EGE model, with guesses for the parameters: Sampling the data First, I'll create a list containing the values stored in the \"priors_h\" dictionary from the PyStan example. This will be useful when defining functions for the sampler. class_priors = [( priors_t [ 'h_mu' ], priors_t [ 'h_sig' ]),( priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ]), ( priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ]),( priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ]), ( priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ]),( priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ])] DNest4 requires the user to set up a class containing the prior, likelihood, and evolution methods. The following shows the class I used for the \"polynom\" model: class DNest4Model ( object ): \"\"\" Class for usage with DNest4 for fitting \"polynom\" model to an invariant mass peak \"\"\" def __init__ ( self ): # DNest4 doesn't require the class to have any \"self\" attributes pass def from_prior ( self ): \"\"\" Sample each parameter from prior distributions \"\"\" h = np . random . normal ( priors_t [ 'h_mu' ], priors_t [ 'h_sig' ]) m_in = np . random . normal ( priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ]) m_max = np . random . normal ( priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ]) m_fin = np . random . normal ( priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ]) t_1 = np . random . normal ( priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ]) t_2 = np . random . normal ( priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ]) return np . array ([ h , m_in , m_max , m_fin , t_1 , t_2 ]) def perturb ( self , theta ): \"\"\" Peturbs a parameter to a new proposal parameter This is done in place, and to only one parameter at a time \"\"\" # log of the Metropolis-Hastings ratio logH = 0.0 # choose random parameter p = np . random . randint ( len ( theta )) # update logH for chosen parameter logH -= - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 # scale parameter theta [ p ] += 1. * randh () # update logH to original value logH += - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 return logH def log_likelihood ( self , theta ): \"\"\" Poisson likelihood \"\"\" # frequency and invariant mass lists y = data_dnest4 [ 'n_t' ] x = data_dnest4 [ 'bins_t' ] # expected value lmbda = np . array ( polynom ( x , * theta )) # terms in a poisson likelihood n = len ( y ) a = np . sum ( gammaln ( np . array ( y ) + 1 )) b = np . sum ( np . array ( y ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b After the class of the model has been defined, we can now set up the hyper-parameters for the sampling process. DNest4 is slightly different to popular nested samplers in that it uses a maximum number of levels and steps, rather than a stopping criterion. These can all be set when creating the sampler object: # create sampler object using model class model = DNest4Model () sampler_1 = dnest4 . DNest4Sampler ( model , backend = dnest4 . backends . CSVBackend ( \".\" , sep = \" \" )) # pass hyper-parameters to gen_1 = sampler_1 . sample ( max_num_levels = 30 , num_steps = 1000 , new_level_interval = 10000 , num_per_step = 10000 , thread_steps = 100 , num_particles = 5 , lam = 10 , beta = 100 ) # perform the sampling process time0 = time () for sample in enumerate ( gen_1 ): pass time1 = time () print ( \"Time taken to sample with DNest4 and the polynom model: {} seconds \\n \" . format ( time1 - time0 )) logZ_pol , info_1 , _ = dnest4 . postprocess ( plot = False ) Time taken to sample with DNest4 and the polynom model: 2415.3539073467255 seconds log(Z) = -834.5583551721998 Information = 9.38512971091302 nats. Effective sample size = 248.86838013413347 Next, we'll run through the same process with the new \"EGE\" model. We need to come up with some new parameter priors for the model. I'll be using normal priors for the amplitude, mean, and baseline parameters. The function is very sensitive to small changes in the other parameters, so I'll use uniform priors for them: priors_h = {} priors_h [ 'N_mu' ], priors_h [ 'N_sig' ] = 120 , 10 priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ] = 120 , 5 priors_h [ 'sig_min' ], priors_h [ 'sig_max' ] = 1 , 10 priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ] = 0.01 , 0.04 priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ] = 0.01 , 0.05 priors_h [ 'b_mu' ], priors_h [ 'b_sig' ] = 2 , 1 class_priors = [( priors_h [ 'N_mu' ], priors_h [ 'N_sig' ]),( priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ]), ( priors_h [ 'sig_min' ], priors_h [ 'sig_max' ]),( priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ]), ( priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ]),( priors_h [ 'b_mu' ], priors_h [ 'b_sig' ])] The modelling process is identical to the \"polynom\" example shown above. The only differences are in the class methods. The following shows the new class for the \"EGE\" model: class DNest4Model ( object ): \"\"\" Class for usage with DNest4 for fitting \"EGE\" model to an invariant mass peak \"\"\" def __init__ ( self ): # DNest4 doesn't require the class to have any \"self\" attributes pass def from_prior ( self ): \"\"\" Sample each parameter from prior distributions \"\"\" N = np . random . normal ( priors_h [ 'N_mu' ], priors_h [ 'N_sig' ]) mu = np . random . normal ( priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ]) sig = np . random . uniform ( priors_h [ 'sig_min' ], priors_h [ 'sig_max' ]) a_L = np . random . uniform ( priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ]) a_H = np . random . uniform ( priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ]) b = np . random . normal ( priors_h [ 'b_mu' ], priors_h [ 'b_sig' ]) return np . array ([ N , mu , sig , a_L , a_H , b ]) def perturb ( self , theta ): \"\"\" Peturbs a parameter to a new proposal parameter This is done in place, and to only one parameter at a time \"\"\" # log of the Metropolis-Hastings ratio logH = 0.0 # choose random parameter p = np . random . randint ( len ( theta )) # update logH for chosen parameter (normal priors only) if p in [ 0 , 1 , 5 ]: logH -= - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 # scale parameter theta [ p ] += 1. * randh () # update logH to original value if p in [ 0 , 1 , 5 ]: # update H for normal priors logH += - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 else : # wrap uniform priors to their prior range theta [ p ] = wrap ( theta [ p ], class_priors [ p ][ 0 ], class_priors [ p ][ 1 ]) return logH def log_likelihood ( self , theta ): \"\"\" Poisson likelihood \"\"\" # frequency and invariant mass lists y = data_dnest4 [ 'n_h' ] x = data_dnest4 [ 'bins_h' ] # expected value lmbda = np . array ( EGE ( x , * theta )) # terms in a poisson likelihood n = len ( y ) a = np . sum ( gammaln ( np . array ( y ) + 1 )) b = np . sum ( np . array ( y ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Now sampling using the same code as before, we can fit the \"EGE\" model to the data. Before this, it's important to note that DNest4 returns posterior samples in the current directory by saving \".txt\" files. These files will be overwritten if they're not renamed or moved to a different directory, so make sure to do either of those things before running this next step: # create sampler object using model class model = DNest4Model () sampler_2 = dnest4 . DNest4Sampler ( model , backend = dnest4 . backends . CSVBackend ( \".\" , sep = \" \" )) # pass hyper-parameters to gen_2 = sampler_2 . sample ( max_num_levels = 30 , num_steps = 1000 , new_level_interval = 10000 , num_per_step = 10000 , thread_steps = 100 , num_particles = 5 , lam = 10 , beta = 100 ) # perform the sampling process time0 = time () for sample in enumerate ( gen_2 ): pass time1 = time () print ( \"Time taken to sample with DNest4 and the EGE model: {} seconds \\n \" . format ( time1 - time0 )) logZ_EGE , info_2 , _ = dnest4 . postprocess ( plot = False ) Time taken to run 'DNest4' is 5569.005742549896 seconds log(Z) = -838.6092109374224 Information = 13.492648493564161 nats. Effective sample size = 247.0799673512741 Results The results of both sampling processes have been saved to \".txt\" files. These can be read using numpy's \"loadtxt\" function: samples_pol = np . loadtxt ( 'simplemodel/posterior_sample.txt' ) samples_EGE = np . loadtxt ( 'newmodel/posterior_sample.txt' ) We can check which model predicts the invariant mass of the Higgs boson to the highest accuracy: means_pol , errs_pol = [], [] means_EGE , errs_EGE = [], [] for i in range ( 6 ): # means and errors of polynom model means_pol . append ( np . mean ( samples_pol [:, i ])) errs_pol . append ( np . std ( samples_pol [:, i ])) # means and errors of EGE model means_EGE . append ( np . mean ( samples_EGE [:, i ])) errs_EGE . append ( np . std ( samples_EGE [:, i ])) print ( 'Mean invariant masses of $m_ {bb} $ distribution: \\n \\n ' + ' EGE model: {} \\u00b1 {} GeV/c \\u00b2 \\n ' . format ( means_EGE [ 1 ], errs_EGE [ 1 ]) + ' polynom model: {} \\u00b1 {} GeV/c \\u00b2 \\n \\n ' . format ( means_pol [ 2 ], errs_pol [ 2 ]) + 'Mass of the Higgs boson: {} GeV/c \\u00b2 ' . format ( 125 )) Mean invariant masses of $m_{bb}$ distribution: EGE model: 121.01404746778172 \u00b1 0.815657973799681 GeV/c\u00b2 polynom model: 120.37903225806451 \u00b1 0.8288423907093776 GeV/c\u00b2 Mass of the Higgs boson: 125 GeV/c\u00b2 The EGE model is slightly better at predicting the Higgs boson mass, however both have simillar errors on their estimates. Plotting the posterior Using the \"plotposts\" function from before, we can create a corner plot for the EGE model. labels = [ 'N' , 'mu' , 'sig' , 'a_L' , 'a_H' , 'b' ] plotposts ( samples_EGE , labels ) This plot shows strong correlations between the exponential growth/decay parameters and the standard deviation. Next I'll use the posterior samples to plot the posteriors over the data. Below are two plots: the first shows the mean posterior plots of both the \"EGE\" and \"polynom\" models overplotted onto the m_{bb} data. The second plot shows the posterior predictive plots of the \"polynom\" and \"EGE\" models. nfits = 150 # collect random sets of parameters of polynom model param_samples_pol = [ np . random . choice ( samples_res [:, i ], nfits ) for i in range ( 6 )] post_samples_pol = np . array ( param_samples_pol ) . T # collect random sets of parameters of EGE model param_samples_EGE = [ np . random . choice ( samples_EGE [:, i ], nfits ) for i in range ( 6 )] post_samples_EGE = np . array ( param_samples_EGE ) . T # begin plotting fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 4 )) x = np . linspace ( 0 , 300 , 1000 ) # mean posterior plots ax1 . plot ( bins_h , n_h , 'k:' , label = 'Original data' ) y_pol = polynom ( x , * means_pol ) y_EGE = EGE ( x , * means_EGE ) ax1 . plot ( x , y_pol , 'orange' , label = 'polynom model' ) ax1 . plot ( x , y_EGE , 'green' , label = 'EGE model' ) ax1 . set_title ( '$m_ {bb} $ invariant mass peak with overplotted \\n mean posterior plots' ) ax1 . set_ylabel ( 'Frequency density' ) ax1 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) # polynom and EGE model posterior predictive plots ax2 . plot ( bins_h , n_h , 'k:' ) for i in range ( nfits ): y_pol = polynom ( x , * post_samples_pol [ i ]) y_EGE = EGE ( x , * post_samples_EGE [ i ]) ax2 . plot ( x , y_pol , 'orange' , alpha = 0.01 , linewidth = 3 ) ax2 . plot ( x , y_EGE , 'green' , alpha = 0.01 , linewidth = 3 ) ax2 . set_title ( '$m_ {bb} $ invariant mass peak with overplotted \\n posterior predictive plots' ) ax2 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) plt . show () We can see there is a little disagreement in the heights of the peak in both models, and the posterior predictive plot shows that the \"EGE\" model has a little more deviation than the \"polynom\" model. Model comparisons Since we previously obtained values of the marginal error back when we ran the sampler, we can calculate the Bayes' factor of the two models. This tells us which model is most likely to explain the observed data, and can be found by running the following: K = np . exp ( logZ_pol - logZ_EGE ) print ( 'Bayes Factor: {} ' . format ( K )) Bayes Factor: 57.44659681610874 A Bayes' factor of 1.0 would suggest that both models are equally likely to explain the observed data. However, this result is in strong favour of the \"polynom\" function, meaning it is much more likely to explain the observed data than the exponential model. Typically, a Bayes factor over 30 is considered a strong preference of models, and over 100 is a decisive preference of models.","title":"Particle Identification in Proton-Proton Collisions with PyStan"},{"location":"particleID/particleID/#using-pystan-and-dnest4-to-identify-particles-produced-in-proton-proton-collisions","text":"The Large Hadron Collider (LHC) is a particle accelerator that is capable of producing proton beams at near light-speed. Two of these beams are targeted at each other, so that very high-energy protons collide head-on. This collision creates many new particles, most of which are known hadrons or mesons. However, some of the created particles are heavy bosons such as the W boson, and the Higgs boson. These bosons have a very short lifetime, limited by the uncertainty principle , and so they decay long before they can reach the particle detectors. The only way to identify the existence of these particles is to infer their presence from their decay products. In the following example, I'll use \"PyStan\" and \"DNest4\" to identify particles created in proton-proton collisions at LHC.","title":"Using PyStan and DNest4 to identify particles produced in proton-proton collisions"},{"location":"particleID/particleID/#useful-imports","text":"# numpy import numpy as np # pandas import pandas as pd # scipy from scipy.special import ndtri from scipy.stats import gaussian_kde # plotting import matplotlib.pyplot as plt import corner # sampler import pystan from dnest4 import randh , wrap import dnest4 print ( \"PyStan version: {} \" . format ( pystan . __version__ )) print ( \"DNest4 version: {} \" . format ( dnest4 . __version__ )) # misc from time import time PyStan version: 2.19.1.1 DNest4 version: 0.2.4","title":"Useful imports"},{"location":"particleID/particleID/#viewing-the-data","text":"First, I'll use pandas to load the data. The data set is made up of 1.1 million collisions, each with 28 different attributes. The first 21 attributes are \"low-level\", and are only really useful for calculating the last 7 attributes. The last 7 attributes describe the invariant mass for different processes. I'll explain further what these processes are later on, but for now I'll start by loading in the data. Since the data set is so large at 1.1 million data points, I decided to randomly sample 10000 of them, just to save some processing time. # load data df = pd . read_csv ( 'HIGGS.csv' , header = None ) # select only last 7 attributes df_reduce = df [ df . columns [ - 7 :]] # randomly choose 10000 data points nrows = 10000 data = df_reduce . sample ( nrows , random_state = 2020 ) For a particle A, which decays via the process A \\rightarrow B + C into particles B and C, the invariant mass squared m_A^2 = m_{B+C}^2 = (E_B + E_C)^2 - |(\\vec{p}_B + \\vec{p}_C)|^2 is a Lorentz invariant quantity, meaning it is the same in all reference frames. This means that in the zero-momentum frame of reference, the invariant mass is just given by the mass of the particle A, m_A . The data set is made up of 10000 collisions, in any of which particle A may have been produced. Particle A's subsequent decay results in a a peak around m_A , and therefore by finding the centre of the peak the invariant mass of the particle can be found. Mass is typically used to identify particles since a particle's mass is unique. There are three attributes in particular that are of interest for us. The first is the invariant mass of the process W \\rightarrow l \\nu that describes a W boson decay into a lepton neutrino pair. The peak invariant mass m_{l\\nu} of this distribution should be at the W boson mass m_W . The second attribute is the invariant mass for the process t \\rightarrow W b that describes a top quark decaying into a W boson and a bottom quark. The invariant mass m_{Wb} should peak at the mass of the top quark m_t . Finally, the third attribute is the invariant mass for the process h_0 \\rightarrow b \\bar{b} of a Higgs decay into a bottom-antibottom meson. The invariant mass m_{b\\bar{b}} should peak at the mass of the Higgs boson m_{h_0} . Since we have the three invariant mass distributions, we can use them to measure the masses of the W boson, top quark, and Higgs boson. This is done by plotting a histogram of the invariant mass distributions, since the y-axis gives an idea of the relative frequency of each collision having a certain invariant mass: hist_w = np . array ( data [ 22 ]) # invariant mass for m_lv hist_t = np . array ( data [ 25 ]) # invariant mass for m_Wb hist_h = np . array ( data [ 26 ]) # invariant mass for m_bb fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , 1 , figsize = ( 10 , 12 )) bins = 1000 # histogram predicting W boson mass n_w , bins_w , patches_w = ax1 . hist ( hist_w , bins ) # histogram predicting top quark mass n_t , bins_t , patches_t = ax2 . hist ( hist_t , bins ) # histogram predicting Higgs boson mass n_h , bins_h , patches_h = ax3 . hist ( hist_h , bins ) ax1 . set_ylabel ( 'Frequency density' ) ax1 . set_title ( 'Invariant mass $m_ {lv} $ of W decay to lepton neutrino pair' ) ax2 . set_ylabel ( 'Frequency density' ) ax2 . set_title ( 'Invariant mass $m_ {Wb} $ of top quark decay to W bottom pair' ) ax3 . set_ylabel ( 'Frequency density' ) ax3 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) ax3 . set_title ( 'Invariant mass $m_ {bb} $ of Higgs decay to bottom antibottom pair' ) plt . show ()","title":"Viewing the data"},{"location":"particleID/particleID/#the-model","text":"The peaks reach a sharp peak, after which they fall off with what looks like a 1/m^\\alpha relationship. To model this, I used a quadratic ascending limb, and a reciprocal quadratic decending limb. The model is implemented in Python with the following parameters: def polynom ( masses , h , m_in , m_max , m_fin , t_1 , t_2 ): \"\"\" Function to model a particle production frequency as a function of invariant mass :param masses: array of invariant masses :param h: amplitude of the peak :param m_in: initial invariant mass, at which the peak begins :param m_max: invariant mass with coresponding maximum frequency :param m_fin: final invariant mass, at which the peak reaches background level :param t_1: describes the ascending limb growth rate :param t_2: describes the descending limb decay rate :return: array of relative frequencies of invariant masses \"\"\" masses = np . array ( masses ) y = [ 0 if m < m_in else h * (( m - m_in ) / ( m_max - m_in )) ** t_1 if m < m_max else h * (( m - m_max ) / ( m_fin - m_max ) + 1 ) ** ( - t_2 ) if m < m_fin else 0 for m in masses ] return np . array ( y ) To show what this model looks like, we can make some quick guesses of the parameters, just by looking at the peak in the m_{l\\nu} distribution: x = np . linspace ( 0 , 400 , 1400 ) # guess at each parameter guess_y = polynom ( x , 380 , - 20 , 80 , 240 , 6 , 14 ) plt . plot ( bins_w , n_w , label = 'Original data' ) plt . plot ( x , guess_y , label = 'Example model' ) plt . xlim ( 0 , 270 ) plt . title ( 'Example peak model overplotted on $m_ {lv} $ data' ) plt . xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) plt . ylabel ( 'Frequency density' ) plt . legend () plt . show () The model looks like it sufficiently describes the frequency peak. The parameter that we are primarily interested in is m_max, since it predicts the most frequent invariant mass in this range; which for this example should be equal to the mass of the W boson.","title":"The model"},{"location":"particleID/particleID/#modelling-with-pystan","text":"PyStan is a Python wrapper for a C++ based sampler \"Stan\". To use PyStan, we first have to write a code string in C++. This code string is split into 5 blocks: functions (optional), data, parameters, transformed parameters (optional), and model. The functions block contains user-defined functions, such as the model function defined above. The data block initialises variables such as the number of data points, the invariant masses, and their respective frequency densities. The parameters block initialises the parameters of the model. The transformed parameters block gives the expected value of the model, given the current parameters. Finally, the model block sets the priors and likelihood distrubutions. pystan_code = \"\"\" functions {{ real polynom(real x, real h, real m_in, real m_max, real m_fin, real t_1, real t_2) if (x < m_in) return 0; else if (x < m_max && t_1 > 0) return h*pow( (x-m_in)/(m_max-m_in), t_1 ); else if (x < m_max && t_1 < 0) return h*pow( (x-m_in)/(m_max-m_in), 0.1 ); else if (x < m_fin) return h*pow( (x-m_max)/(m_fin-m_max) + 1, -t_2 ); else return 0; }} data {{ int<lower=0> N; // number of data points real y[N]; // frequency densities real m[N]; // invariant masses }} parameters {{ // parameters of the model real h; real m_in; real m_max; real m_fin; real<lower=0> t_1; real<lower=0> t_2; real sigma; }} transformed parameters {{ real theta[N]; // for each invariant mass m[j], expected y-value is found for (j in 1:N) theta[j] = polynom(m[j],h,m_in,m_max,m_fin,t_1,t_2); }} model {{ // normal priors h ~ normal( {h_mu} , {h_sig} ); m_in ~ normal( {m_in_mu} , {m_in_sig} ); m_max ~ normal( {m_max_mu} , {m_max_sig} ); m_fin ~ normal( {m_fin_mu} , {m_fin_sig} ); t_1 ~ normal( {t_1_mu} , {t_1_sig} ); t_2 ~ normal( {t_2_mu} , {t_2_sig} ); // uniform parameters sigma ~ uniform(-2, 2); // normal likelihood y ~ normal(theta,pow(10,sigma)); }} \"\"\" In the above code string, there is an extra parameter \"sigma\". This parameter is used to model the noise of the data. Since this values is difficult to guess, the value of sigma is set to be uniform in log-space. This means that noise standard deviation has an equal probabillity of being between an order of magnitude of -2 and 2. This is used as an alternate to using a Poisson distribution. A comparison example between this likelihood and a Poisson likelihood can be found on this site here . Next, we need to pass information to the code string. This is done using a dictionary that contains the number of data points, frequency density, and invariant mass data points, and another dictionary that contains all the prior bounds for each parameter. I'll start using the m_{lv} distribution: data_w = { 'N' : len ( n_w ), 'y' : n_w , 'm' : bins_w } priors_w = {} priors_w [ 'h_mu' ], priors_w [ 'h_sig' ] = 380 , 10 priors_w [ 'm_in_mu' ], priors_w [ 'm_in_sig' ] = - 10 , 5 priors_w [ 'm_max_mu' ], priors_w [ 'm_max_sig' ] = 80 , 5 priors_w [ 'm_fin_mu' ], priors_w [ 'm_fin_sig' ] = 260 , 10 priors_w [ 't_1_mu' ], priors_w [ 't_1_sig' ] = 5 , 2 priors_w [ 't_2_mu' ], priors_w [ 't_2_sig' ] = 16 , 2","title":"Modelling with PyStan"},{"location":"particleID/particleID/#sampling-the-data","text":"The code string must now be compiled using the priors dictionary defined above. time0 = time () model_w = pystan . StanModel ( model_code = pystan_code . format ( ** priors_w )) time1 = time () print ( \"Time taken to compile model: {} seconds\" . format ( time1 - time0 )) INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_32a27c4325414afa7499109a8951a53e NOW. Time taken to compile model: 44.89909029006958 seconds The sampler is now ready to iterate through the data. I'll do this using 1000 samples and 1 chain: Nsamples = 1000 chains = 1 time0 = time () fit_w = model_w . sampling ( data = data_w , iter = Nsamples , chains = chains ) time1 = time () print ( \"Time taken to sample m_lv peak: \" + str ( time1 - time0 ) + \" seconds\" ) Time taken to sample m_lv peak: 1.9440805912017822 seconds The results can be extracted from the sampler using a dictionary as follows: la_w = fit_w . extract ( permuted = True ) # posterior samples samples_w = np . vstack (( la_w [ 'h' ], la_w [ 'm_in' ], la_w [ 'm_max' ], la_w [ 'm_fin' ], la_w [ 't_1' ], la_w [ 't_2' ])) . T Now that we understand how the sampling process works, we can quickly do the exact same process for the other two peaks. First, make guesses for parameter values for each data set: # data for data_t = { 'N' : len ( n_t ), 'y' : n_t , 'm' : bins_t } data_h = { 'N' : len ( n_h ), 'y' : n_h , 'm' : bins_h } priors_t = {} priors_t [ 'h_mu' ], priors_t [ 'h_sig' ] = 110 , 5 priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ] = 90 , 5 priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ] = 170 , 2 priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ] = 400 , 10 priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ] = 0.7 , 0.1 priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ] = 5 , 1 priors_h = {} priors_h [ 'h_mu' ], priors_h [ 'h_sig' ] = 120 , 10 priors_h [ 'm_in_mu' ], priors_h [ 'm_in_sig' ] = 0 , 5 priors_h [ 'm_max_mu' ], priors_h [ 'm_max_sig' ] = 120 , 5 priors_h [ 'm_fin_mu' ], priors_h [ 'm_fin_sig' ] = 350 , 10 priors_h [ 't_1_mu' ], priors_h [ 't_1_sig' ] = 2 , 1 priors_h [ 't_2_mu' ], priors_h [ 't_2_sig' ] = 6 , 1 Now we can sample both peaks in tandem in the exact way as before: model_t = pystan . StanModel ( model_code = pystan_code . format ( ** priors_t )) model_h = pystan . StanModel ( model_code = pystan_code . format ( ** priors_h )) time0 = time () fit_t = model_w . sampling ( data = data_t , iter = Nsamples , chains = chains ) time1 = time () fit_h = model_w . sampling ( data = data_h , iter = Nsamples , chains = chains ) time2 = time () print ( \"Time taken to sample m_Wb peak: \" + str ( time1 - time0 ) + \" seconds\" ) print ( \"Time taken to sample m_bb} peak: \" + str ( time2 - time1 ) + \" seconds\" ) INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_c8d0398ecec8d290f88238a962b61a3e NOW. INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_fc7e337091261cd27b6faca01371228a NOW. Time taken to sample m_Wb peak: 0.8660006523132324 seconds Time taken to sample m_bb peak: 1.7904999256134032 seconds Finally, the results can be collected again: la_t = fit_t . extract ( permuted = True ) la_h = fit_h . extract ( permuted = True ) # posterior samples samples_t = np . vstack (( la_t [ 'h' ], la_t [ 'm_in' ], la_t [ 'm_max' ], la_t [ 'm_fin' ], la_t [ 't_1' ], la_t [ 't_2' ])) . T samples_h = np . vstack (( la_h [ 'h' ], la_h [ 'm_in' ], la_h [ 'm_max' ], la_h [ 'm_fin' ], la_h [ 't_1' ], la_h [ 't_2' ])) . T","title":"Sampling the data"},{"location":"particleID/particleID/#results","text":"Now that all of the peaks have been sampled, we can check out the posterior samples returned by PyStan. We'll start by plotting a corner plot for the m_{l\\nu} model, which shows the posterior distributions for each parameter, along with a contour plot describing how a parameter may vary with any other. This is done using \"corner.py\" and a Gaussian KDE function from scipy: def plotposts ( samples , labels , ** kwargs ): fig = corner . corner ( samples , labels = labels , hist_kwargs = { 'density' : True }, ** kwargs ) pos = [ i * ( len ( labels ) + 1 ) for i in range ( len ( labels ))] for axidx , samps in zip ( pos , samples . T ): kde = gaussian_kde ( samps ) xvals = fig . axes [ axidx ] . get_xlim () xvals = np . linspace ( xvals [ 0 ], xvals [ 1 ], 50 ) fig . axes [ axidx ] . plot ( xvals , kde ( xvals ), color = 'firebrick' ) labels = [ 'h' , 'm_in' , 'm_max' , 'm_fin' , 't_1' , 't_2' ] plotposts ( samples_w , labels ) We're interested in only the value of m_max for each model, since it describes the mass of the particle that created the peak. We can collect the means and standard deviation errors for each parameter, along with randomly sampling from the posteriors so that we can create a posterior predictive plot. # mean and error of parameters for m_lv distribution means_w = [ np . mean ( samples_w [:, i ]) for i in range ( 6 )] errors_w = [ np . std ( samples_w [:, i ]) for i in range ( 6 )] # mean and error of parameters for m_Wb distribution means_t = [ np . mean ( samples_t [:, i ]) for i in range ( 6 )] errors_t = [ np . std ( samples_t [:, i ]) for i in range ( 6 )] # mean and error of parameters for m_bb distribution means_h = [ np . mean ( samples_h [:, i ]) for i in range ( 6 )] errors_h = [ np . std ( samples_h [:, i ]) for i in range ( 6 )] # number of random samples from posteriors nfits = 300 # sample parameters for m_lv distribution param_samples_w = [ np . random . choice ( samples_w [:, i ], nfits ) for i in range ( 6 )] post_samples_w = np . array ( param_samples_w ) . T # sample parameters for m_Wb distribution param_samples_t = [ np . random . choice ( samples_t [:, i ], nfits ) for i in range ( 6 )] post_samples_t = np . array ( param_samples_t ) . T # sample parameters for m_bb distribution param_samples_h = [ np . random . choice ( samples_h [:, i ], nfits ) for i in range ( 6 )] post_samples_h = np . array ( param_samples_h ) . T print ( \"Mean Invariant Mass of m_lv peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_w [ 2 ], errors_w [ 2 ]) + \"Mass of a W boson: 80.4 GeV/c \\u00b2 \\n \\n \" + \"Mean Invariant Mass of m_Wb peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_t [ 2 ], errors_t [ 2 ]) + \"Mass of top quark: 173 GeV/c \\u00b2 \\n \\n \" + \"Mean Invariant Mass of m_bb peak: {} \\u00b1 {} GeV/c \\u00b2 \\n \" . format ( means_h [ 2 ], errors_h [ 2 ]) + \"Mass of Higgs boson: 125 GeV/c \\u00b2 \\n \\n \" ) Mean Invariant Mass of m_lv peak: 78.8824875109673 \u00b1 0.14962446128127846 GeV/c\u00b2 Mass of a W boson: 80.4 GeV/c\u00b2 Mean Invariant Mass of m_Wb peak: 170.739515663670765 \u00b1 2.9494226571197104 GeV/c\u00b2 Mass of top quark: 173 GeV/c\u00b2 Mean Invariant Mass of m_bb peak: 121.08971310473981 \u00b1 0.7304177489930841 GeV/c\u00b2 Mass of Higgs boson: 125 GeV/c\u00b2 The values for m_max make good estimates for the masses of the particles that caused the peaks. The predictions are systematically slightly lower than the true masses, which is likely due to the model I used being only a simplified approximation, instead of a more accurate model. The increased error on the estimate for the mass of the top quark is likely due to a much higher level of noise in the data.","title":"Results"},{"location":"particleID/particleID/#plotting-the-posterior","text":"We can plot the posterior distributions for each peak to see the quality of the fit produced by the sampler. Below are plots for both the mean posterior (left) and the posterior predictive (right). x = np . linspace ( 0 , 1400 , 1400 ) fig , axs = plt . subplots ( 3 , 2 , figsize = ( 10 , 12 )) # m_lv peak # mean posterior plot axs [ 0 , 0 ] . plot ( bins_w , n_w , label = 'Original data' ) res_y = polynom ( x , * means_w ) axs [ 0 , 0 ] . plot ( x , res_y , label = 'Fitted model' ) axs [ 0 , 0 ] . set_xlim ( 0 , 200 ) axs [ 0 , 0 ] . set_ylabel ( 'Frequency density' ) axs [ 0 , 0 ] . set_title ( 'Mean posterior plots' ) axs [ 0 , 0 ] . legend ( loc = 'upper right' ) #plot posterior predictive plot axs [ 0 , 1 ] . plot ( bins_w , n_w ) axs [ 0 , 1 ] . set_title ( 'Posterior predictive plots' ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_w [ i ]) axs [ 0 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 0 , 1 ] . set_xlim ( 0 , 200 ) # m_Wb peak # mean posterior plot axs [ 1 , 0 ] . plot ( bins_t , n_t ) res_y = polynom ( x , * means_t ) axs [ 1 , 0 ] . plot ( x , res_y ) axs [ 1 , 0 ] . set_xlim ( 75 , 400 ) axs [ 1 , 0 ] . set_ylabel ( 'Frequency density' ) # posterior predictive plot axs [ 1 , 1 ] . plot ( bins_t , n_t ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_t [ i ]) if post_samples_t [ i ][ 4 ] > 0 : axs [ 1 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 1 , 1 ] . set_xlim ( 75 , 400 ) # m_bb peak # mean posterior plot axs [ 2 , 0 ] . plot ( bins_h , n_h ) res_y = polynom ( x , * means_h ) axs [ 2 , 0 ] . plot ( x , res_y ) axs [ 2 , 0 ] . set_xlim ( 10 , 300 ) axs [ 2 , 0 ] . set_ylabel ( 'Frequency density' ) axs [ 2 , 0 ] . set_xlabel ( 'Invariant mass GeV/c \\u00b2 ' ) # posterior predictive plot axs [ 2 , 1 ] . plot ( bins_h , n_h ) axs [ 2 , 1 ] . set_xlabel ( 'Invariant mass GeV/c \\u00b2 ' ) for i in range ( nfits ): res_y = polynom ( x , * post_samples_h [ i ]) axs [ 2 , 1 ] . plot ( x , res_y , 'orange' , alpha = 0.02 ) axs [ 2 , 1 ] . set_xlim ( 10 , 300 ) plt . show ()","title":"Plotting the posterior"},{"location":"particleID/particleID/#modelling-with-dnest4","text":"In an attempt to improve on the previous model, I came up with a modification on the Crystal Ball Function . To evaluate whether or not this new model explains the data better than the previous model, I'll use a nested sampler \"Dnest4\" , so that I can calculate the Bayes' factor of the models. DNest4 is a sampler that uses diffuse nested sampling , which is a variation on typical nested sampling that works well when variables have strong dependancies on each other.","title":"Modelling with DNest4"},{"location":"particleID/particleID/#the-model_1","text":"The new model is a little more complex than the previous model, but has the same number of parameters. The function is made of three pieces: an exponential ascending limb, a Gaussian centre, and an exponential descending limb. The function is implemented as follows: def EGE ( masses , N , mu , sig , a_L , a_H , b ): \"\"\" EGE - Exp-Gaussian-Exp Function to describe an invariant mass frequency peak :param N: amplitude of the peak :param mu: mean of Gaussian core :param sig: standard deviation of Gaussian core :param a_L: decay constant of lower limb :param a_R: decay constant of higher limb :param b: lowest frequency limit \"\"\" freq = [] for m in masses : step = ( m - mu ) / sig if step <= - a_L : # lower exponential limb freq . append ( N * np . exp ( 0.5 * ( a_L ** 2 ) + a_L * step ) + b ) elif step <= a_H : # Gaussian core freq . append ( N * np . exp ( - 0.5 * ( step ** 2 )) + b ) else : # higher exponential limb freq . append ( N * np . exp ( 0.5 * ( a_H ** 2 ) - a_H * step ) + b ) return freq I'll attempt to fit both models to the m_{bb} data, since the exponential model doesn't suit the convex ascending limb on the m_{Wb} peak. Below is a plot of the m_{bb} with an overplotted example EGE model, with guesses for the parameters:","title":"The model"},{"location":"particleID/particleID/#sampling-the-data_1","text":"First, I'll create a list containing the values stored in the \"priors_h\" dictionary from the PyStan example. This will be useful when defining functions for the sampler. class_priors = [( priors_t [ 'h_mu' ], priors_t [ 'h_sig' ]),( priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ]), ( priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ]),( priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ]), ( priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ]),( priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ])] DNest4 requires the user to set up a class containing the prior, likelihood, and evolution methods. The following shows the class I used for the \"polynom\" model: class DNest4Model ( object ): \"\"\" Class for usage with DNest4 for fitting \"polynom\" model to an invariant mass peak \"\"\" def __init__ ( self ): # DNest4 doesn't require the class to have any \"self\" attributes pass def from_prior ( self ): \"\"\" Sample each parameter from prior distributions \"\"\" h = np . random . normal ( priors_t [ 'h_mu' ], priors_t [ 'h_sig' ]) m_in = np . random . normal ( priors_t [ 'm_in_mu' ], priors_t [ 'm_in_sig' ]) m_max = np . random . normal ( priors_t [ 'm_max_mu' ], priors_t [ 'm_max_sig' ]) m_fin = np . random . normal ( priors_t [ 'm_fin_mu' ], priors_t [ 'm_fin_sig' ]) t_1 = np . random . normal ( priors_t [ 't_1_mu' ], priors_t [ 't_1_sig' ]) t_2 = np . random . normal ( priors_t [ 't_2_mu' ], priors_t [ 't_2_sig' ]) return np . array ([ h , m_in , m_max , m_fin , t_1 , t_2 ]) def perturb ( self , theta ): \"\"\" Peturbs a parameter to a new proposal parameter This is done in place, and to only one parameter at a time \"\"\" # log of the Metropolis-Hastings ratio logH = 0.0 # choose random parameter p = np . random . randint ( len ( theta )) # update logH for chosen parameter logH -= - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 # scale parameter theta [ p ] += 1. * randh () # update logH to original value logH += - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 return logH def log_likelihood ( self , theta ): \"\"\" Poisson likelihood \"\"\" # frequency and invariant mass lists y = data_dnest4 [ 'n_t' ] x = data_dnest4 [ 'bins_t' ] # expected value lmbda = np . array ( polynom ( x , * theta )) # terms in a poisson likelihood n = len ( y ) a = np . sum ( gammaln ( np . array ( y ) + 1 )) b = np . sum ( np . array ( y ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b After the class of the model has been defined, we can now set up the hyper-parameters for the sampling process. DNest4 is slightly different to popular nested samplers in that it uses a maximum number of levels and steps, rather than a stopping criterion. These can all be set when creating the sampler object: # create sampler object using model class model = DNest4Model () sampler_1 = dnest4 . DNest4Sampler ( model , backend = dnest4 . backends . CSVBackend ( \".\" , sep = \" \" )) # pass hyper-parameters to gen_1 = sampler_1 . sample ( max_num_levels = 30 , num_steps = 1000 , new_level_interval = 10000 , num_per_step = 10000 , thread_steps = 100 , num_particles = 5 , lam = 10 , beta = 100 ) # perform the sampling process time0 = time () for sample in enumerate ( gen_1 ): pass time1 = time () print ( \"Time taken to sample with DNest4 and the polynom model: {} seconds \\n \" . format ( time1 - time0 )) logZ_pol , info_1 , _ = dnest4 . postprocess ( plot = False ) Time taken to sample with DNest4 and the polynom model: 2415.3539073467255 seconds log(Z) = -834.5583551721998 Information = 9.38512971091302 nats. Effective sample size = 248.86838013413347 Next, we'll run through the same process with the new \"EGE\" model. We need to come up with some new parameter priors for the model. I'll be using normal priors for the amplitude, mean, and baseline parameters. The function is very sensitive to small changes in the other parameters, so I'll use uniform priors for them: priors_h = {} priors_h [ 'N_mu' ], priors_h [ 'N_sig' ] = 120 , 10 priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ] = 120 , 5 priors_h [ 'sig_min' ], priors_h [ 'sig_max' ] = 1 , 10 priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ] = 0.01 , 0.04 priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ] = 0.01 , 0.05 priors_h [ 'b_mu' ], priors_h [ 'b_sig' ] = 2 , 1 class_priors = [( priors_h [ 'N_mu' ], priors_h [ 'N_sig' ]),( priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ]), ( priors_h [ 'sig_min' ], priors_h [ 'sig_max' ]),( priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ]), ( priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ]),( priors_h [ 'b_mu' ], priors_h [ 'b_sig' ])] The modelling process is identical to the \"polynom\" example shown above. The only differences are in the class methods. The following shows the new class for the \"EGE\" model: class DNest4Model ( object ): \"\"\" Class for usage with DNest4 for fitting \"EGE\" model to an invariant mass peak \"\"\" def __init__ ( self ): # DNest4 doesn't require the class to have any \"self\" attributes pass def from_prior ( self ): \"\"\" Sample each parameter from prior distributions \"\"\" N = np . random . normal ( priors_h [ 'N_mu' ], priors_h [ 'N_sig' ]) mu = np . random . normal ( priors_h [ 'mu_mu' ], priors_h [ 'mu_sig' ]) sig = np . random . uniform ( priors_h [ 'sig_min' ], priors_h [ 'sig_max' ]) a_L = np . random . uniform ( priors_h [ 'a_L_min' ], priors_h [ 'a_L_max' ]) a_H = np . random . uniform ( priors_h [ 'a_H_min' ], priors_h [ 'a_H_max' ]) b = np . random . normal ( priors_h [ 'b_mu' ], priors_h [ 'b_sig' ]) return np . array ([ N , mu , sig , a_L , a_H , b ]) def perturb ( self , theta ): \"\"\" Peturbs a parameter to a new proposal parameter This is done in place, and to only one parameter at a time \"\"\" # log of the Metropolis-Hastings ratio logH = 0.0 # choose random parameter p = np . random . randint ( len ( theta )) # update logH for chosen parameter (normal priors only) if p in [ 0 , 1 , 5 ]: logH -= - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 # scale parameter theta [ p ] += 1. * randh () # update logH to original value if p in [ 0 , 1 , 5 ]: # update H for normal priors logH += - 0.5 * (( theta [ p ] - class_priors [ p ][ 0 ]) / class_priors [ p ][ 1 ]) ** 2 else : # wrap uniform priors to their prior range theta [ p ] = wrap ( theta [ p ], class_priors [ p ][ 0 ], class_priors [ p ][ 1 ]) return logH def log_likelihood ( self , theta ): \"\"\" Poisson likelihood \"\"\" # frequency and invariant mass lists y = data_dnest4 [ 'n_h' ] x = data_dnest4 [ 'bins_h' ] # expected value lmbda = np . array ( EGE ( x , * theta )) # terms in a poisson likelihood n = len ( y ) a = np . sum ( gammaln ( np . array ( y ) + 1 )) b = np . sum ( np . array ( y ) * np . log ( lmbda )) return - np . sum ( lmbda ) - a + b Now sampling using the same code as before, we can fit the \"EGE\" model to the data. Before this, it's important to note that DNest4 returns posterior samples in the current directory by saving \".txt\" files. These files will be overwritten if they're not renamed or moved to a different directory, so make sure to do either of those things before running this next step: # create sampler object using model class model = DNest4Model () sampler_2 = dnest4 . DNest4Sampler ( model , backend = dnest4 . backends . CSVBackend ( \".\" , sep = \" \" )) # pass hyper-parameters to gen_2 = sampler_2 . sample ( max_num_levels = 30 , num_steps = 1000 , new_level_interval = 10000 , num_per_step = 10000 , thread_steps = 100 , num_particles = 5 , lam = 10 , beta = 100 ) # perform the sampling process time0 = time () for sample in enumerate ( gen_2 ): pass time1 = time () print ( \"Time taken to sample with DNest4 and the EGE model: {} seconds \\n \" . format ( time1 - time0 )) logZ_EGE , info_2 , _ = dnest4 . postprocess ( plot = False ) Time taken to run 'DNest4' is 5569.005742549896 seconds log(Z) = -838.6092109374224 Information = 13.492648493564161 nats. Effective sample size = 247.0799673512741","title":"Sampling the data"},{"location":"particleID/particleID/#results_1","text":"The results of both sampling processes have been saved to \".txt\" files. These can be read using numpy's \"loadtxt\" function: samples_pol = np . loadtxt ( 'simplemodel/posterior_sample.txt' ) samples_EGE = np . loadtxt ( 'newmodel/posterior_sample.txt' ) We can check which model predicts the invariant mass of the Higgs boson to the highest accuracy: means_pol , errs_pol = [], [] means_EGE , errs_EGE = [], [] for i in range ( 6 ): # means and errors of polynom model means_pol . append ( np . mean ( samples_pol [:, i ])) errs_pol . append ( np . std ( samples_pol [:, i ])) # means and errors of EGE model means_EGE . append ( np . mean ( samples_EGE [:, i ])) errs_EGE . append ( np . std ( samples_EGE [:, i ])) print ( 'Mean invariant masses of $m_ {bb} $ distribution: \\n \\n ' + ' EGE model: {} \\u00b1 {} GeV/c \\u00b2 \\n ' . format ( means_EGE [ 1 ], errs_EGE [ 1 ]) + ' polynom model: {} \\u00b1 {} GeV/c \\u00b2 \\n \\n ' . format ( means_pol [ 2 ], errs_pol [ 2 ]) + 'Mass of the Higgs boson: {} GeV/c \\u00b2 ' . format ( 125 )) Mean invariant masses of $m_{bb}$ distribution: EGE model: 121.01404746778172 \u00b1 0.815657973799681 GeV/c\u00b2 polynom model: 120.37903225806451 \u00b1 0.8288423907093776 GeV/c\u00b2 Mass of the Higgs boson: 125 GeV/c\u00b2 The EGE model is slightly better at predicting the Higgs boson mass, however both have simillar errors on their estimates.","title":"Results"},{"location":"particleID/particleID/#plotting-the-posterior_1","text":"Using the \"plotposts\" function from before, we can create a corner plot for the EGE model. labels = [ 'N' , 'mu' , 'sig' , 'a_L' , 'a_H' , 'b' ] plotposts ( samples_EGE , labels ) This plot shows strong correlations between the exponential growth/decay parameters and the standard deviation. Next I'll use the posterior samples to plot the posteriors over the data. Below are two plots: the first shows the mean posterior plots of both the \"EGE\" and \"polynom\" models overplotted onto the m_{bb} data. The second plot shows the posterior predictive plots of the \"polynom\" and \"EGE\" models. nfits = 150 # collect random sets of parameters of polynom model param_samples_pol = [ np . random . choice ( samples_res [:, i ], nfits ) for i in range ( 6 )] post_samples_pol = np . array ( param_samples_pol ) . T # collect random sets of parameters of EGE model param_samples_EGE = [ np . random . choice ( samples_EGE [:, i ], nfits ) for i in range ( 6 )] post_samples_EGE = np . array ( param_samples_EGE ) . T # begin plotting fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 13 , 4 )) x = np . linspace ( 0 , 300 , 1000 ) # mean posterior plots ax1 . plot ( bins_h , n_h , 'k:' , label = 'Original data' ) y_pol = polynom ( x , * means_pol ) y_EGE = EGE ( x , * means_EGE ) ax1 . plot ( x , y_pol , 'orange' , label = 'polynom model' ) ax1 . plot ( x , y_EGE , 'green' , label = 'EGE model' ) ax1 . set_title ( '$m_ {bb} $ invariant mass peak with overplotted \\n mean posterior plots' ) ax1 . set_ylabel ( 'Frequency density' ) ax1 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) # polynom and EGE model posterior predictive plots ax2 . plot ( bins_h , n_h , 'k:' ) for i in range ( nfits ): y_pol = polynom ( x , * post_samples_pol [ i ]) y_EGE = EGE ( x , * post_samples_EGE [ i ]) ax2 . plot ( x , y_pol , 'orange' , alpha = 0.01 , linewidth = 3 ) ax2 . plot ( x , y_EGE , 'green' , alpha = 0.01 , linewidth = 3 ) ax2 . set_title ( '$m_ {bb} $ invariant mass peak with overplotted \\n posterior predictive plots' ) ax2 . set_xlabel ( 'Invariant mass / GeV/c \\u00b2 ' ) plt . show () We can see there is a little disagreement in the heights of the peak in both models, and the posterior predictive plot shows that the \"EGE\" model has a little more deviation than the \"polynom\" model.","title":"Plotting the posterior"},{"location":"particleID/particleID/#model-comparisons","text":"Since we previously obtained values of the marginal error back when we ran the sampler, we can calculate the Bayes' factor of the two models. This tells us which model is most likely to explain the observed data, and can be found by running the following: K = np . exp ( logZ_pol - logZ_EGE ) print ( 'Bayes Factor: {} ' . format ( K )) Bayes Factor: 57.44659681610874 A Bayes' factor of 1.0 would suggest that both models are equally likely to explain the observed data. However, this result is in strong favour of the \"polynom\" function, meaning it is much more likely to explain the observed data than the exponential model. Typically, a Bayes factor over 30 is considered a strong preference of models, and over 100 is a decisive preference of models.","title":"Model comparisons"}]}