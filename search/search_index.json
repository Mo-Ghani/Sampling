{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Uses Of MCMC and Nested Sampling Algorithms There are various off-the-shelf samplers that make use MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used on real world, open source data sets. The Samplers PyMC3 dynesty emcee UltraNest The Data Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Gravitational wave signal","title":"Home"},{"location":"#the-uses-of-mcmc-and-nested-sampling-algorithms","text":"There are various off-the-shelf samplers that make use MCMC and nested sampling algorithms in Python, freely available for the public to use. The following webpage is a collection of demonstrations of how a handful of popular samplers can be used on real world, open source data sets.","title":"The Uses Of MCMC and Nested Sampling Algorithms"},{"location":"#the-samplers","text":"PyMC3 dynesty emcee UltraNest","title":"The Samplers"},{"location":"#the-data","text":"Gamma-ray spectroscopy of a source of Ba-133 Exoplanet light curve from Kepler-10 Gravitational wave signal","title":"The Data"},{"location":"LightCurve/LightCurve/","text":"Using emcee and UltraNest to model the light curves from Kepler-10 Kepler-10 is a star located roughly 608 lightyears from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes infront of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intesity, occuring every time the exoplanet is infront of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existense of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA. Useful imports # numpy import numpy as np # scipy from scipy.special import gammaln, ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt %matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print('emcee version: {}'.format(mc.__version__)) print('UltraNest version: {}'.format(ultranest.__version__)) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2 Viewing the data Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (simillar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits.open(\"kplr011904151-2010265121752_llc.fits\") tab = table[1] data = Table(tab.data) flux_orig = data['PDCSAP_FLUX'][:1325] time_orig = data['TIME'][:1325] flux = [flux_orig[i] for i in range(len(flux_orig)) if str(flux_orig[i]) != \"nan\"] time = [time_orig[i] for i in range(len(time_orig)) if str(flux_orig[i]) != \"nan\"] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # plot all useful data ax1.plot(time, flux, \"k:\") ax1.set_ylabel(\"Flux / e-/s\") # plot zoomed in view of transits ax2.plot(time, flux, \"k:\") ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541325, 541700) ax2.set_xlim(540, 545) plt.suptitle(\"Kepler-10 light curves showing evidence of exoplanet transits\") plt.show() The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy. A problem you may notice right away with this data set is that the flux drop during each transit is small; the flux only decreases by 0.025%. This fluctuation is even comparable to the level of noise in the data. To solve this, I augmented the data set by making two changes. First, I subtracted all the flux below the light curve, so that the lowest points in the data sit just above 1 unit of flux, and then took the natural log of the entire data set. The purpose of this was to stretch out the transit flux change, whilst keeping the noise level in check. This augmentation reults in the flux drop increasing to 20%. The second augmentation is shortening the data set. The full data is made up of over 1300 points, which takes around 8 minutes total to run this script on my machine. To save time, I'll just use the \"zoomed in\" data shown above. floor = 541300 flux_aug = [np.log(i-floor) if i - floor > 1 else 0 for i in flux][:155] time_aug = time[:155] # plot augmented lightcurve plt.figure(figsize=(15,3)) plt.plot(time_aug, flux_aug, 'k:') plt.ylabel('Augmented flux') plt.xlabel('Time / days') plt.title('Augmented light curve from Kepler-10') plt.show() The model Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is simillar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit(time, f, df, p, tt, tf=None, off=0, square=False): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immidiately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None: tf = tt if tt <= tf: # Default to square shaped transit square = True y = [] if not square: # define slope of sides of trapezium h = f*df*tt/(tt-tf) grad = 2*h/tt for i in time: j = (i + off) % p if j < tt: # transit # square shaped transit if square: y.append(f*(1 - df)) # trapezium shaped transit elif j/tt < 0.5: # first half of transit val = f - grad*j if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # last half of transit val = (grad*j) - 2*h + f if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # no transit y.append(f) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models. After the sampling has been completed, all of the augmentations I made earlier need to be undone. The following functions will do just that: def unaug_f(f_aug): \"\"\" returns array of original f, given array of augmented f \"\"\" f = np.exp(np.array(f_aug))+floor return f def unaug_df(df_aug, f_mean, f_is_aug=False): \"\"\" returns array of original df, given array of augmented df and mean original/augmented f \"\"\" if f_is_aug: f_mean = unaug_f(f_mean) df = 1 - ( (f_mean-floor)**(1-df_aug) + floor ) / f return df Modelling with emcee Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # normal prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [(f_min, f_max), (df_mu, df_sig), (p_mu, p_sig), (tt_mu, tt_sig), (tf_mu, tf_sig), (off_mu, off_sig)] # remove tf for square transit parameters priors_square = priors[:4] + priors[5:] Sampling the data The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very simillar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior(theta): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors[i][0] < theta[i] < priors[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors[i][0]) / priors[i][1])**2 return lprior def logprior_square(theta): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors_square)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors_square[i][0] < theta[i] < priors_square[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors_square[i][0]) / priors_square[i][1])**2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike(theta): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, tf_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, tf_like, off=off_like)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b def loglike_square(theta): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, off=off_like, square=True)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior(theta): lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta) def logposterior_square(theta): lprior = logprior_square(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_square(theta) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" sanples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range(len(priors)): if i == 0: inisamples.append(np.random.uniform(priors[i][0], priors[i][1],Nens)) else: inisamples.append(np.random.normal(priors[i][0], priors[i][1],Nens)) inisamples = np.array(inisamples).T inisamples_square = [] for i in range(len(priors_square)): if i == 0: inisamples_square.append(np.random.uniform(priors_square[i][0], priors_square[i][1],Nens)) else: inisamples_square.append(np.random.normal(priors_square[i][0], priors_square[i][1],Nens)) inisamples_square = np.array(inisamples_square).T ndims = inisamples.shape[1] ndims_square = inisamples_square.shape[1] # no. iterations Nburn = 500 Nsamples = 500 loglike.ncalls = 0 loglike_square.ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc.EnsembleSampler(Nens, ndims, logposterior) sampler_square = mc.EnsembleSampler(Nens, ndims_square, logposterior_square) # perform sampling t0 = timer() sampler.run_mcmc(inisamples, Nsamples+Nburn) t1 = timer() print(\"time taken to sample a trapezium transit model with emcee: {} seconds\".format(t1-t0)) sampler_square.run_mcmc(inisamples_square, Nsamples+Nburn) t2 = timer() print(\"time taken to sample a square transit model with emcee: {} seconds\".format(t2-t1)) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler.chain[:, Nburn:, :].reshape((-1, ndims)) samples_square = sampler_square.chain[:, Nburn:, :].reshape((-1, ndims_square)) Results Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['Aug Flux', 'Aug dFlux', 'Period', 'Transit Time', 'Transit Flat Time', 'Offset'] plotposts(samples_trapez, labels) For each model, we can find the mean and standard deviation of each parameter using the traces. To do this, we have to unaugment f and df, using the functions described above. For the trapezium transit model: f, ferr = np.mean(unaug_f(samples_trapez[:,0]) ), np.std(unaug_f(samples_trapez[:,0])) df, dferr = np.mean(unaug_df(samples_trapez[:,1],f) ), np.std(unaug_df(samples_trapez[:,1],f)) p, perr = np.mean(samples_trapez[:,2]), np.std(samples_trapez[:,2]) tt, tterr = np.mean(samples_trapez[:,3]), np.std(samples_trapez[:,3]) tf, tferr = np.mean(samples_trapez[:,4]), np.std(samples_trapez[:,4]) off, offerr = np.mean(samples_trapez[:,5]), np.std(samples_trapez[:,5]) print(\"Parameters describing a trapezium shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f,ferr) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df,dferr) + \" period = {} \\u00B1 {} days \\n\".format(p,perr) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt,tterr) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf,tferr) + \" offset = {} \\u00B1 {} days \\n\".format(off,offerr)) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square, ferr_square = np.mean(unaug_f(samples_square[:,0]) ), np.std(unaug_f(samples_square[:,0])) df_square, dferr_square = np.mean(unaug_df(samples_square[:,1],f_square) ), np.std(unaug_df(samples_square[:,1],f_square)) p_square, perr_square = np.mean(samples_square[:,2]), np.std(samples_square[:,2]) tt_square, tterr_square = np.mean(samples_square[:,3]), np.std(samples_square[:,3]) off_square, offerr_square = np.mean(samples_square[:,4]), np.std(samples_square[:,4]) print(\"Parameters describing a square shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_square,ferr_square) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_square,dferr_square) + \" period = {} \\u00B1 {} days \\n\".format(p_square,perr_square) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_square,tterr_square) + \" offset = {} \\u00B1 {} days \\n\".format(off_square,offerr_square)) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b. Plotting the posterior We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np.random.choice(unaug_f(samples_trapez[:,0]),n_fits) dfsamps_trap_emcee = np.random.choice(unaug_df(samples_trapez[:,1],f),n_fits) psamps_trap_emcee = np.random.choice(samples_trapez[:,2],n_fits) ttsamps_trap_emcee = np.random.choice(samples_trapez[:,3],n_fits) tfsamps_trap_emcee = np.random.choice(samples_trapez[:,4],n_fits) offsamps_trap_emcee = np.random.choice(samples_trapez[:,5],n_fits) fsamps_square_emcee = np.random.choice(unaug_f(samples_square[:,0]),n_fits) dfsamps_square_emcee = np.random.choice(unaug_df(samples_square[:,1],f_square),n_fits) psamps_square_emcee = np.random.choice(samples_square[:,2],n_fits) ttsamps_square_emcee = np.random.choice(samples_square[:,3],n_fits) offsamps_square_emcee = np.random.choice(samples_square[:,4],n_fits) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f,df,p,tt,tf,off) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\") plt.show() The same process can be repeated for the square transit model: fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_square,df_square,p_square,tt_square,off=off_square) ax1.plot(x, y, \"b-\", alpha = 0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_square_emcee[i], dfsamps_square_emcee[i], psamps_square_emcee[i], ttsamps_square_emcee[i], off=offsamps_square_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\") plt.show() From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system. Model comparisons Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs, RpRs_err = np.sqrt(df), dferr/(np.sqrt(df)) RpRs_square, RpRs_square_err = np.sqrt(df_square), dferr_square/(np.sqrt(df_square)) print(\"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {}\".format(RpRs, RpRs_err)) print(\"Planet to star radius ratio (square transit model): {} \\u00B1 {}\".format( RpRs_square, RpRs_square_err)) print(\"True planet to star radius ratio: {}\".format(0.0127)) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 True planet to star radius ratio: 0.0127 Both models are in close agreement with eachother, and are quite close to the true value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and true ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square, tferr_square = 0.99*tt_square, 0.99*tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor(df,tt,tf): # return semi-minor axis, given model parameters numerator = (tt**2)*(1-np.sqrt(df))**2 - (tf**2)*(1+np.sqrt(df))**2 denominator = tt**2 - tf**2 return np.sqrt(numerator / denominator) def normsemimajor(df,p,tt,tf): # return normalised semi-major axis, given model parameters numerator = 2*p*(df)**(1/4) denominator = np.pi*np.sqrt(tt**2 - tf**2) return numerator / denominator # semiminor for trapezium model b_trap = semiminor(df, tt, tf) b_trap_err = abs(semiminor(df + dferr, tt, tf) - b_trap) # semiminor for square model b_square = semiminor(df_square, tt_square, tf_square) b_square_err = abs(semiminor(df_square + dferr_square, tt_square, tf_square) - b_square) # semimajor for trapezium model aRs_trap = normsemimajor(df, p, tt, tf) aRs_trap_err = abs(normsemimajor(df+dferr,p+perr,tt+tterr,tf+tferr) - aRs_trap) # semimajor for square model aRs_square = normsemimajor(df_square, p_square, tt_square, tf_square) aRs_square_err = abs(normsemimajor(df_square+dferr_square,p_square+perr_square, tt_square+tterr_square,tf_square+tferr_square) - aRs_square) print(\"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {}\".format(aRs_trap, aRs_trap_err)) print(\"Normalised semi-major axis (square transit model): {} \\u00B1 {}\".format(aRs_square, aRs_square_err)) print(\"True normalised semi-major axis: {}\".format(3.40)) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 True normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np.arccos(b_trap/aRs_trap) * (180/np.pi) I_trap_err = abs(np.arccos((b_trap+b_trap_err)/(aRs_trap-aRs_trap_err)) * (180/np.pi) - I_trap) I_square = np.arccos(b_square/aRs_square) * (180/np.pi) I_square_err = abs(np.arccos((b_square+b_square_err)/(aRs_square-aRs_square_err) ) * (180/np.pi) - I_square) print(\"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\".format(I_trap,I_trap_err)) print(\"Inclination angle (square transit model): {} \\u00B1 {} degrees\".format(I_square,I_square_err)) print(\"True inclination angle: {} degrees\".format(84.4)) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees True inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density(df,p,tt,aRs): # return density of star given model parameters G = 6.67408e-11 wt = tt*np.pi/p # transform p from days to seconds p *= 86400 numerator = 3*np.pi*aRs**3 denominator = G*p**2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density(df,p,tt,aRs_trap) stard_trap_err = abs(star_density(df+dferr,p-perr,tt-tterr,aRs_trap+aRs_trap_err) - stard_trap) # star density predicted by square model stard_square = star_density(df_square,p_square,tt_square,aRs_square) stard_square_err = abs(star_density(df_square+dferr_square,p_square-perr_square,tt_square-tterr_square, aRs_square+aRs_square_err) - stard_square) print(\"Star density (trapezium transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_trap, stard_trap_err)) print(\"Star density (square transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_square, stard_square_err)) print(\"True star density: {} kg/m\\u00b3\".format(1070)) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only infront of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above. Modelling with UltraNest If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package. Sampling the data This works in a simillar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors[i][1]-priors[i][0])*theta[i] + priors[i][0] else: # normal transform for remaining parameters params[i] = priors[i][0] + priors[i][1]*ndtri(theta[i]) return np.array(params) def prior_transform_square(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors_square[i][1]-priors_square[i][0] )*theta[i] + priors_square[i][0] else: # normal transform for remaining parameters params[i] = priors_square[i][0] + priors_square[i][1]*ndtri(theta[i]) return np.array(params) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','tf','off'], loglike, prior_transform) sampler_square = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','off'], loglike_square, prior_transform_square) # use \"slice\" sampler, due to high dimensionality nsteps = 2*len(priors) sampler.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) sampler_square.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer() results_trap = sampler.run(min_num_live_points=nlive) t1 = timer() results_square = sampler_square.run(min_num_live_points=nlive) t2 = timer() print('\\n \\n'+ 'Time taken to sample trapezium shaped model with UltraNest: {} seconds'.format(t1-t0)) print('Time taken to sample square shaped model with UltraNest: {} seconds'.format(t2-t1)) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds Results To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np.array(results_trap[\"weighted_samples\"][\"points\"]) weights_trap = np.array(results_trap[\"weighted_samples\"][\"weights\"]) resample_trap = np.random.rand(len(weights_trap)) < weights_trap/max(weights_trap) samples_trap_ultra = samples_points_trap[resample_trap, :] The mean parameter values and their errors can be found easily, remembering to unaugment f and df: # parameter means and errors for trapezium transit f_trap_ultra, ferr_trap_ultra = np.mean(unaug_f(samples_trap_ultra[:,0]) ), np.std(unaug_f(samples_trap_ultra[:,0])) df_trap_ultra, dferr_trap_ultra = np.mean(unaug_df(samples_trap_ultra[:,1],f_trap_ultra) ), np.std(unaug_df(samples_trap_ultra[:,1],f_trap_ultra)) p_trap_ultra, perr_trap_ultra = np.mean(samples_trap_ultra[:,2] ), np.std(samples_trap_ultra[:,2]) tt_trap_ultra, tterr_trap_ultra = np.mean(samples_trap_ultra[:,3] ), np.std(samples_trap_ultra[:,3]) tf_trap_ultra, tferr_trap_ultra = np.mean(samples_trap_ultra[:,4] ), np.std(samples_trap_ultra[:,4]) off_trap_ultra, offerr_trap_ultra = np.mean(samples_trap_ultra[:,5] ), np.std(samples_trap_ultra[:,5]) print(\"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_trap_ultra,ferr_trap_ultra) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_trap_ultra,dferr_trap_ultra) + \" period = {} \\u00B1 {} days \\n\".format(p_trap_ultra,perr_trap_ultra) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_trap_ultra,tterr_trap_ultra) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf_trap_ultra,tferr_trap_ultra) + \" offset = {} \\u00B1 {} days \\n\".format(off_trap_ultra,offerr_trap_ultra)) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days Plotting the posterior We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np.random.choice(unaug_f(samples_trap_ultra[:,0]),nfits) dfsamps_trap_ultra = np.random.choice(unaug_df(samples_trap_ultra[:,1],f_trap_ultra),nfits) psamps_trap_ultra = np.random.choice(samples_trap_ultra[:,2],nfits) ttsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,3],nfits) tfsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,4],nfits) offsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,5],nfits) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15,10)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_trap_ultra,df_trap_ultra,p_trap_ultra, tt_trap_ultra,tf_trap_ultra,off_trap_ultra) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") ax1.set_title(\"UltraNest mean posterior plot\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_title(\"UltraNest posterior predictive plot\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) # emcee and UltraNest overlapping posterior predictive plot for i in range(n_fits): y_ultra = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) y_emcee = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax3.plot(x, y_ultra, 'b-', alpha=0.008, linewidth=5, label='UltraNest' if i == 0 else '') ax3.plot(x, y_emcee, 'r-', alpha=0.008, linewidth=5, label='emcee' if i == 0 else '') ax3.plot(time, flux, 'k:', linewidth=3) ax3.set_ylabel('Flux / e-/s') ax3.set_xlabel('Time / days') ax3.set_title('emcee (red) and UltraNest (blue) overlayed posterior predictive plots') ax3.set_ylim(541300, 541600) ax3.set_xlim(540, 545) leg = ax3.legend(loc='lower right') for lh in leg.legendHandles: lh.set_alpha(0.8) plt.suptitle('A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)') plt.show() It seems from this that the two models are very simillar, however since the low density regious are mostly blue, UltraNest may produce a greater uncertainty than emcee. Model comparisons Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap, logZerr_trap = results_trap['logz'], results_trap['logzerr'] logZ_square, logZerr_square = results_square['logz'], results_square['logzerr'] print(\"Marginalised likelihood for trapezium transit model: {} \u00b1 {}\".format(logZ_trap, logZerr_trap)) print(\"Marginalised likelihood for square transit model: {} \u00b1 {}\".format(logZ_square, logZerr_square)) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np.exp(logZ_trap - logZ_square) print(\"Bayes factor: {}\".format(K)) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"emcee"},{"location":"LightCurve/LightCurve/#using-emcee-and-ultranest-to-model-the-light-curves-from-kepler-10","text":"Kepler-10 is a star located roughly 608 lightyears from Earth. Kepler-10 was targeted by NASA in their search for an Earth-like exoplanet, and in 2011 the first exoplanet orbiting Kepler-10 was discovered. The planet, Kepler-10b, is a rocky planet with 1.4x the radius of Earth, and 3.7x the mass. As Kepler-10b passes infront of its star, it obstructs some flux (the light energy per unit time per unit area) from the star, casting a shadow towards Earth. We see this as a slight periodic dip in light intesity, occuring every time the exoplanet is infront of its star. Measuring the light curve (flux as a function of time) from a star with an exoplanet is called \"transit detection\", and can be used to infer the existense of an exoplanet and find the properties of the star-planet system. In this example, I will create a model describing the flux of a star with, a single orbiting planet, as a function of time. I will then use the \"emcee\" and \"UltraNest\" samplers to fit the model parameters to some real Kepler-10 light curve data, provided by NASA.","title":"Using emcee and UltraNest to model the light curves from Kepler-10"},{"location":"LightCurve/LightCurve/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.special import gammaln, ndtri from scipy.stats import gaussian_kde # astropy from astropy.io import fits from astropy.table import Table # plotting import corner from matplotlib import pyplot as plt %matplotlib inline # samplers import emcee as mc import ultranest import ultranest.stepsampler as stepsampler print('emcee version: {}'.format(mc.__version__)) print('UltraNest version: {}'.format(ultranest.__version__)) # misc from time import time as timer emcee version: 3.0.2 UltraNest version: 2.2.2","title":"Useful imports"},{"location":"LightCurve/LightCurve/#viewing-the-data","text":"Light curve models can vary from simple square shaped transits, to extremely complicated transits involving limb-darkening and other effects. To decide which model is most appropriate, we need to first see the data we will be using. The light curve data is in the form of a FITS file. These files can be easily loaded into a Table format (simillar to a Pandas DataFrame) using astropy. The data is quite large, so I chose to look at the first 1325 data points only. Whilst extracting the data, any data points with a flux of \"nan\" need to be removed from both the \"flux\" and \"time\" lists. table = fits.open(\"kplr011904151-2010265121752_llc.fits\") tab = table[1] data = Table(tab.data) flux_orig = data['PDCSAP_FLUX'][:1325] time_orig = data['TIME'][:1325] flux = [flux_orig[i] for i in range(len(flux_orig)) if str(flux_orig[i]) != \"nan\"] time = [time_orig[i] for i in range(len(time_orig)) if str(flux_orig[i]) != \"nan\"] Now, we can plot the light curve we will be using and decide how complicated our model needs to be. Of the two plots below, the first shows the whole light curve that I'll be using, and the second shows a \"zoomed in\" segment of the light curve. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # plot all useful data ax1.plot(time, flux, \"k:\") ax1.set_ylabel(\"Flux / e-/s\") # plot zoomed in view of transits ax2.plot(time, flux, \"k:\") ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541325, 541700) ax2.set_xlim(540, 545) plt.suptitle(\"Kepler-10 light curves showing evidence of exoplanet transits\") plt.show() The above plot shows regular dips in flux, as expected from exoplanet transits. The dips appear to be \"V-shaped\", with sloped sides and a flat bottom. This suggests we could use a model of regular trapezium shaped flux drops. However, the slopes are fairly steep, so we could also use a much simpler square shaped flux drop instead. This would save on time, but may come at the cost of accuracy. A problem you may notice right away with this data set is that the flux drop during each transit is small; the flux only decreases by 0.025%. This fluctuation is even comparable to the level of noise in the data. To solve this, I augmented the data set by making two changes. First, I subtracted all the flux below the light curve, so that the lowest points in the data sit just above 1 unit of flux, and then took the natural log of the entire data set. The purpose of this was to stretch out the transit flux change, whilst keeping the noise level in check. This augmentation reults in the flux drop increasing to 20%. The second augmentation is shortening the data set. The full data is made up of over 1300 points, which takes around 8 minutes total to run this script on my machine. To save time, I'll just use the \"zoomed in\" data shown above. floor = 541300 flux_aug = [np.log(i-floor) if i - floor > 1 else 0 for i in flux][:155] time_aug = time[:155] # plot augmented lightcurve plt.figure(figsize=(15,3)) plt.plot(time_aug, flux_aug, 'k:') plt.ylabel('Augmented flux') plt.xlabel('Time / days') plt.title('Augmented light curve from Kepler-10') plt.show()","title":"Viewing the data"},{"location":"LightCurve/LightCurve/#the-model","text":"Below is a diagram showing which parameters are needed to define a trapezium shaped transit (left), and how I will go about implementing the model in Python (right). I started with a simple recurring step function, then modified the step to have the triangular shape with height \"h\", which can be calculated using basic trigonometry. Finally, I added a hard floor at a flux change of df, to create the trapezium shape. The square transit shape is simillar, but tt and tf are equal. Using this, I defined a the function that will be used to model the transit: def transit(time, f, df, p, tt, tf=None, off=0, square=False): \"\"\" Flux, from a uniform star source with single orbiting planet, as a function of time :param time: 1D array, input times :param f: unobscured flux, max flux level :param df: ratio of obscured to unobscured flux :param p: period of planet's orbit :param tt: total time of transit :param tf: time during transit in which flux doesn't change :param off: time offset. A value of 0 means the transit begins immidiately :param square: If True, the shape of the transit will be square (tt == tf) :return: 1D array, flux from the star \"\"\" if tf is None: tf = tt if tt <= tf: # Default to square shaped transit square = True y = [] if not square: # define slope of sides of trapezium h = f*df*tt/(tt-tf) grad = 2*h/tt for i in time: j = (i + off) % p if j < tt: # transit # square shaped transit if square: y.append(f*(1 - df)) # trapezium shaped transit elif j/tt < 0.5: # first half of transit val = f - grad*j if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # last half of transit val = (grad*j) - 2*h + f if val < f*(1 - df): y.append(f*(1 - df)) else: y.append(val) else: # no transit y.append(f) return y I'll be using both a trapezium and square shape transit in tandem throughout this example, and I'll compare the performance and accuracies of both models. After the sampling has been completed, all of the augmentations I made earlier need to be undone. The following functions will do just that: def unaug_f(f_aug): \"\"\" returns array of original f, given array of augmented f \"\"\" f = np.exp(np.array(f_aug))+floor return f def unaug_df(df_aug, f_mean, f_is_aug=False): \"\"\" returns array of original df, given array of augmented df and mean original/augmented f \"\"\" if f_is_aug: f_mean = unaug_f(f_mean) df = 1 - ( (f_mean-floor)**(1-df_aug) + floor ) / f return df","title":"The model"},{"location":"LightCurve/LightCurve/#modelling-with-emcee","text":"Now that we know the parameters that will describe the model, we can start guessing at the parameter priors by using the plots above. Since the square transit model does not require the \"tf\" parameter, we can omit it from the list of square transit priors for a little extra time save. This model is quite complicated with six parameters, and eyeballing the values of each parameter can be tricky. Using a little trial and error, I came up with the following guesses: # normal prior on flux f_min = 4.9 f_max = 5.8 # normal prior on flux drop df_mu = 0.19 df_sig = 0.005 # normal prior on period p_mu = 0.8372 p_sig = 0.008 # normal prior on total transit time tt_mu = 0.145 tt_sig = 0.01 # normal prior on flat transit time tf_mu = 0.143 tf_sig = 0.01 # normal prior on offset off_mu = 0.1502 off_sig = 0.0008 priors = [(f_min, f_max), (df_mu, df_sig), (p_mu, p_sig), (tt_mu, tt_sig), (tf_mu, tf_sig), (off_mu, off_sig)] # remove tf for square transit parameters priors_square = priors[:4] + priors[5:]","title":"Modelling with emcee"},{"location":"LightCurve/LightCurve/#sampling-the-data","text":"The \"emcee\" sampler requires the user to provide a prior, likelihood, and posterior function, all in their log forms. These functions are very simillar for the trapezium and square shaped transit models; the key difference being the \"tf\" parameter is omitted for the square model. Since I decided on using normal and uniform priors for each parameter, The log of the prior takes the following forms: def logprior(theta): \"\"\" Function to return the log of the prior for a trapezium shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors[i][0] < theta[i] < priors[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors[i][0]) / priors[i][1])**2 return lprior def logprior_square(theta): \"\"\" Function to return the log of the prior for a square shaped transit light curve model :param theta: tuple or list containing each parameter \"\"\" lprior = 0 for i in range(len(priors_square)): # sum log priors from each parameter if i == 0: # prior for uniform parameters if priors_square[i][0] < theta[i] < priors_square[i][1]: pass else: lprior = -np.inf else: # prior for non-uniform parameters lprior -= 0.5*((theta[i] - priors_square[i][0]) / priors_square[i][1])**2 return lprior The likelihood takes the form of a Poisson distribution, since flux is a non-negative quantity. The expected value of the likelihood \"lmbda\" is found using the \"transit\" function defined above. def loglike(theta): \"\"\" Function to return the log likelihood of the trapezium shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, tf_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, tf_like, off=off_like)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b def loglike_square(theta): \"\"\" Function to return the log likelihood of the square shpaed transit light curve model :param theta: tuple or list containing each parameter :param obs: list or array containing the observed flux of each data point :param times: list or array containing the times at which each data point is recorded \"\"\" # unpack parameters f_like, df_like, p_like, tt_like, off_like = theta # expected value lmbda = np.array(transit(time_aug, f_like, df_like, p_like, tt_like, off=off_like, square=True)) n = len(flux_aug) a = np.sum(gammaln(np.array(flux_aug)+1)) b = np.sum(np.array(flux_aug) * np.log(lmbda)) return -np.sum(lmbda) - a + b When using MCMC, the log posterior can be found as the sum of the log prior and log likelihood: def logposterior(theta): lprior = logprior(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike(theta) def logposterior_square(theta): lprior = logprior_square(theta) # check log prior is finite if not np.isfinite(lprior): return -np.inf return lprior + loglike_square(theta) Next, we can start setting up the MCMC model. To start, I'll draw 200 \"ensemble\" sanples from each prior distribution, which will be used to represent the priors. I'll also define 500 \"burn-in\" iterations to allow the chain to converge, and 500 further iterations to produce the posteriors. # no. ensemble points Nens = 200 inisamples = [] for i in range(len(priors)): if i == 0: inisamples.append(np.random.uniform(priors[i][0], priors[i][1],Nens)) else: inisamples.append(np.random.normal(priors[i][0], priors[i][1],Nens)) inisamples = np.array(inisamples).T inisamples_square = [] for i in range(len(priors_square)): if i == 0: inisamples_square.append(np.random.uniform(priors_square[i][0], priors_square[i][1],Nens)) else: inisamples_square.append(np.random.normal(priors_square[i][0], priors_square[i][1],Nens)) inisamples_square = np.array(inisamples_square).T ndims = inisamples.shape[1] ndims_square = inisamples_square.shape[1] # no. iterations Nburn = 500 Nsamples = 500 loglike.ncalls = 0 loglike_square.ncalls = 0 Now that everything is set up, we can perform the sampling process: sampler = mc.EnsembleSampler(Nens, ndims, logposterior) sampler_square = mc.EnsembleSampler(Nens, ndims_square, logposterior_square) # perform sampling t0 = timer() sampler.run_mcmc(inisamples, Nsamples+Nburn) t1 = timer() print(\"time taken to sample a trapezium transit model with emcee: {} seconds\".format(t1-t0)) sampler_square.run_mcmc(inisamples_square, Nsamples+Nburn) t2 = timer() print(\"time taken to sample a square transit model with emcee: {} seconds\".format(t2-t1)) time taken to sample a trapezium transit model with emcee: 22.782975673675537 seconds time taken to sample a square transit model with emcee: 19.97200083732605 seconds The burn-in points can be removed before collecting the chains as follows: samples_trapez = sampler.chain[:, Nburn:, :].reshape((-1, ndims)) samples_square = sampler_square.chain[:, Nburn:, :].reshape((-1, ndims_square))","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results","text":"Let's take a look at what we found. Looking at the trapezium model, we can plot the posteriors of each parameter, along with contour plots describing how one parameter may vary with any other. This can be done using \"corner.py\", and a scipy Gaussian KDE function. def plotposts(samples, labels, **kwargs): fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) pos = [i*(len(labels)+1) for i in range(len(labels))] for axidx, samps in zip(pos, samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 50) fig.axes[axidx].plot(xvals, kde(xvals), color='firebrick') labels = ['Aug Flux', 'Aug dFlux', 'Period', 'Transit Time', 'Transit Flat Time', 'Offset'] plotposts(samples_trapez, labels) For each model, we can find the mean and standard deviation of each parameter using the traces. To do this, we have to unaugment f and df, using the functions described above. For the trapezium transit model: f, ferr = np.mean(unaug_f(samples_trapez[:,0]) ), np.std(unaug_f(samples_trapez[:,0])) df, dferr = np.mean(unaug_df(samples_trapez[:,1],f) ), np.std(unaug_df(samples_trapez[:,1],f)) p, perr = np.mean(samples_trapez[:,2]), np.std(samples_trapez[:,2]) tt, tterr = np.mean(samples_trapez[:,3]), np.std(samples_trapez[:,3]) tf, tferr = np.mean(samples_trapez[:,4]), np.std(samples_trapez[:,4]) off, offerr = np.mean(samples_trapez[:,5]), np.std(samples_trapez[:,5]) print(\"Parameters describing a trapezium shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f,ferr) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df,dferr) + \" period = {} \\u00B1 {} days \\n\".format(p,perr) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt,tterr) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf,tferr) + \" offset = {} \\u00B1 {} days \\n\".format(off,offerr)) Parameters describing a trapezium shaped transit model: unobstructed flux = 541508.1852844431 \u00b1 36.9344420907229 e-/s fractional flux decrease = 0.00024461083692660444 \u00b1 3.7035342916871527e-06 period = 0.8370801841354675 \u00b1 0.008002462715920132 days total transit time = 0.14446313515960185 \u00b1 0.010012520942390852 days flat transit time = 0.14221868661500203 \u00b1 0.010486564635702324 days offset = 0.1501856219851231 \u00b1 0.000792190343252601 days The same can be done for the square transit model: f_square, ferr_square = np.mean(unaug_f(samples_square[:,0]) ), np.std(unaug_f(samples_square[:,0])) df_square, dferr_square = np.mean(unaug_df(samples_square[:,1],f_square) ), np.std(unaug_df(samples_square[:,1],f_square)) p_square, perr_square = np.mean(samples_square[:,2]), np.std(samples_square[:,2]) tt_square, tterr_square = np.mean(samples_square[:,3]), np.std(samples_square[:,3]) off_square, offerr_square = np.mean(samples_square[:,4]), np.std(samples_square[:,4]) print(\"Parameters describing a square shaped transit model: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_square,ferr_square) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_square,dferr_square) + \" period = {} \\u00B1 {} days \\n\".format(p_square,perr_square) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_square,tterr_square) + \" offset = {} \\u00B1 {} days \\n\".format(off_square,offerr_square)) Parameters describing a square shaped transit model: unobstructed flux = 541509.5710077788 \u00b1 38.29278598182498 e-/s fractional flux decrease = 0.00024373683466889182 \u00b1 3.7647641082140636e-06 period = 0.8372673158389097 \u00b1 0.008000578546054328 days total transit time = 0.14409081829513087 \u00b1 0.010074301386211069 days offset = 0.1501740077644908 \u00b1 0.00081315394152917 days The period in both cases is around 20 hours. This is unique to one body in the Kepler-10 system: Our model describes the transits of Kepler-10b.","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior","text":"We can sample from the posteriors further to create slightly different sets of the parameters. From this, we can plot a new line over our original data, creating a posterior predictive plot. The regions in which the model is most likely to fall in will appear darker on the plot, and so the darker the plot, the higher the probabillity of the flux passing through it. Start by randomly choosing 400 of each parameter for the trapezium and square models: n_fits = 400 fsamps_trap_emcee = np.random.choice(unaug_f(samples_trapez[:,0]),n_fits) dfsamps_trap_emcee = np.random.choice(unaug_df(samples_trapez[:,1],f),n_fits) psamps_trap_emcee = np.random.choice(samples_trapez[:,2],n_fits) ttsamps_trap_emcee = np.random.choice(samples_trapez[:,3],n_fits) tfsamps_trap_emcee = np.random.choice(samples_trapez[:,4],n_fits) offsamps_trap_emcee = np.random.choice(samples_trapez[:,5],n_fits) fsamps_square_emcee = np.random.choice(unaug_f(samples_square[:,0]),n_fits) dfsamps_square_emcee = np.random.choice(unaug_df(samples_square[:,1],f_square),n_fits) psamps_square_emcee = np.random.choice(samples_square[:,2],n_fits) ttsamps_square_emcee = np.random.choice(samples_square[:,3],n_fits) offsamps_square_emcee = np.random.choice(samples_square[:,4],n_fits) Below are two plots of the results of the MCMC algorithm. The first shows the entire original light curve data set, with a model with mean parameters plotted on top. The second shows a \"zoomed in\" view of a few exoplanet transits, with the posterior predictive overplotted. fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f,df,p,tt,tf,off) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean trapezium transit model (top) and posterior predictive plot (bottom)\") plt.show() The same process can be repeated for the square transit model: fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,6)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_square,df_square,p_square,tt_square,off=off_square) ax1.plot(x, y, \"b-\", alpha = 0.8) ax1.set_ylabel(\"Flux / e-/s\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_square_emcee[i], dfsamps_square_emcee[i], psamps_square_emcee[i], ttsamps_square_emcee[i], off=offsamps_square_emcee[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_xlabel(\"Time / days\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) plt.suptitle(\"A light curve from Kepler-10 with overplotted\" + \" mean square transit model (top) and posterior predictive plot (bottom)\") plt.show() From the posterior predictive plots, it seems that both models explain the data fairly well. The trapezium model seems like it follows the data a little closer, but I won't dismiss the square transit model just yet. To decide which model best describes the data, we can see which best predicts properties of the Kepler-10 system.","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons","text":"Now that we have the mean values for each parameter, we can start to infer information about the star-planet system the model describes. I'll use both models for this, and find out which model predicts the properties the system with the greatest accuracy. The first property we can find is the ratio of the planet radius \"Rp\" and star radius \"Rs\". This is simply the square root of the mean flux drop: RpRs, RpRs_err = np.sqrt(df), dferr/(np.sqrt(df)) RpRs_square, RpRs_square_err = np.sqrt(df_square), dferr_square/(np.sqrt(df_square)) print(\"Planet to star radius ratio (trapezium transit model): {} \\u00B1 {}\".format(RpRs, RpRs_err)) print(\"Planet to star radius ratio (square transit model): {} \\u00B1 {}\".format( RpRs_square, RpRs_square_err)) print(\"True planet to star radius ratio: {}\".format(0.0127)) Planet to star radius ratio (trapezium transit model): 0.01564003954363941 \u00b1 0.00023679826904231388 Planet to star radius ratio (square transit model): 0.015612073362269722 \u00b1 0.00024114440285122598 True planet to star radius ratio: 0.0127 Both models are in close agreement with eachother, and are quite close to the true value of the planet to star radius ratio (to within the same order of magnitude). We only used a handful of transits, and under one month of data, so the difference between predicted and true ratios here are acceptable. Sampling with the entire dataset instead of the shortened data I used does not improve this estimate. Next, we can attempt to find incination angle \"I\", usually defined as the angle between the plane of a celestial body's orbit and the plane that is normal to the line of sight from Earth. This isn't quite possible for the square model, due to the total transit and flat transit times being equal. To account for this, I'll instead say the flat transit time is 99% of the total transit time, just to keep everything finite. tf_square, tferr_square = 0.99*tt_square, 0.99*tterr_square The calculations become pretty complex here, and error propagation becomes difficult. Instead, I'll approximate the errors, as we're mostly interested in the relative errors between the two models anyway. To begin, we start by approximating the semi-minor axis \"b\", and the normalised semi-major axis \"aRs\" as below: def semiminor(df,tt,tf): # return semi-minor axis, given model parameters numerator = (tt**2)*(1-np.sqrt(df))**2 - (tf**2)*(1+np.sqrt(df))**2 denominator = tt**2 - tf**2 return np.sqrt(numerator / denominator) def normsemimajor(df,p,tt,tf): # return normalised semi-major axis, given model parameters numerator = 2*p*(df)**(1/4) denominator = np.pi*np.sqrt(tt**2 - tf**2) return numerator / denominator # semiminor for trapezium model b_trap = semiminor(df, tt, tf) b_trap_err = abs(semiminor(df + dferr, tt, tf) - b_trap) # semiminor for square model b_square = semiminor(df_square, tt_square, tf_square) b_square_err = abs(semiminor(df_square + dferr_square, tt_square, tf_square) - b_square) # semimajor for trapezium model aRs_trap = normsemimajor(df, p, tt, tf) aRs_trap_err = abs(normsemimajor(df+dferr,p+perr,tt+tterr,tf+tferr) - aRs_trap) # semimajor for square model aRs_square = normsemimajor(df_square, p_square, tt_square, tf_square) aRs_square_err = abs(normsemimajor(df_square+dferr_square,p_square+perr_square, tt_square+tterr_square,tf_square+tferr_square) - aRs_square) print(\"Normalised semi-major axis (trapezium transit model): {} \\u00B1 {}\".format(aRs_trap, aRs_trap_err)) print(\"Normalised semi-major axis (square transit model): {} \\u00B1 {}\".format(aRs_square, aRs_square_err)) print(\"True normalised semi-major axis: {}\".format(3.40)) Normalised semi-major axis (trapezium transit model): 3.11343439016216 \u00b1 0.24897493968215692 Normalised semi-major axis (square transit model): 3.38730776154764 \u00b1 0.11671862724332449 True normalised semi-major axis: 3.4 The ratio of semi-minor axis to normalised semi-major axis gives the cosine of the inclination angle of Kepler-10b. Therefore, the predictions of the inclination from both models are as follows: I_trap = np.arccos(b_trap/aRs_trap) * (180/np.pi) I_trap_err = abs(np.arccos((b_trap+b_trap_err)/(aRs_trap-aRs_trap_err)) * (180/np.pi) - I_trap) I_square = np.arccos(b_square/aRs_square) * (180/np.pi) I_square_err = abs(np.arccos((b_square+b_square_err)/(aRs_square-aRs_square_err) ) * (180/np.pi) - I_square) print(\"Inclination angle (trapezium transit model): {} \\u00B1 {} degrees\".format(I_trap,I_trap_err)) print(\"Inclination angle (square transit model): {} \\u00B1 {} degrees\".format(I_square,I_square_err)) print(\"True inclination angle: {} degrees\".format(84.4)) Inclination angle (trapezium transit model): 81.68965145311803 \u00b1 0.9307232291459115 degrees Inclination angle (square transit model): 83.21823835267094 \u00b1 0.5963780613143967 degrees True inclination angle: 84.4 degrees An inclination of 90 degrees means that the planet orbits parallel to the line of sight from Earth. Again, both models make a decent attempt estimating the inclination, however the square transit shape is a little more accurate, and has a smaller uncertainty. This might suggest the square transit model might actually be a little better for this data. Finally, we can attempt to predict the density of the star, Kepler-10. This makes the assumption that the radius of the star is much bigger than the radius of the planet. Since we measured the ratio of planet to star radii to be around 0.015, this assumption is pretty reasonable. The star density can be calculated using the semi-major and semi-minor axes, along with some other parameters from the models: def star_density(df,p,tt,aRs): # return density of star given model parameters G = 6.67408e-11 wt = tt*np.pi/p # transform p from days to seconds p *= 86400 numerator = 3*np.pi*aRs**3 denominator = G*p**2 return numerator / denominator # star density predicted by trapezium model stard_trap = star_density(df,p,tt,aRs_trap) stard_trap_err = abs(star_density(df+dferr,p-perr,tt-tterr,aRs_trap+aRs_trap_err) - stard_trap) # star density predicted by square model stard_square = star_density(df_square,p_square,tt_square,aRs_square) stard_square_err = abs(star_density(df_square+dferr_square,p_square-perr_square,tt_square-tterr_square, aRs_square+aRs_square_err) - stard_square) print(\"Star density (trapezium transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_trap, stard_trap_err)) print(\"Star density (square transit model): {} \\u00B1 {} kg/m\\u00b3\".format(stard_square, stard_square_err)) print(\"True star density: {} kg/m\\u00b3\".format(1070)) Star density (trapezium transit model): 964.774196469593 \u00b1 100.61863106968042 kg/m\u00b3 Star density (square transit model): 1048.78853195487 \u00b1 79.23816897368705 kg/m\u00b3 True star density: 1070 kg/m\u00b3 This shows that not only does a square transit shape predict the density of the star remarkably well, it also has a slightly lower fractional uncertainty on it's estimate compared to the trapezium shaped transit. A reason for this may be that Kepler-10 is around the same size as the Sun, and Kepler-10b is actually larger than Earth, yet the planet is only infront of its star for only 3 hours. This means the planet has to be travelling fast, resulting in very steep slopes on the trapezium transit light curve. It seems involving the extra \"tf\" parameter only serves to complicate the model and add uncertainty when using a square wave is just as good, as is proven above.","title":"Model comparisons"},{"location":"LightCurve/LightCurve/#modelling-with-ultranest","text":"If we want a more definitive way of determining which model better desribes the observed data, we'll need to find the Bayes factor, which requires the marginalised likelihoods for the trapezium and square transit shapes. We can do this using the \"UltraNest\" nested sampling package.","title":"Modelling with UltraNest"},{"location":"LightCurve/LightCurve/#sampling-the-data_1","text":"This works in a simillar way to emcee, in fact we can use the same likelihood functions defined earlier, however we do have to create a new prior function. UltraNest samples from a unit hypercube parameter space, and so the prior function must transform the parameters back into their true space. The two functions below show how this is done using scipy's inverse error function \"ndtri\": def prior_transform(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a trapezium transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors[i][1]-priors[i][0])*theta[i] + priors[i][0] else: # normal transform for remaining parameters params[i] = priors[i][0] + priors[i][1]*ndtri(theta[i]) return np.array(params) def prior_transform_square(theta): \"\"\" Transforms parameters from a unit hypercube space to their true space for a square transit model \"\"\" params = [0 for i in range(len(theta))] for i in range(len(theta)): if i == 0: # uniform transform for f params[i] = (priors_square[i][1]-priors_square[i][0] )*theta[i] + priors_square[i][0] else: # normal transform for remaining parameters params[i] = priors_square[i][0] + priors_square[i][1]*ndtri(theta[i]) return np.array(params) We can now create a model for both the trapezium and square transit shapes. Since there are up to 6 parameters in a model, the UltraNest sampler may struggle to perform. To solve this, use a slice sampler as is shown below: # initialise samplers sampler = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','tf','off'], loglike, prior_transform) sampler_square = ultranest.ReactiveNestedSampler(['f', 'df','p','tt','off'], loglike_square, prior_transform_square) # use \"slice\" sampler, due to high dimensionality nsteps = 2*len(priors) sampler.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) sampler_square.stepsampler = stepsampler.RegionSliceSampler(nsteps=nsteps) # define live points and stopping criterion nlive = 400 stop = 0.5 # run the samplers t0 = timer() results_trap = sampler.run(min_num_live_points=nlive) t1 = timer() results_square = sampler_square.run(min_num_live_points=nlive) t2 = timer() print('\\n \\n'+ 'Time taken to sample trapezium shaped model with UltraNest: {} seconds'.format(t1-t0)) print('Time taken to sample square shaped model with UltraNest: {} seconds'.format(t2-t1)) [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .17 [-273.5034..-273.5033]*| it/evals=2795/188281 eff=1.4876% N=400 [ultranest] Likelihood function evaluations: 188565 [ultranest] logZ = -275.8 +- 0.0255 [ultranest] Effective samples strategy satisfied (ESS = 2007.1, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.46+-0.06 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.07, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. [ultranest] Sampling 400 live points from prior ... VBox(children=(HTML(value=''), GridspecLayout(children=(HTML(value=\"<div style='background-color:#6E6BF4;'>&nb\u2026 [ultranest] Explored until L=-3e+02 .21 [-273.4930..-273.4929]*| it/evals=2798/184875 eff=1.5167% N=400 [ultranest] Likelihood function evaluations: 184875 [ultranest] logZ = -275.8 +- 0.03241 [ultranest] Effective samples strategy satisfied (ESS = 2008.9, need >400) [ultranest] Posterior uncertainty strategy is satisfied (KL: 0.45+-0.11 nat, need <0.50 nat) [ultranest] Evidency uncertainty strategy is satisfied (dlogz=0.06, need <0.5) [ultranest] logZ error budget: single: 0.04 bs:0.03 tail:0.01 total:0.03 required:<0.50 [ultranest] done iterating. Time taken to sample trapezium shaped model with UltraNest: 45.595500469207764 seconds Time taken to sample square shaped model with UltraNest: 40.1266610622406 seconds","title":"Sampling the data"},{"location":"LightCurve/LightCurve/#results_1","text":"To retrieve the traces from a nested sampler, you must resample according the some weights which are produced during the sampling. I'll only do this for the trapezium model for now, just to keep the code from getting too cluttered: samples_points_trap = np.array(results_trap[\"weighted_samples\"][\"points\"]) weights_trap = np.array(results_trap[\"weighted_samples\"][\"weights\"]) resample_trap = np.random.rand(len(weights_trap)) < weights_trap/max(weights_trap) samples_trap_ultra = samples_points_trap[resample_trap, :] The mean parameter values and their errors can be found easily, remembering to unaugment f and df: # parameter means and errors for trapezium transit f_trap_ultra, ferr_trap_ultra = np.mean(unaug_f(samples_trap_ultra[:,0]) ), np.std(unaug_f(samples_trap_ultra[:,0])) df_trap_ultra, dferr_trap_ultra = np.mean(unaug_df(samples_trap_ultra[:,1],f_trap_ultra) ), np.std(unaug_df(samples_trap_ultra[:,1],f_trap_ultra)) p_trap_ultra, perr_trap_ultra = np.mean(samples_trap_ultra[:,2] ), np.std(samples_trap_ultra[:,2]) tt_trap_ultra, tterr_trap_ultra = np.mean(samples_trap_ultra[:,3] ), np.std(samples_trap_ultra[:,3]) tf_trap_ultra, tferr_trap_ultra = np.mean(samples_trap_ultra[:,4] ), np.std(samples_trap_ultra[:,4]) off_trap_ultra, offerr_trap_ultra = np.mean(samples_trap_ultra[:,5] ), np.std(samples_trap_ultra[:,5]) print(\"Parameters describing a trapezium shaped transit model from UltraNest: \\n \\n\" + \" unobstructed flux = {} \\u00B1 {} e-/s \\n\".format(f_trap_ultra,ferr_trap_ultra) + \"fractional flux decrease = {} \\u00B1 {} \\n\".format(df_trap_ultra,dferr_trap_ultra) + \" period = {} \\u00B1 {} days \\n\".format(p_trap_ultra,perr_trap_ultra) + \" total transit time = {} \\u00B1 {} days \\n\".format(tt_trap_ultra,tterr_trap_ultra) + \" flat transit time = {} \\u00B1 {} days \\n\".format(tf_trap_ultra,tferr_trap_ultra) + \" offset = {} \\u00B1 {} days \\n\".format(off_trap_ultra,offerr_trap_ultra)) Parameters describing a trapezium shaped transit model from UltraNest: unobstructed flux = 541506.0367278325 \u00b1 38.07219702378676 e-/s fractional flux decrease = 0.0002456676060063817 \u00b1 3.5643330145173593e-06 period = 0.8376992378349714 \u00b1 0.007805962818414923 days total transit time = 0.14467439301959553 \u00b1 0.010237468121439578 days flat transit time = 0.14315067160042744 \u00b1 0.009933449672703609 days offset = 0.15020443219136784 \u00b1 0.0008319592417544441 days","title":"Results"},{"location":"LightCurve/LightCurve/#plotting-the-posterior_1","text":"We can make the same posterior plots as with emcee, for the sake of comparison. The samples for the posterior predictive plot can be collected as follows: nfits = 400 # samples for trapezium transit posterior predictive plot fsamps_trap_ultra = np.random.choice(unaug_f(samples_trap_ultra[:,0]),nfits) dfsamps_trap_ultra = np.random.choice(unaug_df(samples_trap_ultra[:,1],f_trap_ultra),nfits) psamps_trap_ultra = np.random.choice(samples_trap_ultra[:,2],nfits) ttsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,3],nfits) tfsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,4],nfits) offsamps_trap_ultra = np.random.choice(samples_trap_ultra[:,5],nfits) Below are three plots. The first two plots show the mean posterior plot and the posteroir predictive plot respectively, produced by UltraNest. The third shows the posterior predictive plot from UltraNest, with the posterior predictive plot from emcee overplotted: fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15,10)) # mean plot ax1.plot(time, flux, \"k:\", linewidth=3) x = np.linspace(min(time), max(time), 1500) y = transit(x,f_trap_ultra,df_trap_ultra,p_trap_ultra, tt_trap_ultra,tf_trap_ultra,off_trap_ultra) ax1.plot(x, y, \"b-\", alpha=0.8) ax1.set_ylabel(\"Flux / e-/s\") ax1.set_title(\"UltraNest mean posterior plot\") # posterior predictive plot for i in range(n_fits): y = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) ax2.plot(x, y, \"b-\", alpha=0.01, linewidth=5) ax2.plot(time, flux, \"k:\", linewidth=3) ax2.set_ylabel(\"Flux / e-/s\") ax2.set_title(\"UltraNest posterior predictive plot\") ax2.set_ylim(541300, 541600) ax2.set_xlim(540, 545) # emcee and UltraNest overlapping posterior predictive plot for i in range(n_fits): y_ultra = transit(x, fsamps_trap_ultra[i], dfsamps_trap_ultra[i], psamps_trap_ultra[i], ttsamps_trap_ultra[i], tfsamps_trap_ultra[i], offsamps_trap_ultra[i]) y_emcee = transit(x, fsamps_trap_emcee[i], dfsamps_trap_emcee[i], psamps_trap_emcee[i], ttsamps_trap_emcee[i], tfsamps_trap_emcee[i], offsamps_trap_emcee[i]) ax3.plot(x, y_ultra, 'b-', alpha=0.008, linewidth=5, label='UltraNest' if i == 0 else '') ax3.plot(x, y_emcee, 'r-', alpha=0.008, linewidth=5, label='emcee' if i == 0 else '') ax3.plot(time, flux, 'k:', linewidth=3) ax3.set_ylabel('Flux / e-/s') ax3.set_xlabel('Time / days') ax3.set_title('emcee (red) and UltraNest (blue) overlayed posterior predictive plots') ax3.set_ylim(541300, 541600) ax3.set_xlim(540, 545) leg = ax3.legend(loc='lower right') for lh in leg.legendHandles: lh.set_alpha(0.8) plt.suptitle('A light curve from Kepler-10 with overplotted' + ' mean trapezium transit model (top) and posterior predictive plot (bottom)') plt.show() It seems from this that the two models are very simillar, however since the low density regious are mostly blue, UltraNest may produce a greater uncertainty than emcee.","title":"Plotting the posterior"},{"location":"LightCurve/LightCurve/#model-comparisons_1","text":"Instead of going through all the same inclination angle and star density calculations, I'll instead use the logs of the marginalised likelihoods to compare the models. These can be easily collected from the sampler results: logZ_trap, logZerr_trap = results_trap['logz'], results_trap['logzerr'] logZ_square, logZerr_square = results_square['logz'], results_square['logzerr'] print(\"Marginalised likelihood for trapezium transit model: {} \u00b1 {}\".format(logZ_trap, logZerr_trap)) print(\"Marginalised likelihood for square transit model: {} \u00b1 {}\".format(logZ_square, logZerr_square)) Marginalised likelihood for trapezium transit model: -275.82725389875935 \u00b1 0.03821787065493758 Marginalised likelihood for square transit model: -275.8168531966252 \u00b1 0.06290063669114569 Using these marginal likelihoods, we can find the Bayes factor. This is the defined as the ratio of the marginal likelihoods. If the Bayes factor is larger than one, it means that the trapezium model is more likely to produce the observed data. K = np.exp(logZ_trap - logZ_square) print(\"Bayes factor: {}\".format(K)) Bayes factor: 0.9896531981395177 This result tells us that the square transit model is only slightly more likely to produce the observed light curve, as we first predicted earlier with emcee. In reality, the transit model makes a lot more sense than a square transit shape. However, since there were only around 4 or 5 data points per transit, their shape was likely misrepresented. If instead we used a data set of a planet with a longer transit time, or took flux measurements more frequently, then the trapezium transit shape may become prefered.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/","text":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133. Useful imports # numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import pymc3 as pm print('PyMC3 version: {}'.format(pm.__version__)) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print('dynesty version: {}'.format(dynesty.__version__)) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1 Viewing the data The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\". This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open(cali_dir, 'r') as file: calibration = file.read().splitlines() calibration = list(map(float, calibration)) c_0 = calibration[0] c_2 = calibration[2] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open(spectra_dir, 'r') as file: counts = [int(j) for j in file] counts = counts[2:] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np.array(range(len(counts))) # Bins for gamma-ray spectrum # Plot the spectrum plt.figure(figsize=(15,5)) plt.plot(xrange, counts, 'b') plt.fill(xrange, counts, 'b', alpha= 0.4) plt.xlabel('Energy / MCA Channels') plt.ylabel('Counts') plt.title('Gamma-Ray Spectrum of a sample of Ba-133') plt.yscale('log') plt.xlim(540, 3500) plt.show() The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks(counts, height=1300, distance=100)[0][3:] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Lets plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [c_0*np.array(range(peak-ranger, peak+ranger)) + c_2 for peak in peaks] peaks_y = [counts[peak-ranger:peak+ranger] for peak in peaks] # Plot selected peaks from gamma-ray spectrum fig, axs = plt.subplots(2,3, figsize=(12,7)) for i in range(2): for j in range(3): ind = 3*i + j axs[i,j].plot(peaks_x[ind], peaks_y[ind], 'b') axs[i, j].fill(peaks_x[ind], peaks_y[ind], 'b', alpha=0.2) if i == 1: axs[i,j].set_xlabel('Energy / KeV') if j == 0: axs[i,j].set_ylabel('Counts') fig.suptitle('Photopeaks produced by a sample of Ba-133', y=0.95) plt.show() The model The decays that cause the photopeaks in a GRS have a descrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature, rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian fuction to be used: def gauss(x, a, xc, w, y0): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a*np.exp(-(x-xc)**2/(2*w**2))+y0 Modelling with PyMC3 Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [pm.Model() for i in range(len(peaks))] a_guesses = [23000., 900., 6100., 13800., 39800., 5300.] xc_guesses = [81., 161., 276.5, 303., 356., 384.] y0_guesses = [1700., 1350., 300., 300., 250., 50.] Sampling the data Next, I'll use the above guesses to initialise each model. PyMC3 requires the used to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probabillity distribution functions. This is done within the scope of each model defined above, using the \"with\" statement: for i in range(len(peaks)): with gauss_models[i]: # set prior parameters # amplitude a_mu = a_guesses[i] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[i] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses[i] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Uniform('Standard Deviation', lower=w_min, upper=w_max) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # Expected value of outcome mu = gauss(peaks_x[i], a_model, xc_model, w_model, y0_model) # Poisson likelihood of observations Y_obs = pm.Poisson('Y_obs', mu=mu, observed=peaks_y[i]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging.getLogger('pymc3') logger.setLevel(logging.ERROR) # perform sampling traces = [] t0 = time() for i in range(len(peaks)): with gauss_models[i]: traces.append(pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True)) t1 = time() timepymc3 = t1-t0 # time taken to sample all of the photopeaks print('{} seconds ({} seconds per peak) taken to run PyMC3 sampling.'.format(timepymc3, timepymc3/6)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling. Sampling the data - Alternate method The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probabillity distribution in log-space, meaning the standard deviation has an equal probabillity of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using simillar code as before, but with the new likelihood. gauss_model_alt = pm.Model() with gauss_model_alt: # set prior parameters # amplitude a_mu = a_guesses[1] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[1] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses[1] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = -1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Normal('Standard Deviation', mu=w_mu, sd=w_sig) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # set uniform prior sigma_model = pm.Uniform('Noise', lower=sigma_min, upper=sigma_max) # Expected value of outcome mu = gauss(peaks_x[1], a_model, xc_model, w_model, y0_model) # Normal likelihood of observations with noise Y_obs = pm.Normal('Y_obs', mu=mu, sd=10 ** sigma_model, observed=peaks_y[1]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time() with gauss_model_alt: trace_alt = pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True) t1_alt = time() timepymc3_alt = t1_alt-t0_alt print('{} seconds taken to run PyMC3 alternate sampling.'.format(timepymc3_alt)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np.vstack((trace_alt['Amplitude'], trace_alt['Peak Energy'], trace_alt['Standard Deviation'], trace_alt['Background Counts'], trace_alt['Noise'])).T # mean and standard deviation error of each parameter a_alt, a_err_alt = np.mean(samples_alt[:,0]), np.std(samples_alt[:,0]) xc_alt, xc_err_alt = np.mean(samples_alt[:,1]), np.std(samples_alt[:,1]) w_alt, w_err_alt = np.mean(samples_alt[:,2]), np.std(samples_alt[:,2]) y0_alt, y0_err_alt = np.mean(samples_alt[:,3]), np.std(samples_alt[:,3]) sigma_alt, sigma_err_alt = np.mean(samples_alt[:,4]), np.std(samples_alt[:,4]) # print values print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_alt, a_err_alt) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_alt, xc_err_alt) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_alt, w_err_alt) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_alt, y0_err_alt) + ' Noise = {} \\u00B1 {} counts'.format(sigma_alt, sigma_err_alt)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts Results Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [np.vstack((trace['Amplitude'], trace['Peak Energy'], trace['Standard Deviation'], trace['Background Counts'])).T for trace in traces] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range(len(peaks))] for ind in range(len(peaks)): resdict[ind]['a_mu'] = np.mean(all_pymc3_samples[ind][:, 0]) resdict[ind]['a_sig'] = np.std(all_pymc3_samples[ind][:, 0]) resdict[ind]['xc_mu'] = np.mean(all_pymc3_samples[ind][:, 1]) resdict[ind]['xc_sig'] = np.std(all_pymc3_samples[ind][:, 1]) resdict[ind]['w_mu'] = np.mean(all_pymc3_samples[ind][:, 2]) resdict[ind]['w_sig'] = np.std(all_pymc3_samples[ind][:, 2]) resdict[ind]['y0_mu'] = np.mean(all_pymc3_samples[ind][:, 3]) resdict[ind]['y0_sig'] = np.std(all_pymc3_samples[ind][:, 3]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts(samples, labels, **kwargs): \"\"\" Function to plot posteriors using corner.py and scipy's gaussian KDE function. \"\"\" fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) plt.subplots_adjust(wspace=0.2, hspace=0.2) # plot KDE smoothed version of distributions for axidx, samps in zip([0, 5, 10, 15], samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 100) fig.axes[axidx].plot(xvals, kde(xvals), color=\"firebrick\") # create corner plot for peak with noisiest data labels = [r'Amplitude', r'Peak Energy', r'Standard Deviation', r'Background Counts'] corner_plot_samples = all_pymc3_samples[1] plotposts(corner_plot_samples, labels) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a, a_err = resdict[1]['a_mu'], resdict[1]['a_sig'] xc, xc_err = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w, w_err = resdict[1]['w_mu'], resdict[1]['w_sig'] y0, y0_err = resdict[1]['y0_mu'], resdict[1]['y0_sig'] print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a, a_err) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc, xc_err) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w, w_err) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0, y0_err)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts Plotting the posterior Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig, axs = plt.subplots(2, 3, figsize=(12, 7)) for i in range(2): for j in range(3): ind = 3 * i + j a = resdict[ind]['a_mu'] xc = resdict[ind]['xc_mu'] w = resdict[ind]['w_mu'] y0 = resdict[ind]['y0_mu'] x = peaks_x[ind] y = peaks_y[ind] # plot original data axs[i, j].plot(x, y, 'b.', alpha=1, label=('Original Data' if all(num == 0 for num in [i,j]) else '')) # plot fitted curve over the data xsmooth = np.linspace(x[0], x[-1], len(x) * 100) axs[i, j].plot(xsmooth, gauss(xsmooth, a, xc, w, y0), 'k:', alpha=1, label=('Fitted Model' if all(num == 0 for num in [i,j]) else '')) if i == 1: axs[i, j].set_xlabel('Energy / keV') if j == 0: axs[i, j].set_ylabel('Counts') leg = fig.legend(loc='lower right', numpoints=1) for lh in leg.legendHandles: lh.set_alpha(1) fig.suptitle('Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling') plt.show() Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3, xc_samps_pymc3, w_samps_pymc3, y0_samps_pymc3 = ([] for i in range(4)) for ind in range(len(peaks)): a_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 0], size=n_fits)) xc_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 1], size=n_fits)) w_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 2], size=n_fits)) y0_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 3], size=n_fits)) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x[ind] xsmooth = np.linspace(x[0], x[-1], len(x) * 100) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4)) for i in range(n_fits): ax1.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.01, linewidth=2) ax2.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.02, linewidth=2) ax1.set_ylabel('Counts') ax1.set_xlabel('Energy / keV') ax2.set_xlim(159.4,161.8) ax2.set_ylim(1800, 2250) ax2.set_xlabel('Energy / keV') fig.suptitle('Posterior predictive plot for a photopeak from a sample of Ba-133') plt.show() Model comparisons Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak: # parameter means and standard deviations of peak a_pymc3, aerr_pymc3 = resdict[1]['a_mu'], resdict[1]['a_sig'] xc_pymc3, xcerr_pymc3 = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w_pymc3, werr_pymc3 = resdict[1]['w_mu'], resdict[1]['w_sig'] y0_pymc3, y0err_pymc3 = resdict[1]['y0_mu'], resdict[1]['y0_sig'] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate.quad(lambda t: gauss(t, a_pymc3, xc_pymc3, w_pymc3, y0_pymc3), 159.1, 162.2)[0] / c_0 peak_integral_err = np.sqrt(2 * np.pi * ((w_pymc3 * aerr_pymc3) ** 2 + (a_pymc3 * werr_pymc3) ** 2 )) / c_0 percent_err = 100*peak_integral_err/peak_integral print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral, peak_integral_err) + 'Percentage error = {}%'.format(percent_err)) Total counts = 22933.86204613347 \u00b1 215.58699451020811 counts Percentage error = 0.9400378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate.quad(lambda t: gauss(t, a_alt, xc_alt, w_alt, y0_alt), 159.1, 162.2)[0] / c_0 peak_integral_err_alt = np.sqrt(2 * np.pi * ((w_alt * a_err_alt) ** 2 + (a_alt * w_err_alt) ** 2 )) / c_0 percent_err_alt = 100*peak_integral_err_alt/peak_integral_alt print('Alternate total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_alt, peak_integral_err_alt) + 'Alternate percentage error = {}%'.format(percent_err_alt)) Alternate total counts = 22943.751210742565 \u00b1 216.76313608385547 counts Alternate percentage error = 0.9447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful. Modelling with dynesty Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they can do a better job. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here. Sampling the data Using dynesty is slightly more complicated than PyMC3. Nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform(theta): # unpack the transformed parameters a_t, xc_t, w_t, y0_t = theta # define our prior guesses for each parameter a_mu, a_sig = a_guesses[1], 50. xc_mu, xc_sig = xc_guesses[1], 1. w_mu, w_sig = 0.7, 0.3 y0_mu, y0_sig = y0_guesses[1], 30. # convert back to a = a_mu + a_sig*ndtri(a_t) xc = xc_mu + xc_sig*ndtri(xc_t) w = w_mu + w_sig*ndtri(w_t) y0 = y0_mu + y0_sig*ndtri(y0_t) return a,xc,w,y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike(theta): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like, xc_like, w_like, y0_like = theta # expected value lmbda = np.array(gauss(peaks_x[1], a_like, xc_like, w_like, y0_like)) n = len(peaks_y[1]) a = np.sum(gammaln(np.array(peaks_y[1])+1)) b = np.sum(np.array(peaks_y[1]) * np.log(lmbda)) return -np.sum(lmbda) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points, sampling algorithm, sampling method, and a stopping criterion. Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler(loglike, priortransform, nparam, bound='multi', sample='unif', nlive=1000) t0_dynesty = time() sampler.run_nested(dlogz=stop, print_progress=False) t1_dynesty = time() print('{} seconds taken to run dynesty sampling'.format(t1_dynesty-t0_dynesty)) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler.results print(results.summary()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None Results To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np.exp(results['logwt'] - results['logz'][-1]) samples = results.samples dynesty_samples = resample_equal(samples, weights) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a, xc, w, y0 = [dynesty_samples[:,i] for i in range(4)] a_dynesty, aerr_dynesty = np.mean(a), np.std(a) xc_dynesty, xcerr_dynesty = np.mean(xc), np.std(xc) w_dynesty, werr_dynesty = np.mean(w), np.std(w) y0_dynesty, y0err_dynesty = np.mean(y0), np.std(y0) nfits = 300 a_samps_dynesty = np.random.normal(a_dynesty, aerr_dynesty, nfits) xc_samps_dynesty = np.random.normal(xc_dynesty, xcerr_dynesty, nfits) w_samps_dynesty = np.random.normal(w_dynesty, werr_dynesty, nfits) y0_samps_dynesty = np.random.normal(y0_dynesty, y0err_dynesty, nfits) print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_dynesty, aerr_dynesty) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_dynesty, xcerr_dynesty) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_dynesty, werr_dynesty) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_dynesty, y0err_dynesty)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts Plotting the posterior So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'b.') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'k:') ax1.set_ylim(1200) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01) ax2.set_ylim(1200) plt.suptitle('Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of\\n' + ' Ba-133, sampled by dynesty') plt.show() Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'k.',label='Data') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'b:',label='dynesty') ax1.plot(xsmooth, gauss(xsmooth,a_pymc3,xc_pymc3,w_pymc3,y0_pymc3),'r:',label='PyMC3') ax1.set_ylim(1200) leg1 = ax1.legend(loc='upper right') for lh in leg1.legendHandles: lh.set_alpha(1) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01, label=('dynesty' if i == 0 else '')) ax2.plot(xsmooth, gauss(xsmooth,a_samps_pymc3[1][i],xc_samps_pymc3[1][i], w_samps_pymc3[1][i],y0_samps_pymc3[1][i]),'r-',alpha=0.01, label=('PyMC3' if i == 0 else '')) ax2.set_ylim(1200) leg2 = ax2.legend(loc='upper right') for lh in leg2.legendHandles: lh.set_alpha(1) plt.suptitle('Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + '\\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)') plt.show() Model comparisons This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate.quad(lambda t: gauss(t, a_dynesty, xc_dynesty, w_dynesty, y0_dynesty), 159.1, 162.2)[0] / c_0 peak_integral_err_dynesty = np.sqrt(2 * np.pi * ((w_dynesty * aerr_dynesty)** 2 + (a_dynesty * werr_dynesty) ** 2 )) / c_0 percent_err_dynesty = 100*peak_integral_err_dynesty/peak_integral_dynesty print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_dynesty, peak_integral_err_dynesty) + 'Percentage error = {}%'.format(percent_err_dynesty)) Total counts = 22968.854117562147 \u00b1 224.93466521968398 counts Percentage error = 0.9793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error around 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"PyMC3"},{"location":"PyMC3_GRS/PyMC3_GRS/#using-pymc3-and-dynesty-to-fit-gaussian-curves-to-photopeaks-in-a-gamma-ray-spectrum","text":"A gamma-ray spectrum (GRS) is a histogram describing the counts of detected photons as a function of photon energy. GRS can be useful when evaluating the dosage received from a sample containing unknown radioisotopes. To do this, the total counts produced above background by a source has to be calculated. Above the background level, a gamma source produces sharp peaks, called \"photopeaks\", due to discrete energy level changes in a nucleus. A method for finding the total counts is to fit a curve to every photopeak in a GRS, and integrate each one to find the total area contained under photopeaks. In this example, I'll use MCMC to fit Gaussian curves to peaks found in gamma-ray spectrum of a sample of Ba-133.","title":"Using PyMC3 and dynesty to fit Gaussian curves to photopeaks in a gamma-ray spectrum"},{"location":"PyMC3_GRS/PyMC3_GRS/#useful-imports","text":"# numpy import numpy as np # scipy from scipy.signal import find_peaks from scipy.stats import gaussian_kde from scipy import integrate from scipy.special import ndtri, gammaln # Plotting import corner import matplotlib.pyplot as plt %matplotlib inline # Samplers import pymc3 as pm print('PyMC3 version: {}'.format(pm.__version__)) import dynesty from dynesty import NestedSampler from dynesty.utils import resample_equal print('dynesty version: {}'.format(dynesty.__version__)) # misc import logging from time import time PyMC3 version: 3.8 dynesty version: 1.0.1","title":"Useful imports"},{"location":"PyMC3_GRS/PyMC3_GRS/#viewing-the-data","text":"The data is in the form of a histogram with over 16000 bins, each with width of one \"MCA-Channel\". This unit of energy is specific to the detector used to collect the GRS, and so we also must calibrate the spectrum to have a bin width in keV. Start by loading in both the calibration parameters, and the entire gamma-ray spectrum as a list: #Load detector calibration cali_dir = 'calibration.txt' with open(cali_dir, 'r') as file: calibration = file.read().splitlines() calibration = list(map(float, calibration)) c_0 = calibration[0] c_2 = calibration[2] #Load gamma-ray spectrum data spectra_dir = 'Ba.TKA' with open(spectra_dir, 'r') as file: counts = [int(j) for j in file] counts = counts[2:] The spectrum contains an X-ray region at lower energies, and an extremely noisy region at higher energies. Both of these regions are not very useful for this demonstration, so I'll only show the section I'll be searching for photopeaks. xrange = np.array(range(len(counts))) # Bins for gamma-ray spectrum # Plot the spectrum plt.figure(figsize=(15,5)) plt.plot(xrange, counts, 'b') plt.fill(xrange, counts, 'b', alpha= 0.4) plt.xlabel('Energy / MCA Channels') plt.ylabel('Counts') plt.title('Gamma-Ray Spectrum of a sample of Ba-133') plt.yscale('log') plt.xlim(540, 3500) plt.show() The spectrum is made up of a smooth background counts curve, with sharp peaks sitting on top. These are the photopeaks we're searching for. Using scipy's \"find_peaks\" function, we can select some photopeaks in the spectrum to analyse. This function looks for local maxima by comparing a point to it's neighbours. The optional arguments specify the minimum height for a peak to be returned, and a \"neighbourhood width\", so only the largest peak in a given neighbourhood will be returned. # Find prominent peaks in data using scipy peaks = find_peaks(counts, height=1300, distance=100)[0][3:] This function returns the indicies at which a peak maximum is located in the gamma-ray spectrum. Next, I'll define a \"radius\" of 20 bins around each peak centre, and create lists containing the data for each peak. Lets plot each peak to see what the function found: # select an area around peak to be plotted & calibrate energy scale to keV ranger = 20 peaks_x = [c_0*np.array(range(peak-ranger, peak+ranger)) + c_2 for peak in peaks] peaks_y = [counts[peak-ranger:peak+ranger] for peak in peaks] # Plot selected peaks from gamma-ray spectrum fig, axs = plt.subplots(2,3, figsize=(12,7)) for i in range(2): for j in range(3): ind = 3*i + j axs[i,j].plot(peaks_x[ind], peaks_y[ind], 'b') axs[i, j].fill(peaks_x[ind], peaks_y[ind], 'b', alpha=0.2) if i == 1: axs[i,j].set_xlabel('Energy / KeV') if j == 0: axs[i,j].set_ylabel('Counts') fig.suptitle('Photopeaks produced by a sample of Ba-133', y=0.95) plt.show()","title":"Viewing the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#the-model","text":"The decays that cause the photopeaks in a GRS have a descrete energy. The width of the photopeaks is caused by imperfections in the detector crystal, such as defects or excess thermal energy. This causes each peak to have a Gaussian nature, rather than a sharp peak. I'll attempt to fit a Gaussian curve to each peak, by first defining the Gaussian fuction to be used: def gauss(x, a, xc, w, y0): \"\"\" Gaussian function :param x: 1D array of input points :param a: Amplitude of peak :param xc: Mean peak energy :param w: Standard deviation of peak :param y0: Background counts under peak :return: 1D array of Gaussian output points \"\"\" return a*np.exp(-(x-xc)**2/(2*w**2))+y0","title":"The model"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-pymc3","text":"Our goal is to find the values of the parameters above that best explain each photopeak. To ensure that the algorithms quickly converge on the most likely parameter values, I'll guess some values for the parameters of each peak, simply by using the plots above. Since the standard deviation appears roughly the same for all the peaks, I'll set the prior to be uniform. #initialise a model for each peak, and define guesses for the parameters gauss_models = [pm.Model() for i in range(len(peaks))] a_guesses = [23000., 900., 6100., 13800., 39800., 5300.] xc_guesses = [81., 161., 276.5, 303., 356., 384.] y0_guesses = [1700., 1350., 300., 300., 250., 50.]","title":"Modelling with PyMC3"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data","text":"Next, I'll use the above guesses to initialise each model. PyMC3 requires the used to provide a prior for each parameter, and a likelihood function, which can be easily set using the PyMC3 in-built Normal, Uniform, and Poisson probabillity distribution functions. This is done within the scope of each model defined above, using the \"with\" statement: for i in range(len(peaks)): with gauss_models[i]: # set prior parameters # amplitude a_mu = a_guesses[i] # mean of amplitude of peaks a_sig = 100. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[i] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_min = 0.3 # lower bound of peak standard deviation w_max = 2.5 # upper bound of peak standard deviation # background counts y0_mu = y0_guesses[i] # mean of background counts y0_sig = 30. # standard deviation of background counts # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Uniform('Standard Deviation', lower=w_min, upper=w_max) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # Expected value of outcome mu = gauss(peaks_x[i], a_model, xc_model, w_model, y0_model) # Poisson likelihood of observations Y_obs = pm.Poisson('Y_obs', mu=mu, observed=peaks_y[i]) Now each model has been initialised, the MCMC sampling algorithm can now be applied. PyMC3 uses a set of samples, as well as a set of tuning samples. We can also use the \"time\" package to record how long it took to sample all of the photopeaks. Nsamples = 800 # number of samples Ntune = 1000 # number of tuning samples # disable PyMC3 console logs, for neatness logger = logging.getLogger('pymc3') logger.setLevel(logging.ERROR) # perform sampling traces = [] t0 = time() for i in range(len(peaks)): with gauss_models[i]: traces.append(pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True)) t1 = time() timepymc3 = t1-t0 # time taken to sample all of the photopeaks print('{} seconds ({} seconds per peak) taken to run PyMC3 sampling.'.format(timepymc3, timepymc3/6)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 699.00draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 757.94draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 715.65draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 674.49draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 661.42draws/s] Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:10<00:00, 710.30draws/s] 276.74090600013733 seconds (46.123484333356224 seconds per peak) taken to run PyMC3 sampling.","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data-alternate-method","text":"The above method uses a Poisson likelihood, since a metric like counts is non-negative. Although, since the peaks in the gamma-ray spectrum have large enough amplitudes, the likelihood can be well approximated by a normal distribution, with a estimate for the noise standard deviation. Guessing this standard deviation value is tricky, so instead we can set it as an extra parameter for the sampler. A good prior to start with is a uniform probabillity distribution in log-space, meaning the standard deviation has an equal probabillity of having any order of magnitude between an upper and lower bound. I'll showcase this method, but I'll use the previous method for the results section below. I'll also use only the 2nd peak found, as it has the noisiest data and will likely produce the most interesting results. Start by initiating a new set of models using simillar code as before, but with the new likelihood. gauss_model_alt = pm.Model() with gauss_model_alt: # set prior parameters # amplitude a_mu = a_guesses[1] # mean of amplitude of peaks a_sig = 50. # standard deviation of amplitude of peaks # peak energy xc_mu = xc_guesses[1] # mean of peak energy xc_sig = 1. # standard deviation of peak energy # standard deviation w_mu = 1.2 # mean of peak standard deviation w_sig = 1. # standard deviation of peak standard deviation # background counts y0_mu = y0_guesses[1] # mean of background counts y0_sig = 30. # standard deviation of background counts # noise deviation sigma_min = -1 # minimum order of magnitude of the noise deviation sigma_max = 2 # maximum order of magnitude of the noise deviation # set normal priors a_model = pm.Normal('Amplitude', mu=a_mu, sd=a_sig) xc_model = pm.Normal('Peak Energy', mu=xc_mu, sd=xc_sig) w_model = pm.Normal('Standard Deviation', mu=w_mu, sd=w_sig) y0_model = pm.Normal('Background Counts', mu=y0_mu, sd=y0_sig) # set uniform prior sigma_model = pm.Uniform('Noise', lower=sigma_min, upper=sigma_max) # Expected value of outcome mu = gauss(peaks_x[1], a_model, xc_model, w_model, y0_model) # Normal likelihood of observations with noise Y_obs = pm.Normal('Y_obs', mu=mu, sd=10 ** sigma_model, observed=peaks_y[1]) Performing the sampling again gives our alternate posteriors: Nsamples = 800 Ntune = 1000 # perform sampling t0_alt = time() with gauss_model_alt: trace_alt = pm.sample(Nsamples, tune=Ntune, discard_tuned_samples=True) t1_alt = time() timepymc3_alt = t1_alt-t0_alt print('{} seconds taken to run PyMC3 alternate sampling.'.format(timepymc3_alt)) Sampling 4 chains, 0 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7200/7200 [00:09<00:00, 727.73draws/s] 43.72043991088867 seconds taken to run PyMC3 alternate sampling. We can now briefly use the trace to see what values the sampler converged on for each parameter. I'll return to these values later when finding the uncertainty of the counts under the photopeak. # collect samples of each parameter samples_alt = np.vstack((trace_alt['Amplitude'], trace_alt['Peak Energy'], trace_alt['Standard Deviation'], trace_alt['Background Counts'], trace_alt['Noise'])).T # mean and standard deviation error of each parameter a_alt, a_err_alt = np.mean(samples_alt[:,0]), np.std(samples_alt[:,0]) xc_alt, xc_err_alt = np.mean(samples_alt[:,1]), np.std(samples_alt[:,1]) w_alt, w_err_alt = np.mean(samples_alt[:,2]), np.std(samples_alt[:,2]) y0_alt, y0_err_alt = np.mean(samples_alt[:,3]), np.std(samples_alt[:,3]) sigma_alt, sigma_err_alt = np.mean(samples_alt[:,4]), np.std(samples_alt[:,4]) # print values print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_alt, a_err_alt) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_alt, xc_err_alt) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_alt, w_err_alt) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_alt, y0_err_alt) + ' Noise = {} \\u00B1 {} counts'.format(sigma_alt, sigma_err_alt)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 805.5161312481847 \u00b1 22.51101688252669 counts Peak Energy = 160.6059216398261 \u00b1 0.018834853387709613 keV Standard Deviation = 0.5233786079419919 \u00b1 0.019371192247871535 keV Background Counts = 1333.599554840382 \u00b1 7.251969008469188 counts Noise = 1.5885697487281636 \u00b1 0.05223363708405703 counts","title":"Sampling the data - Alternate method"},{"location":"PyMC3_GRS/PyMC3_GRS/#results","text":"Now that the data has been sampled, we can collect the information for each parameter posterior using the traces. By using a dictionary, we can also collect the mean and standard deviation for each parameter, which will be useful later for plotting the fitted curves. # collect traces of each parameter from each peak all_pymc3_samples = [np.vstack((trace['Amplitude'], trace['Peak Energy'], trace['Standard Deviation'], trace['Background Counts'])).T for trace in traces] # dictionaries to contain mean and standard deviation of each peak resdict = [{} for i in range(len(peaks))] for ind in range(len(peaks)): resdict[ind]['a_mu'] = np.mean(all_pymc3_samples[ind][:, 0]) resdict[ind]['a_sig'] = np.std(all_pymc3_samples[ind][:, 0]) resdict[ind]['xc_mu'] = np.mean(all_pymc3_samples[ind][:, 1]) resdict[ind]['xc_sig'] = np.std(all_pymc3_samples[ind][:, 1]) resdict[ind]['w_mu'] = np.mean(all_pymc3_samples[ind][:, 2]) resdict[ind]['w_sig'] = np.std(all_pymc3_samples[ind][:, 2]) resdict[ind]['y0_mu'] = np.mean(all_pymc3_samples[ind][:, 3]) resdict[ind]['y0_sig'] = np.std(all_pymc3_samples[ind][:, 3]) To visualise the information given for each parameter, we can define a function to plot the parameter posteriors, and also create contour plots that describe how any two parameters might depend on each other. This is done using \"corner.py\". As an example, I'll use the 2nd peak again due to its noisy data: def plotposts(samples, labels, **kwargs): \"\"\" Function to plot posteriors using corner.py and scipy's gaussian KDE function. \"\"\" fig = corner.corner(samples, labels=labels, hist_kwargs={'density': True}, **kwargs) plt.subplots_adjust(wspace=0.2, hspace=0.2) # plot KDE smoothed version of distributions for axidx, samps in zip([0, 5, 10, 15], samples.T): kde = gaussian_kde(samps) xvals = fig.axes[axidx].get_xlim() xvals = np.linspace(xvals[0], xvals[1], 100) fig.axes[axidx].plot(xvals, kde(xvals), color=\"firebrick\") # create corner plot for peak with noisiest data labels = [r'Amplitude', r'Peak Energy', r'Standard Deviation', r'Background Counts'] corner_plot_samples = all_pymc3_samples[1] plotposts(corner_plot_samples, labels) This corner plot shows that the amplitude of a photopeak and its standard deviation are dependant, since their contour plot is not symmetric. Now that we have the parameter posteriors, along with their means and standard deviations, we can state the most likely value of each parameter, with their uncertainties: a, a_err = resdict[1]['a_mu'], resdict[1]['a_sig'] xc, xc_err = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w, w_err = resdict[1]['w_mu'], resdict[1]['w_sig'] y0, y0_err = resdict[1]['y0_mu'], resdict[1]['y0_sig'] print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a, a_err) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc, xc_err) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w, w_err) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0, y0_err)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 789.3499838312316 \u00b1 25.40762174271906 counts Peak Energy = 160.60556153256505 \u00b1 0.019374762693360942 keV Standard Deviation = 0.5324207733753832 \u00b1 0.017697951515753676 keV Background Counts = 1334.1324943543657 \u00b1 6.560639436166777 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior","text":"Using the mean values for each parameter, we can define a Gaussian curve for each peak. Plotting this curve over the original data gives the best fit curve for that data. This best fit can be integrated, and by summing the integrals for each peak, the total counts of the gamma-ray spectrum can be found. # plot each peak, with the fitted Gaussians superimposed fig, axs = plt.subplots(2, 3, figsize=(12, 7)) for i in range(2): for j in range(3): ind = 3 * i + j a = resdict[ind]['a_mu'] xc = resdict[ind]['xc_mu'] w = resdict[ind]['w_mu'] y0 = resdict[ind]['y0_mu'] x = peaks_x[ind] y = peaks_y[ind] # plot original data axs[i, j].plot(x, y, 'b.', alpha=1, label=('Original Data' if all(num == 0 for num in [i,j]) else '')) # plot fitted curve over the data xsmooth = np.linspace(x[0], x[-1], len(x) * 100) axs[i, j].plot(xsmooth, gauss(xsmooth, a, xc, w, y0), 'k:', alpha=1, label=('Fitted Model' if all(num == 0 for num in [i,j]) else '')) if i == 1: axs[i, j].set_xlabel('Energy / keV') if j == 0: axs[i, j].set_ylabel('Counts') leg = fig.legend(loc='lower right', numpoints=1) for lh in leg.legendHandles: lh.set_alpha(1) fig.suptitle('Photopeaks produced by a sample of Ba-133,' + ' with fitted Gaussian curves from MCMC sampling') plt.show() Alternatively, instead of using the means of the parameters to plot the fitted curve, we can use the posterior distributions to randomly sample predictions of each parameter. We can then overplot multiple curves onto the data set. This is useful as instead of only showing the most likely model, it visualises the overall uncertainty of the fit. Again, I'll use the noisiest peak as an example. First, randomly choose 300 of each parameter from their posteriors: # number of curves to plot per peak n_fits = 300 a_samps_pymc3, xc_samps_pymc3, w_samps_pymc3, y0_samps_pymc3 = ([] for i in range(4)) for ind in range(len(peaks)): a_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 0], size=n_fits)) xc_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 1], size=n_fits)) w_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 2], size=n_fits)) y0_samps_pymc3.append(np.random.choice(all_pymc3_samples[ind][:, 3], size=n_fits)) We now have 300 sets of potential parammeters. For each set of parameters, define and overplot a Gaussian curve as before, each curve being slightly different. In regions of the plot where a lot of curves overlap, the plot will appear darker relative to regions with fewer curves. The resulting plots show the regions where a fitted curve is more likely to fall. This is called a posterior predictive plot. The plot below shows the posterior predictive distribution for the noisiest photopeak. I also included a second plot, which shows a \"zoomed in\" view of the tip of the peak, at which the most deviation occurs. ind = 1 x = peaks_x[ind] xsmooth = np.linspace(x[0], x[-1], len(x) * 100) fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4)) for i in range(n_fits): ax1.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.01, linewidth=2) ax2.plot(xsmooth, gauss(xsmooth, a_samps_pymc3[ind][i], xc_samps_pymc3[ind][i], w_samps_pymc3[ind][i], y0_samps_pymc3[ind][i]), 'b-', alpha=0.02, linewidth=2) ax1.set_ylabel('Counts') ax1.set_xlabel('Energy / keV') ax2.set_xlim(159.4,161.8) ax2.set_ylim(1800, 2250) ax2.set_xlabel('Energy / keV') fig.suptitle('Posterior predictive plot for a photopeak from a sample of Ba-133') plt.show()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons","text":"Now that we have a model with the mean fitted parameters for each curve, we can integrate to find the total area under a curve. Using the uncertainty in each parameter from above, the pecentage error in the total counts can be found. This can be used as a nice way to judge the quality of the fit, and whether the curve can be \"trusted\" to approximate the data. Using scipy's \"integrate.quad\" function makes the integration simple. I'll use the same example peak as perviously, integrating between the bottom of the tails of the peak: # parameter means and standard deviations of peak a_pymc3, aerr_pymc3 = resdict[1]['a_mu'], resdict[1]['a_sig'] xc_pymc3, xcerr_pymc3 = resdict[1]['xc_mu'], resdict[1]['xc_sig'] w_pymc3, werr_pymc3 = resdict[1]['w_mu'], resdict[1]['w_sig'] y0_pymc3, y0err_pymc3 = resdict[1]['y0_mu'], resdict[1]['y0_sig'] # integrate, dividing by the calibration coefficient (to remove keV from the units) peak_integral = integrate.quad(lambda t: gauss(t, a_pymc3, xc_pymc3, w_pymc3, y0_pymc3), 159.1, 162.2)[0] / c_0 peak_integral_err = np.sqrt(2 * np.pi * ((w_pymc3 * aerr_pymc3) ** 2 + (a_pymc3 * werr_pymc3) ** 2 )) / c_0 percent_err = 100*peak_integral_err/peak_integral print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral, peak_integral_err) + 'Percentage error = {}%'.format(percent_err)) Total counts = 22933.86204613347 \u00b1 215.58699451020811 counts Percentage error = 0.9400378971345341% This percentage error was found using a Poisson likelihood, as described above. For a comparison, this integration can be repeated for the alternate sampling method, with a normal likelihood. Using the same alternate parameters that were found earlier, run the same integration process as before: peak_integral_alt = integrate.quad(lambda t: gauss(t, a_alt, xc_alt, w_alt, y0_alt), 159.1, 162.2)[0] / c_0 peak_integral_err_alt = np.sqrt(2 * np.pi * ((w_alt * a_err_alt) ** 2 + (a_alt * w_err_alt) ** 2 )) / c_0 percent_err_alt = 100*peak_integral_err_alt/peak_integral_alt print('Alternate total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_alt, peak_integral_err_alt) + 'Alternate percentage error = {}%'.format(percent_err_alt)) Alternate total counts = 22943.751210742565 \u00b1 216.76313608385547 counts Alternate percentage error = 0.9447589197287152% It appears both methods result in a very simillar pecentage error, even though the alternate method is a little faster on my machine. This validates the theory that using a normal likelihood distribution on this photopeak approximates a Poisson distribution pretty well. In both cases, a percentage error of around 1% is more than acceptable, in general. Initially, I used a least-squares algorithm to fit curves to this same data set, which produced a percentage error around 1.3%. This leads me to conclude that using an MCMC algorithm was quite successful.","title":"Model comparisons"},{"location":"PyMC3_GRS/PyMC3_GRS/#modelling-with-dynesty","text":"Now that we've evaluated PyMC3's abillity to sample the gamma-ray spectrum, we can explore other samplers to see if they can do a better job. For this, I'll use \"dynesty\". This sampler uses nested sampling, rather than MCMC. The key difference between these algorithms is that nested sampling produces an estimate for the marginal likelihood, whilst MCMC does not. I'll again stick to using just the second peak, since we're only really interested in the relative performance of the samplers here.","title":"Modelling with dynesty"},{"location":"PyMC3_GRS/PyMC3_GRS/#sampling-the-data_1","text":"Using dynesty is slightly more complicated than PyMC3. Nested sampling algorithms need to sample from a uniform hyper-cube parameter space. All of our priors have a normal prior distribution, so we first need to define a \"prior transform\" function. This function will transform the priors into the right format, and then transform them back after the sampling. def priortransform(theta): # unpack the transformed parameters a_t, xc_t, w_t, y0_t = theta # define our prior guesses for each parameter a_mu, a_sig = a_guesses[1], 50. xc_mu, xc_sig = xc_guesses[1], 1. w_mu, w_sig = 0.7, 0.3 y0_mu, y0_sig = y0_guesses[1], 30. # convert back to a = a_mu + a_sig*ndtri(a_t) xc = xc_mu + xc_sig*ndtri(xc_t) w = w_mu + w_sig*ndtri(w_t) y0 = y0_mu + y0_sig*ndtri(y0_t) return a,xc,w,y0 Next, we need to define a Poisson log likelihood function. For dynesty, I'll be using a hand-made likelihood function: def loglike(theta): \"\"\" Function to return the log likelihood :param theta: tuple or list containing each parameter :param obs: list or array containing the observed counts of each data point :param times: list or array containing the energy at which each data point is recorded \"\"\" # unpack parameters a_like, xc_like, w_like, y0_like = theta # expected value lmbda = np.array(gauss(peaks_x[1], a_like, xc_like, w_like, y0_like)) n = len(peaks_y[1]) a = np.sum(gammaln(np.array(peaks_y[1])+1)) b = np.sum(np.array(peaks_y[1]) * np.log(lmbda)) return -np.sum(lmbda) - a + b Now we can begin to set up the hyperparameters for the nested sampling algorithm. For dynesty, we need to provide the number of live points, sampling algorithm, sampling method, and a stopping criterion. Since the Gaussian model only has 4 parameters, we can choose a bound and sampling method that work well with low-dimensional models: stop = 0.1 # stopping criterion nparam = 4 # number of parameters sampler = NestedSampler(loglike, priortransform, nparam, bound='multi', sample='unif', nlive=1000) t0_dynesty = time() sampler.run_nested(dlogz=stop, print_progress=False) t1_dynesty = time() print('{} seconds taken to run dynesty sampling'.format(t1_dynesty-t0_dynesty)) 10.890472173690796 seconds taken to run dynesty sampling We can now collect the results and view the summary of the sampling process, which includes the log of the marginal likelihood, number of interations, and some other values: results = sampler.results print(results.summary()) Summary ======= nlive: 1000 niter: 13873 ncall: 86323 eff(%): 17.229 logz: -212.097 +/- 0.141 None","title":"Sampling the data"},{"location":"PyMC3_GRS/PyMC3_GRS/#results_1","text":"To retrieve the posteriors for each parameter from a nested sampling algorithm, you need to resample using weights, which dynesty outputs with the results. This can be done easily as is shown below: weights = np.exp(results['logwt'] - results['logz'][-1]) samples = results.samples dynesty_samples = resample_equal(samples, weights) Now, using the same methods as before, we can print the mean and error of the parameters, and sample the posteriors to produce a posterior predictive plot for the second peak in our data: a, xc, w, y0 = [dynesty_samples[:,i] for i in range(4)] a_dynesty, aerr_dynesty = np.mean(a), np.std(a) xc_dynesty, xcerr_dynesty = np.mean(xc), np.std(xc) w_dynesty, werr_dynesty = np.mean(w), np.std(w) y0_dynesty, y0err_dynesty = np.mean(y0), np.std(y0) nfits = 300 a_samps_dynesty = np.random.normal(a_dynesty, aerr_dynesty, nfits) xc_samps_dynesty = np.random.normal(xc_dynesty, xcerr_dynesty, nfits) w_samps_dynesty = np.random.normal(w_dynesty, werr_dynesty, nfits) y0_samps_dynesty = np.random.normal(y0_dynesty, y0err_dynesty, nfits) print('Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: \\n \\n ' + ' Amplitude = {} \\u00B1 {} counts \\n'.format(a_dynesty, aerr_dynesty) + ' Peak Energy = {} \\u00B1 {} keV \\n'.format(xc_dynesty, xcerr_dynesty) + ' Standard Deviation = {} \\u00B1 {} keV \\n'.format(w_dynesty, werr_dynesty) + ' Background Counts = {} \\u00B1 {} counts \\n'.format(y0_dynesty, y0err_dynesty)) Parameter mean values with uncertainties for a photopeak in a sample of Ba-133: Amplitude = 809.6473274589662 \u00b1 24.636099224592314 counts Peak Energy = 160.60478084398832 \u00b1 0.019342705091128825 keV Standard Deviation = 0.5237467836545735 \u00b1 0.01933901750104941 keV Background Counts = 1333.4565559019425 \u00b1 6.802782860785766 counts","title":"Results"},{"location":"PyMC3_GRS/PyMC3_GRS/#plotting-the-posterior_1","text":"So far, dynesty is in strong agreement with PyMC3, with both the values and errors being comparable to those from either PyMC3 sampling method we investigated. Below are two plots, the left plot shows the mean Gaussian curve produced by dynesty, and the right shows the posterior predictive plot: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'b.') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'k:') ax1.set_ylim(1200) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01) ax2.set_ylim(1200) plt.suptitle('Mean posterior (left) and posterior predictive (right) of a photopeak in a sample of\\n' + ' Ba-133, sampled by dynesty') plt.show() Again, this looks very simillar to the results produced by PyMC3. We can confirm this by remaking the same plot as above, but this time overplotting the mean and posterior predictive plots from PyMC3: x, y = peaks_x[1], peaks_y[1] fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,5)) xsmooth = np.linspace(min(x),max(x), 1000) # mean posterior plot ax1.plot(x, y, 'k.',label='Data') ax1.plot(xsmooth, gauss(xsmooth,a_dynesty,xc_dynesty,w_dynesty,y0_dynesty),'b:',label='dynesty') ax1.plot(xsmooth, gauss(xsmooth,a_pymc3,xc_pymc3,w_pymc3,y0_pymc3),'r:',label='PyMC3') ax1.set_ylim(1200) leg1 = ax1.legend(loc='upper right') for lh in leg1.legendHandles: lh.set_alpha(1) # posterior predictive plot for i in range(nfits): ax2.plot(xsmooth, gauss(xsmooth,a_samps_dynesty[i],xc_samps_dynesty[i], w_samps_dynesty[i],y0_samps_dynesty[i]),'b-',alpha=0.01, label=('dynesty' if i == 0 else '')) ax2.plot(xsmooth, gauss(xsmooth,a_samps_pymc3[1][i],xc_samps_pymc3[1][i], w_samps_pymc3[1][i],y0_samps_pymc3[1][i]),'r-',alpha=0.01, label=('PyMC3' if i == 0 else '')) ax2.set_ylim(1200) leg2 = ax2.legend(loc='upper right') for lh in leg2.legendHandles: lh.set_alpha(1) plt.suptitle('Mean posterior (left) and posterior predictive (right) plots of a photopeak in a sample' + '\\n Ba-133, sampled by PyMC3 (red) and dynesty (blue)') plt.show()","title":"Plotting the posterior"},{"location":"PyMC3_GRS/PyMC3_GRS/#model-comparisons_1","text":"This above plot shows quite nicely that whilst dynesty predicts a slightly higher amplitude than PyMC3, both samplers do agree with eachother to very simillar degrees of accuracy, with only a small discrepancy at the very tip of the peak. We can determine if this has a large impact on the results of the GRS analysis by finding the area under the mean curve produced by dynesty: peak_integral_dynesty = integrate.quad(lambda t: gauss(t, a_dynesty, xc_dynesty, w_dynesty, y0_dynesty), 159.1, 162.2)[0] / c_0 peak_integral_err_dynesty = np.sqrt(2 * np.pi * ((w_dynesty * aerr_dynesty)** 2 + (a_dynesty * werr_dynesty) ** 2 )) / c_0 percent_err_dynesty = 100*peak_integral_err_dynesty/peak_integral_dynesty print('Total counts = {} \\u00B1 {} counts \\n'.format(peak_integral_dynesty, peak_integral_err_dynesty) + 'Percentage error = {}%'.format(percent_err_dynesty)) Total counts = 22968.854117562147 \u00b1 224.93466521968398 counts Percentage error = 0.9793029468008915% Within their errors, dynesty and both PyMC3 methods agree on the total counts under the curve, and have produce a percentage error around 1%. PyMC3 is a little more intuative to use in general, however dynesty is a lot faster than PyMC3 on my machine, and also gives a value for the marginal likelihood in the process.","title":"Model comparisons"}]}